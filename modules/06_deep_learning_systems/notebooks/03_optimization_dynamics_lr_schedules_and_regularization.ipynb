{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "repo_root = Path.cwd().parent.parent.parent\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "TrainingConfig = safe_import_from('06_deep_learning_systems.src.config', 'TrainingConfig')\n",
    "SimpleMLP = safe_import_from('06_deep_learning_systems.src.models', 'SimpleMLP')\n",
    "get_mnist_loaders = safe_import_from('06_deep_learning_systems.src.datasets', 'get_mnist_loaders')\n",
    "Trainer = safe_import_from('06_deep_learning_systems.src.trainer', 'Trainer')\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core.seeding', 'set_seed')\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n",
    "\n",
    "# Setup\n",
    "reports_dir = Path(\"../reports/notebook_03\")\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c9905",
   "metadata": {},
   "source": [
    "## Part 1: Optimizer Comparison - SGD vs Adam\n",
    "\n",
    "### Theory (Minimal)\n",
    "\n",
    "**SGD (Stochastic Gradient Descent)**:\n",
    "```\n",
    "Î¸_{t+1} = Î¸_t - lr * âˆ‡L(Î¸_t)\n",
    "```\n",
    "- Simple, stable, requires careful LR tuning\n",
    "- With momentum: smooth out gradients\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)**:\n",
    "```\n",
    "m_t = Î²â‚*m_{t-1} + (1-Î²â‚)*âˆ‡L     # momentum\n",
    "v_t = Î²â‚‚*v_{t-1} + (1-Î²â‚‚)*âˆ‡LÂ²    # adaptive LR\n",
    "Î¸_{t+1} = Î¸_t - lr * m_t/âˆš(v_t)\n",
    "```\n",
    "- Adapts LR per parameter\n",
    "- More forgiving to LR choice\n",
    "- May generalize worse on some problems\n",
    "\n",
    "Let's compare them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea2dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (small subset for speed)\n",
    "print(\"Loading MNIST...\")\n",
    "train_loader, val_loader, test_loader = get_mnist_loaders(\n",
    "    data_dir=Path(\"../../../data\"),\n",
    "    batch_size=128,\n",
    "    val_split=0.1,\n",
    "    num_workers=0,\n",
    "    seed=42,\n",
    ")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_experiment(optimizer_name: str, lr: float, epochs: int = 5):\n",
    "    \"\"\"Run training with specified optimizer and LR.\"\"\"\n",
    "    set_seed(42)  # Same init for fair comparison\n",
    "    \n",
    "    config = TrainingConfig(\n",
    "        name=f\"{optimizer_name}_lr{lr}\",\n",
    "        seed=42,\n",
    "        model_type=\"SimpleMLP\",\n",
    "        input_dim=784,\n",
    "        hidden_dims=[128, 64],\n",
    "        output_dim=10,\n",
    "        batch_size=128,\n",
    "        num_epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        optimizer=optimizer_name,\n",
    "        early_stop_patience=100,  # No early stopping\n",
    "        save_artifacts=False,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    \n",
    "    model = SimpleMLP(\n",
    "        input_dim=config.input_dim,\n",
    "        hidden_dims=config.hidden_dims,\n",
    "        output_dim=config.output_dim,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(config, model, device=\"cpu\")\n",
    "    history = trainer.train(train_loader, val_loader)\n",
    "    test_metrics = trainer.evaluate(test_loader)\n",
    "    \n",
    "    return history, test_metrics\n",
    "\n",
    "# Experiment 1: SGD with good LR\n",
    "print(\"\\nExperiment 1: SGD (lr=0.01)\")\n",
    "print(\"=\"*60)\n",
    "history_sgd, metrics_sgd = run_training_experiment(\"sgd\", lr=0.01, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332d8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Adam with same LR\n",
    "print(\"\\nExperiment 2: Adam (lr=0.01)\")\n",
    "print(\"=\"*60)\n",
    "history_adam, metrics_adam = run_training_experiment(\"adam\", lr=0.01, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Adam with smaller LR (more typical)\n",
    "print(\"\\nExperiment 3: Adam (lr=0.001)\")\n",
    "print(\"=\"*60)\n",
    "history_adam_small, metrics_adam_small = run_training_experiment(\"adam\", lr=0.001, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    (\"SGD (lr=0.01)\", metrics_sgd['accuracy'], history_sgd['train_loss'][-1]),\n",
    "    (\"Adam (lr=0.01)\", metrics_adam['accuracy'], history_adam['train_loss'][-1]),\n",
    "    (\"Adam (lr=0.001)\", metrics_adam_small['accuracy'], history_adam_small['train_loss'][-1]),\n",
    "]\n",
    "\n",
    "for name, acc, train_loss in results:\n",
    "    print(f\"{name:20s} â†’ Test Acc: {acc:.4f}, Train Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcab636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = np.arange(1, 6)\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(epochs, history_sgd['train_loss'], 'o-', label='SGD (lr=0.01)', linewidth=2, markersize=8)\n",
    "axes[0].plot(epochs, history_adam['train_loss'], 's-', label='Adam (lr=0.01)', linewidth=2, markersize=8)\n",
    "axes[0].plot(epochs, history_adam_small['train_loss'], '^-', label='Adam (lr=0.001)', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Train Loss', fontsize=12)\n",
    "axes[0].set_title('Optimizer Comparison: Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "axes[1].plot(epochs, history_sgd['val_accuracy'], 'o-', label='SGD (lr=0.01)', linewidth=2, markersize=8)\n",
    "axes[1].plot(epochs, history_adam['val_accuracy'], 's-', label='Adam (lr=0.01)', linewidth=2, markersize=8)\n",
    "axes[1].plot(epochs, history_adam_small['val_accuracy'], '^-', label='Adam (lr=0.001)', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Validation Accuracy', fontsize=12)\n",
    "axes[1].set_title('Optimizer Comparison: Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"optimizer_comparison.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Saved to {reports_dir / 'optimizer_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296738ed",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- **SGD**: Stable but may need more tuning\n",
    "- **Adam (lr=0.01)**: May be too aggressive (faster but less stable)\n",
    "- **Adam (lr=0.001)**: Typically good default for Adam\n",
    "\n",
    "**Rule of thumb**: Start with Adam (lr=1e-3), then try SGD with momentum (lr=1e-2) if you have time to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c1dc2",
   "metadata": {},
   "source": [
    "## Part 2: Learning Rate Sensitivity\n",
    "\n",
    "**Question**: How sensitive is training to LR choice?\n",
    "\n",
    "Let's sweep over learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92446fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR sweep for Adam\n",
    "learning_rates = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2]\n",
    "lr_results = []\n",
    "\n",
    "print(\"Learning Rate Sweep (Adam)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining with lr={lr:.0e}...\")\n",
    "    history, metrics = run_training_experiment(\"adam\", lr=lr, epochs=5)\n",
    "    \n",
    "    lr_results.append({\n",
    "        'lr': lr,\n",
    "        'test_acc': metrics['accuracy'],\n",
    "        'test_loss': metrics['loss'],\n",
    "        'final_train_loss': history['train_loss'][-1],\n",
    "        'final_val_acc': history['val_accuracy'][-1],\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test acc: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d553d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LR sensitivity\n",
    "df_lr = pd.DataFrame(lr_results)\n",
    "print(\"\\nLR Sweep Results:\")\n",
    "print(df_lr.to_string(index=False))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Test accuracy vs LR\n",
    "axes[0].semilogx(df_lr['lr'], df_lr['test_acc'], 'o-', linewidth=2, markersize=10, color='steelblue')\n",
    "axes[0].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[0].set_title('LR Sensitivity: Test Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(1e-3, color='red', linestyle='--', alpha=0.5, label='Typical LR')\n",
    "axes[0].legend()\n",
    "\n",
    "# Train loss vs LR\n",
    "axes[1].loglog(df_lr['lr'], df_lr['final_train_loss'], 's-', linewidth=2, markersize=10, color='orange')\n",
    "axes[1].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[1].set_ylabel('Final Train Loss', fontsize=12)\n",
    "axes[1].set_title('LR Sensitivity: Train Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, which='both')\n",
    "axes[1].axvline(1e-3, color='red', linestyle='--', alpha=0.5, label='Typical LR')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"lr_sensitivity.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Saved to {reports_dir / 'lr_sensitivity.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e78b85",
   "metadata": {},
   "source": [
    "### Key Insights\n",
    "\n",
    "- **Too low** (1e-4): Slow convergence, may not reach optimum\n",
    "- **Sweet spot** (1e-3 to 3e-3): Good balance\n",
    "- **Too high** (1e-2): May overshoot or be unstable\n",
    "\n",
    "**Finding good LR**: Look for the \"elbow\" where accuracy plateaus and loss is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddec623",
   "metadata": {},
   "source": [
    "## Part 3: Learning Rate Schedules\n",
    "\n",
    "**Idea**: Start with high LR (fast progress), then decay (fine-tune).\n",
    "\n",
    "**Common schedules**:\n",
    "- **StepLR**: Multiply by Î³ every N epochs\n",
    "- **CosineAnnealing**: Smooth decay following cosine curve\n",
    "- **ReduceLROnPlateau**: Decay when val loss plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_schedule(schedule_name: str, epochs: int = 10):\n",
    "    \"\"\"Train with LR schedule.\"\"\"\n",
    "    set_seed(42)\n",
    "    \n",
    "    model = SimpleMLP(input_dim=784, hidden_dims=[128, 64], output_dim=10)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)  # Start high\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create scheduler\n",
    "    if schedule_name == \"step\":\n",
    "        scheduler = StepLR(optimizer, step_size=3, gamma=0.5)  # Halve every 3 epochs\n",
    "    elif schedule_name == \"cosine\":\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    # Training loop\n",
    "    history = {'train_loss': [], 'val_acc': [], 'lr': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += (pred == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        \n",
    "        # Record\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: loss={avg_train_loss:.4f}, val_acc={val_acc:.4f}, lr={current_lr:.6f}\")\n",
    "        \n",
    "        # Step scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Experiment: Constant LR\n",
    "print(\"\\nExperiment: Constant LR (no schedule)\")\n",
    "print(\"=\"*60)\n",
    "history_constant = train_with_schedule(\"none\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf12c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: StepLR\n",
    "print(\"\\nExperiment: StepLR (halve every 3 epochs)\")\n",
    "print(\"=\"*60)\n",
    "history_step = train_with_schedule(\"step\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Cosine Annealing\n",
    "print(\"\\nExperiment: Cosine Annealing\")\n",
    "print(\"=\"*60)\n",
    "history_cosine = train_with_schedule(\"cosine\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize schedules\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "epochs = np.arange(1, 11)\n",
    "\n",
    "# LR schedules\n",
    "axes[0, 0].plot(epochs, history_constant['lr'], 'o-', label='Constant', linewidth=2, markersize=6)\n",
    "axes[0, 0].plot(epochs, history_step['lr'], 's-', label='StepLR', linewidth=2, markersize=6)\n",
    "axes[0, 0].plot(epochs, history_cosine['lr'], '^-', label='Cosine', linewidth=2, markersize=6)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[0, 0].set_title('LR Schedules', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Train loss\n",
    "axes[0, 1].plot(epochs, history_constant['train_loss'], 'o-', label='Constant', linewidth=2, markersize=6)\n",
    "axes[0, 1].plot(epochs, history_step['train_loss'], 's-', label='StepLR', linewidth=2, markersize=6)\n",
    "axes[0, 1].plot(epochs, history_cosine['train_loss'], '^-', label='Cosine', linewidth=2, markersize=6)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Train Loss', fontsize=12)\n",
    "axes[0, 1].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Val accuracy\n",
    "axes[1, 0].plot(epochs, history_constant['val_acc'], 'o-', label='Constant', linewidth=2, markersize=6)\n",
    "axes[1, 0].plot(epochs, history_step['val_acc'], 's-', label='StepLR', linewidth=2, markersize=6)\n",
    "axes[1, 0].plot(epochs, history_cosine['val_acc'], '^-', label='Cosine', linewidth=2, markersize=6)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Validation Accuracy', fontsize=12)\n",
    "axes[1, 0].set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Final comparison table\n",
    "axes[1, 1].axis('off')\n",
    "table_data = [\n",
    "    ['Schedule', 'Final Val Acc', 'Final Loss'],\n",
    "    ['Constant', f\"{history_constant['val_acc'][-1]:.4f}\", f\"{history_constant['train_loss'][-1]:.4f}\"],\n",
    "    ['StepLR', f\"{history_step['val_acc'][-1]:.4f}\", f\"{history_step['train_loss'][-1]:.4f}\"],\n",
    "    ['Cosine', f\"{history_cosine['val_acc'][-1]:.4f}\", f\"{history_cosine['train_loss'][-1]:.4f}\"],\n",
    "]\n",
    "table = axes[1, 1].table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                         colWidths=[0.3, 0.35, 0.35])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "# Style header\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"lr_schedules.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Saved to {reports_dir / 'lr_schedules.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec508c",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- **Constant**: Simple, but may overshoot later\n",
    "- **StepLR**: Sudden drops help escape plateaus\n",
    "- **Cosine**: Smooth decay, popular for training from scratch\n",
    "\n",
    "**When to use schedules**: Long training runs (>20 epochs), or when you see val loss plateau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467a51f",
   "metadata": {},
   "source": [
    "## Part 4: Regularization - Weight Decay & Dropout\n",
    "\n",
    "**Goal**: Prevent overfitting\n",
    "\n",
    "**Weight Decay (L2 regularization)**:\n",
    "```\n",
    "Loss = CrossEntropy + Î» * ||Î¸||Â²\n",
    "```\n",
    "Penalizes large weights â†’ smoother decision boundaries\n",
    "\n",
    "**Dropout**:\n",
    "Randomly zero out neurons during training â†’ ensemble effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization sweep\n",
    "reg_configs = [\n",
    "    {\"name\": \"No regularization\", \"weight_decay\": 0.0, \"dropout\": 0.0},\n",
    "    {\"name\": \"Weight decay\", \"weight_decay\": 1e-4, \"dropout\": 0.0},\n",
    "    {\"name\": \"Dropout\", \"weight_decay\": 0.0, \"dropout\": 0.3},\n",
    "    {\"name\": \"Both\", \"weight_decay\": 1e-4, \"dropout\": 0.3},\n",
    "]\n",
    "\n",
    "reg_results = []\n",
    "\n",
    "print(\"Regularization Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for cfg in reg_configs:\n",
    "    print(f\"\\n{cfg['name']}...\")\n",
    "    set_seed(42)\n",
    "    \n",
    "    config = TrainingConfig(\n",
    "        name=cfg['name'],\n",
    "        seed=42,\n",
    "        model_type=\"SimpleMLP\",\n",
    "        input_dim=784,\n",
    "        hidden_dims=[128, 64],\n",
    "        output_dim=10,\n",
    "        dropout=cfg['dropout'],\n",
    "        batch_size=128,\n",
    "        num_epochs=5,\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=cfg['weight_decay'],\n",
    "        optimizer=\"adam\",\n",
    "        early_stop_patience=100,\n",
    "        save_artifacts=False,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    \n",
    "    model = SimpleMLP(\n",
    "        input_dim=config.input_dim,\n",
    "        hidden_dims=config.hidden_dims,\n",
    "        output_dim=config.output_dim,\n",
    "        dropout=config.dropout,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(config, model, device=\"cpu\")\n",
    "    history = trainer.train(train_loader, val_loader)\n",
    "    test_metrics = trainer.evaluate(test_loader)\n",
    "    \n",
    "    reg_results.append({\n",
    "        'name': cfg['name'],\n",
    "        'test_acc': test_metrics['accuracy'],\n",
    "        'train_loss': history['train_loss'][-1],\n",
    "        'val_loss': history['val_loss'][-1],\n",
    "        'gap': history['train_loss'][-1] - history['val_loss'][-1],  # Overfitting indicator\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test acc: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Train/Val gap: {reg_results[-1]['gap']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c45d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regularization effects\n",
    "df_reg = pd.DataFrame(reg_results)\n",
    "print(\"\\nRegularization Results:\")\n",
    "print(df_reg.to_string(index=False))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x_pos = np.arange(len(df_reg))\n",
    "\n",
    "# Test accuracy\n",
    "axes[0].bar(x_pos, df_reg['test_acc'], color='steelblue', alpha=0.7)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(df_reg['name'], rotation=15, ha='right')\n",
    "axes[0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[0].set_title('Regularization Effect on Test Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0.95, 0.98])\n",
    "\n",
    "# Train-Val gap (overfitting indicator)\n",
    "colors = ['red' if gap < 0 else 'orange' for gap in df_reg['gap']]\n",
    "axes[1].bar(x_pos, np.abs(df_reg['gap']), color=colors, alpha=0.7)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(df_reg['name'], rotation=15, ha='right')\n",
    "axes[1].set_ylabel('|Train Loss - Val Loss|', fontsize=12)\n",
    "axes[1].set_title('Overfitting Gap (smaller = better)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"regularization_comparison.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Saved to {reports_dir / 'regularization_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e7fc2",
   "metadata": {},
   "source": [
    "### Key Insights\n",
    "\n",
    "- **No regularization**: May overfit (train loss << val loss)\n",
    "- **Weight decay**: Smooth regularization, good default (1e-4 to 1e-5)\n",
    "- **Dropout**: Stronger regularization, use when overfitting is severe\n",
    "- **Both**: Can combine, but may hurt underfitting regime\n",
    "\n",
    "**Rule**: Start without regularization on small datasets. Add if train/val gap is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7cf907",
   "metadata": {},
   "source": [
    "## Part 5: Save Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01196caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"optimizer_comparison\": {\n",
    "        \"sgd_lr0.01\": float(metrics_sgd['accuracy']),\n",
    "        \"adam_lr0.01\": float(metrics_adam['accuracy']),\n",
    "        \"adam_lr0.001\": float(metrics_adam_small['accuracy']),\n",
    "    },\n",
    "    \"lr_sensitivity\": df_lr.to_dict('records'),\n",
    "    \"best_lr\": float(df_lr.loc[df_lr['test_acc'].idxmax(), 'lr']),\n",
    "    \"lr_schedules\": {\n",
    "        \"constant\": float(history_constant['val_acc'][-1]),\n",
    "        \"step\": float(history_step['val_acc'][-1]),\n",
    "        \"cosine\": float(history_cosine['val_acc'][-1]),\n",
    "    },\n",
    "    \"regularization\": df_reg.to_dict('records'),\n",
    "    \"recommendations\": {\n",
    "        \"default_optimizer\": \"adam\",\n",
    "        \"default_lr_adam\": 0.001,\n",
    "        \"default_lr_sgd\": 0.01,\n",
    "        \"default_weight_decay\": 1e-4,\n",
    "        \"use_schedule_if\": \"training >20 epochs or val loss plateaus\",\n",
    "        \"use_dropout_if\": \"severe overfitting (train_loss << val_loss)\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(reports_dir / \"summary.json\", 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved summary to {reports_dir / 'summary.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff4cc9",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### âœ… Optimization Best Practices\n",
    "\n",
    "1. **Start simple**: Adam with lr=1e-3, no regularization\n",
    "2. **LR search**: Try [1e-4, 3e-4, 1e-3, 3e-3] and pick best\n",
    "3. **Add schedule**: If training >20 epochs or val loss plateaus\n",
    "4. **Regularize if overfitting**: Weight decay (1e-4) first, then dropout (0.3)\n",
    "5. **SGD alternative**: If you have time to tune, SGD+momentum can generalize better\n",
    "\n",
    "### âš ï¸ Common Pitfalls\n",
    "\n",
    "1. **LR too high**: Loss explodes or oscillates wildly\n",
    "2. **LR too low**: Training is painfully slow, gets stuck in plateaus\n",
    "3. **Over-regularization**: Hurts when model is underfitting\n",
    "4. **Tuning prematurely**: Fix bugs (overfit test) before tuning hyperparameters\n",
    "5. **Ignoring train/val gap**: Sign of overfitting that regularization can fix\n",
    "\n",
    "### ðŸ”§ Debugging Optimization Issues\n",
    "\n",
    "| Symptom | Likely Cause | Fix |\n",
    "|---------|--------------|-----|\n",
    "| Loss explodes | LR too high | Reduce by 10x |\n",
    "| Loss barely moves | LR too low | Increase by 10x |\n",
    "| Val loss >> train loss | Overfitting | Add weight decay or dropout |\n",
    "| Both losses high | Underfitting | Remove regularization, increase capacity |\n",
    "| Validation plateaus | Local minimum | Try LR schedule or restart with different seed |\n",
    "| Unstable training | Batch size or LR | Increase batch size or reduce LR |\n",
    "\n",
    "### ðŸ“‹ Hyperparameter Tuning Workflow\n",
    "\n",
    "```python\n",
    "# 1. Start with defaults\n",
    "optimizer = Adam(lr=1e-3, weight_decay=0)\n",
    "\n",
    "# 2. Verify overfit test passes\n",
    "assert overfit_test_passes(), \"Fix bugs first!\"\n",
    "\n",
    "# 3. Quick LR search (3-5 values)\n",
    "for lr in [1e-4, 3e-4, 1e-3, 3e-3]:\n",
    "    train_and_evaluate(lr)\n",
    "\n",
    "# 4. Check for overfitting\n",
    "if train_loss << val_loss:\n",
    "    add_weight_decay(1e-4)\n",
    "    # or add_dropout(0.3)\n",
    "\n",
    "# 5. Add schedule if needed\n",
    "if num_epochs > 20:\n",
    "    use_cosine_schedule()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af52b7",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323e024",
   "metadata": {},
   "source": [
    "### Exercise 1: Find Optimal LR for SGD\n",
    "\n",
    "Run an LR sweep for SGD (not Adam) and find the best learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff9a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad176dc",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement ReduceLROnPlateau\n",
    "\n",
    "Use `torch.optim.lr_scheduler.ReduceLROnPlateau` to automatically reduce LR when validation loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b092bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e24a7d",
   "metadata": {},
   "source": [
    "### Exercise 3: Overfitting Challenge\n",
    "\n",
    "Intentionally create a model that overfits severely (train acc >99%, val acc <90%). Then fix it with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06882d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hints: very large model, no regularization, train longer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5d85d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solutions</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0be49",
   "metadata": {},
   "source": [
    "### Solution 1: SGD LR Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD LR sweep\n",
    "sgd_lrs = [1e-3, 3e-3, 1e-2, 3e-2, 1e-1]\n",
    "sgd_results = []\n",
    "\n",
    "for lr in sgd_lrs:\n",
    "    print(f\"SGD with lr={lr:.0e}\")\n",
    "    history, metrics = run_training_experiment(\"sgd\", lr=lr, epochs=5)\n",
    "    sgd_results.append({'lr': lr, 'test_acc': metrics['accuracy']})\n",
    "    print(f\"  Test acc: {metrics['accuracy']:.4f}\\n\")\n",
    "\n",
    "df_sgd = pd.DataFrame(sgd_results)\n",
    "best_lr = df_sgd.loc[df_sgd['test_acc'].idxmax(), 'lr']\n",
    "print(f\"Best SGD LR: {best_lr:.0e}\")\n",
    "print(df_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935e4b1",
   "metadata": {},
   "source": [
    "### Solution 2: ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "set_seed(42)\n",
    "model = SimpleMLP(784, [128, 64], 10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Train\n",
    "    model.train()\n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            val_loss += criterion(model(X), y).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: val_loss={val_loss:.4f}, lr={optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Step scheduler based on val loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(\"\\nâ†’ LR reduced automatically when val loss plateaued!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae5287f",
   "metadata": {},
   "source": [
    "### Solution 3: Overfitting Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overfitting scenario\n",
    "print(\"Step 1: Create severely overfitting model\\n\")\n",
    "set_seed(42)\n",
    "\n",
    "# Very large model, no regularization\n",
    "config_overfit = TrainingConfig(\n",
    "    name=\"overfit\",\n",
    "    seed=42,\n",
    "    model_type=\"SimpleMLP\",\n",
    "    input_dim=784,\n",
    "    hidden_dims=[512, 512, 256],  # Very large\n",
    "    output_dim=10,\n",
    "    dropout=0.0,  # No dropout\n",
    "    batch_size=128,\n",
    "    num_epochs=15,  # Train longer\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.0,  # No weight decay\n",
    "    optimizer=\"adam\",\n",
    "    early_stop_patience=100,\n",
    "    save_artifacts=False,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "model_overfit = SimpleMLP(\n",
    "    input_dim=config_overfit.input_dim,\n",
    "    hidden_dims=config_overfit.hidden_dims,\n",
    "    output_dim=config_overfit.output_dim,\n",
    ")\n",
    "\n",
    "trainer_overfit = Trainer(config_overfit, model_overfit, device=\"cpu\")\n",
    "history_overfit = trainer_overfit.train(train_loader, val_loader)\n",
    "\n",
    "print(f\"\\nOverfitting achieved:\")\n",
    "print(f\"  Final train loss: {history_overfit['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final val loss: {history_overfit['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Gap: {history_overfit['val_loss'][-1] - history_overfit['train_loss'][-1]:.4f}\")\n",
    "\n",
    "# Fix with regularization\n",
    "print(\"\\n\\nStep 2: Fix with regularization\\n\")\n",
    "set_seed(42)\n",
    "\n",
    "config_fixed = TrainingConfig(\n",
    "    name=\"fixed\",\n",
    "    seed=42,\n",
    "    model_type=\"SimpleMLP\",\n",
    "    input_dim=784,\n",
    "    hidden_dims=[512, 512, 256],\n",
    "    output_dim=10,\n",
    "    dropout=0.5,  # Added dropout\n",
    "    batch_size=128,\n",
    "    num_epochs=15,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-3,  # Added weight decay\n",
    "    optimizer=\"adam\",\n",
    "    early_stop_patience=100,\n",
    "    save_artifacts=False,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "model_fixed = SimpleMLP(\n",
    "    input_dim=config_fixed.input_dim,\n",
    "    hidden_dims=config_fixed.hidden_dims,\n",
    "    output_dim=config_fixed.output_dim,\n",
    "    dropout=config_fixed.dropout,\n",
    ")\n",
    "\n",
    "trainer_fixed = Trainer(config_fixed, model_fixed, device=\"cpu\")\n",
    "history_fixed = trainer_fixed.train(train_loader, val_loader)\n",
    "\n",
    "print(f\"\\nAfter regularization:\")\n",
    "print(f\"  Final train loss: {history_fixed['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final val loss: {history_fixed['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Gap: {history_fixed['val_loss'][-1] - history_fixed['train_loss'][-1]:.4f}\")\n",
    "print(\"\\nâœ“ Overfitting reduced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfc9d8",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9644934d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 04**: Monitoring & error analysis - confusion matrices, per-class metrics, error inspection\n",
    "\n",
    "**Remember**: Start simple (Adam, lr=1e-3, no regularization), verify it works (overfit test), then tune systematically."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
