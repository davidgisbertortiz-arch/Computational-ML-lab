{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ecb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "repo_root = Path.cwd().parent.parent.parent\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "TrainingConfig = safe_import_from('06_deep_learning_systems.src.config', 'TrainingConfig')\n",
    "SimpleMLP = safe_import_from('06_deep_learning_systems.src.models', 'SimpleMLP')\n",
    "get_mnist_loaders = safe_import_from('06_deep_learning_systems.src.datasets', 'get_mnist_loaders')\n",
    "Trainer = safe_import_from('06_deep_learning_systems.src.trainer', 'Trainer')\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core.seeding', 'set_seed')\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n",
    "\n",
    "# Setup\n",
    "reports_dir = Path(\"../reports/notebook_04\")\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77cd84",
   "metadata": {},
   "source": [
    "## Part 1: Train a Model\n",
    "\n",
    "First, train a simple model to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading MNIST...\")\n",
    "train_loader, val_loader, test_loader = get_mnist_loaders(\n",
    "    data_dir=Path(\"../../../data\"),\n",
    "    batch_size=128,\n",
    "    val_split=0.1,\n",
    "    num_workers=0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Get raw test dataset for error inspection\n",
    "test_dataset = test_loader.dataset\n",
    "\n",
    "print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268dac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"\\nTraining model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "config = TrainingConfig(\n",
    "    name=\"monitoring_demo\",\n",
    "    seed=42,\n",
    "    model_type=\"SimpleMLP\",\n",
    "    input_dim=784,\n",
    "    hidden_dims=[128, 64],\n",
    "    output_dim=10,\n",
    "    batch_size=128,\n",
    "    num_epochs=5,\n",
    "    learning_rate=1e-3,\n",
    "    optimizer=\"adam\",\n",
    "    early_stop_patience=100,\n",
    "    save_artifacts=False,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "model = SimpleMLP(\n",
    "    input_dim=config.input_dim,\n",
    "    hidden_dims=config.hidden_dims,\n",
    "    output_dim=config.output_dim,\n",
    ")\n",
    "\n",
    "trainer = Trainer(config, model, device=\"cpu\")\n",
    "history = trainer.train(train_loader, val_loader)\n",
    "test_metrics = trainer.evaluate(test_loader)\n",
    "\n",
    "print(f\"\\nâœ“ Training complete\")\n",
    "print(f\"  Test accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Test loss: {test_metrics['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3ebf6",
   "metadata": {},
   "source": [
    "## Part 2: Confusion Matrix\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Confusion Matrix**: Rows = true labels, Columns = predicted labels\n",
    "\n",
    "```\n",
    "           Predicted\n",
    "           0  1  2  ...\n",
    "True  0  [100  2  1 ...]\n",
    "      1  [  1 95  3 ...]\n",
    "      2  [  0  4 92 ...]\n",
    "     ...\n",
    "```\n",
    "\n",
    "**Diagonal** = correct predictions\n",
    "**Off-diagonal** = confusions (e.g., \"8\" predicted as \"3\")\n",
    "\n",
    "Let's compute it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for all test data\n",
    "print(\"Computing predictions...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []  # For confidence analysis\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        logits = model(X_batch)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "print(f\"âœ“ Predictions computed for {len(all_preds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbcc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Normalize for better visualization\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Raw counts\n",
    "im1 = axes[0].imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xticks(range(10))\n",
    "axes[0].set_yticks(range(10))\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        text_color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "        axes[0].text(j, i, str(cm[i, j]), ha='center', va='center',\n",
    "                    color=text_color, fontsize=9)\n",
    "\n",
    "# Normalized (percentages)\n",
    "im2 = axes[1].imshow(cm_normalized, cmap='Blues', interpolation='nearest', vmin=0, vmax=1)\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xticks(range(10))\n",
    "axes[1].set_yticks(range(10))\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        text_color = 'white' if cm_normalized[i, j] > 0.5 else 'black'\n",
    "        axes[1].text(j, i, f\"{cm_normalized[i, j]:.2f}\", ha='center', va='center',\n",
    "                    color=text_color, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"confusion_matrix.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Saved to {reports_dir / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b232a735",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Look for:\n",
    "- **Dark diagonal**: Good! Most predictions correct\n",
    "- **Off-diagonal bright spots**: Systematic confusions (e.g., 4â†”9, 3â†”8, 5â†”3)\n",
    "- **Weak rows**: Classes the model struggles with\n",
    "\n",
    "**Action**: If you see systematic confusions, inspect those pairs to understand why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84564d",
   "metadata": {},
   "source": [
    "## Part 3: Per-Class Metrics\n",
    "\n",
    "### Theory\n",
    "\n",
    "For each class:\n",
    "\n",
    "**Precision** = TP / (TP + FP) = \"Of predictions for class X, how many correct?\"\n",
    "**Recall** = TP / (TP + FN) = \"Of true class X samples, how many found?\"\n",
    "**F1 Score** = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "**Example**:\n",
    "- Precision low â†’ Model predicts this class too often (false alarms)\n",
    "- Recall low â†’ Model misses this class (false negatives)\n",
    "\n",
    "Let's compute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-class metrics\n",
    "report = classification_report(all_labels, all_preds, output_dict=True, target_names=[str(i) for i in range(10)])\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Class':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class_metrics = []\n",
    "for i in range(10):\n",
    "    class_name = str(i)\n",
    "    precision = report[class_name]['precision']\n",
    "    recall = report[class_name]['recall']\n",
    "    f1 = report[class_name]['f1-score']\n",
    "    support = report[class_name]['support']\n",
    "    \n",
    "    class_metrics.append({\n",
    "        'class': i,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': support,\n",
    "    })\n",
    "    \n",
    "    print(f\"{class_name:<10} {precision:<12.4f} {recall:<12.4f} {f1:<12.4f} {support:<10.0f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Macro Avg':<10} {report['macro avg']['precision']:<12.4f} {report['macro avg']['recall']:<12.4f} {report['macro avg']['f1-score']:<12.4f}\")\n",
    "print(f\"{'Weighted Avg':<10} {report['weighted avg']['precision']:<12.4f} {report['weighted avg']['recall']:<12.4f} {report['weighted avg']['f1-score']:<12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5216202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "classes = [str(i) for i in range(10)]\n",
    "precisions = [report[c]['precision'] for c in classes]\n",
    "recalls = [report[c]['recall'] for c in classes]\n",
    "f1s = [report[c]['f1-score'] for c in classes]\n",
    "\n",
    "x = np.arange(10)\n",
    "width = 0.25\n",
    "\n",
    "# Precision, Recall, F1\n",
    "axes[0].bar(x - width, precisions, width, label='Precision', alpha=0.8)\n",
    "axes[0].bar(x, recalls, width, label='Recall', alpha=0.8)\n",
    "axes[0].bar(x + width, f1s, width, label='F1-Score', alpha=0.8)\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Per-Class Metrics', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(classes)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "\n",
    "# Find worst classes\n",
    "class_f1s = [(i, f1s[i]) for i in range(10)]\n",
    "class_f1s_sorted = sorted(class_f1s, key=lambda x: x[1])\n",
    "worst_classes = class_f1s_sorted[:3]\n",
    "best_classes = class_f1s_sorted[-3:]\n",
    "\n",
    "colors = ['red' if i in [c[0] for c in worst_classes] else 'green' if i in [c[0] for c in best_classes] else 'gray' for i in range(10)]\n",
    "\n",
    "axes[1].bar(x, f1s, color=colors, alpha=0.7)\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('F1-Score', fontsize=12)\n",
    "axes[1].set_title('F1-Score by Class (Red=Worst, Green=Best)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(classes)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"per_class_metrics.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Saved to {reports_dir / 'per_class_metrics.png'}\")\n",
    "print(f\"\\nWorst classes: {[c[0] for c in worst_classes]}\")\n",
    "print(f\"Best classes: {[c[0] for c in best_classes]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3034e74",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- **High precision, low recall**: Model is conservative (misses many but rarely wrong)\n",
    "- **Low precision, high recall**: Model is aggressive (finds many but many false alarms)\n",
    "- **Both low**: Model struggles with this class\n",
    "\n",
    "**Action**: Focus on improving worst classes (data augmentation, more samples, feature engineering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc680f05",
   "metadata": {},
   "source": [
    "## Part 4: Error Inspection - Find Worst Predictions\n",
    "\n",
    "**Goal**: Look at actual failures to find patterns.\n",
    "\n",
    "**Strategy**:\n",
    "1. Find samples with highest confidence but wrong prediction\n",
    "2. Visualize them\n",
    "3. Look for patterns (rotations, unusual writing, class confusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find errors\n",
    "errors = all_preds != all_labels\n",
    "error_indices = np.where(errors)[0]\n",
    "\n",
    "print(f\"Total errors: {errors.sum()} / {len(all_labels)} ({100*errors.mean():.2f}%)\")\n",
    "\n",
    "# Compute confidence for each prediction\n",
    "confidences = all_probs[np.arange(len(all_probs)), all_preds]\n",
    "\n",
    "# Find high-confidence errors (worst failures)\n",
    "error_confidences = confidences[error_indices]\n",
    "sorted_error_idx = error_indices[np.argsort(error_confidences)[::-1]]  # Highest confidence first\n",
    "\n",
    "print(f\"\\nTop 10 most confident errors:\")\n",
    "for i, idx in enumerate(sorted_error_idx[:10]):\n",
    "    true_label = all_labels[idx]\n",
    "    pred_label = all_preds[idx]\n",
    "    confidence = confidences[idx]\n",
    "    print(f\"  {i+1}. Sample {idx}: True={true_label}, Pred={pred_label}, Confidence={confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize worst errors\n",
    "n_errors_to_show = 20\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(sorted_error_idx[:n_errors_to_show]):\n",
    "    # Get image\n",
    "    img, true_label = test_dataset[idx]\n",
    "    img_display = img.squeeze().numpy()\n",
    "    \n",
    "    pred_label = all_preds[idx]\n",
    "    confidence = confidences[idx]\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].imshow(img_display, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.3f}\",\n",
    "                     fontsize=10, color='red')\n",
    "\n",
    "plt.suptitle('Top 20 Most Confident Errors', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"worst_errors.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Saved to {reports_dir / 'worst_errors.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907bc3fc",
   "metadata": {},
   "source": [
    "### What to Look For\n",
    "\n",
    "Common patterns in errors:\n",
    "- **Ambiguous writing**: Poorly written digits that humans might also misclassify\n",
    "- **Systematic confusions**: 4â†”9, 3â†”8, 5â†”3, 7â†”1\n",
    "- **Edge cases**: Unusual orientations, thick/thin strokes\n",
    "- **Data quality**: Corrupted images, wrong labels\n",
    "\n",
    "**Action**: If you see patterns, consider:\n",
    "- Data augmentation (rotations, scaling)\n",
    "- More training data for confused classes\n",
    "- Feature engineering (e.g., aspect ratio features for 1 vs 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc39e0",
   "metadata": {},
   "source": [
    "## Part 5: Slice Analysis - Per-Class Error Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-class error rates\n",
    "class_error_rates = []\n",
    "\n",
    "for class_id in range(10):\n",
    "    class_mask = all_labels == class_id\n",
    "    class_errors = errors[class_mask]\n",
    "    error_rate = class_errors.mean()\n",
    "    n_samples = class_mask.sum()\n",
    "    n_errors = class_errors.sum()\n",
    "    \n",
    "    class_error_rates.append({\n",
    "        'class': class_id,\n",
    "        'error_rate': error_rate,\n",
    "        'n_errors': n_errors,\n",
    "        'n_samples': n_samples,\n",
    "    })\n",
    "\n",
    "print(\"\\nPer-Class Error Analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Class':<10} {'Error Rate':<15} {'Errors':<10} {'Samples':<10}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for entry in class_error_rates:\n",
    "    print(f\"{entry['class']:<10} {entry['error_rate']:<15.4f} {entry['n_errors']:<10} {entry['n_samples']:<10}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error rates\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "classes = [entry['class'] for entry in class_error_rates]\n",
    "error_rates = [entry['error_rate'] for entry in class_error_rates]\n",
    "\n",
    "colors = ['red' if er > np.mean(error_rates) else 'steelblue' for er in error_rates]\n",
    "\n",
    "ax.bar(classes, error_rates, color=colors, alpha=0.7)\n",
    "ax.axhline(np.mean(error_rates), color='orange', linestyle='--', linewidth=2, label='Mean Error Rate')\n",
    "ax.set_xlabel('Class', fontsize=12)\n",
    "ax.set_ylabel('Error Rate', fontsize=12)\n",
    "ax.set_title('Per-Class Error Rates (Red = Worse than Average)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(classes)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"per_class_error_rates.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Saved to {reports_dir / 'per_class_error_rates.png'}\")\n",
    "\n",
    "# Identify problematic classes\n",
    "mean_error_rate = np.mean(error_rates)\n",
    "problematic_classes = [c for c, er in zip(classes, error_rates) if er > mean_error_rate]\n",
    "print(f\"\\nProblematic classes (above average error): {problematic_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc606c8",
   "metadata": {},
   "source": [
    "## Part 6: Save Metrics for Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f74015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all metrics\n",
    "metrics_summary = {\n",
    "    \"overall\": {\n",
    "        \"accuracy\": float(test_metrics['accuracy']),\n",
    "        \"loss\": float(test_metrics['loss']),\n",
    "        \"total_errors\": int(errors.sum()),\n",
    "        \"total_samples\": int(len(all_labels)),\n",
    "        \"error_rate\": float(errors.mean()),\n",
    "    },\n",
    "    \"per_class_metrics\": class_metrics,\n",
    "    \"per_class_errors\": class_error_rates,\n",
    "    \"worst_classes\": [int(c[0]) for c in worst_classes],\n",
    "    \"best_classes\": [int(c[0]) for c in best_classes],\n",
    "    \"problematic_classes\": [int(c) for c in problematic_classes],\n",
    "    \"confusion_matrix\": cm.tolist(),\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(reports_dir / \"evaluation_metrics.json\", 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved all metrics to {reports_dir / 'evaluation_metrics.json'}\")\n",
    "\n",
    "# Save error indices for further analysis\n",
    "np.save(reports_dir / \"error_indices.npy\", error_indices)\n",
    "print(f\"âœ“ Saved error indices to {reports_dir / 'error_indices.npy'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0bccce",
   "metadata": {},
   "source": [
    "## Part 7: Error Pattern Analysis - Confusion Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common confusion pairs\n",
    "confusion_pairs = []\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusion_pairs.append({\n",
    "                'true': i,\n",
    "                'pred': j,\n",
    "                'count': cm[i, j],\n",
    "                'rate': cm[i, j] / cm[i].sum(),\n",
    "            })\n",
    "\n",
    "# Sort by count\n",
    "confusion_pairs_sorted = sorted(confusion_pairs, key=lambda x: x['count'], reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 Confusion Pairs:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Trueâ†’Pred':<15} {'Count':<10} {'Rate':<10}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for pair in confusion_pairs_sorted[:10]:\n",
    "    print(f\"{pair['true']}â†’{pair['pred']:<12} {pair['count']:<10} {pair['rate']:<10.4f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26883d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize examples of most common confusion\n",
    "if confusion_pairs_sorted:\n",
    "    top_confusion = confusion_pairs_sorted[0]\n",
    "    true_class = top_confusion['true']\n",
    "    pred_class = top_confusion['pred']\n",
    "    \n",
    "    print(f\"\\nMost common confusion: {true_class} â†’ {pred_class} ({top_confusion['count']} cases)\")\n",
    "    \n",
    "    # Find examples\n",
    "    confusion_mask = (all_labels == true_class) & (all_preds == pred_class)\n",
    "    confusion_indices = np.where(confusion_mask)[0]\n",
    "    \n",
    "    # Show examples\n",
    "    n_examples = min(12, len(confusion_indices))\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, idx in enumerate(confusion_indices[:n_examples]):\n",
    "        img, true_label = test_dataset[idx]\n",
    "        img_display = img.squeeze().numpy()\n",
    "        confidence = confidences[idx]\n",
    "        \n",
    "        axes[i].imshow(img_display, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"True: {true_label}, Pred: {pred_class}\\nConf: {confidence:.3f}\",\n",
    "                         fontsize=10)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_examples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Examples of {true_class}â†’{pred_class} Confusion', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(reports_dir / f\"confusion_{true_class}_to_{pred_class}.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ“ Saved to {reports_dir / f'confusion_{true_class}_to_{pred_class}.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d4a80",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### âœ… Evaluation Best Practices\n",
    "\n",
    "1. **Beyond accuracy**: Always check confusion matrix and per-class metrics\n",
    "2. **Error inspection**: Visualize failures to find patterns\n",
    "3. **Slice analysis**: Identify problematic subgroups\n",
    "4. **Track experiments**: Save metrics to JSON for comparison\n",
    "5. **Systematic debugging**: Look for class imbalances, systematic confusions, data quality issues\n",
    "\n",
    "### âš ï¸ Common Evaluation Mistakes\n",
    "\n",
    "1. **Only looking at accuracy**: Hides class imbalance and failure modes\n",
    "2. **Ignoring confident errors**: High-confidence mistakes are the most dangerous\n",
    "3. **Not inspecting failures**: Patterns in errors guide improvements\n",
    "4. **Averaging over classes**: Per-class metrics reveal weak points\n",
    "5. **No experiment tracking**: Can't compare models without saved metrics\n",
    "\n",
    "### ðŸ”§ Error Analysis Workflow\n",
    "\n",
    "```python\n",
    "# 1. Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "# â†’ Identify systematic confusions\n",
    "\n",
    "# 2. Per-class metrics\n",
    "report = classification_report(y_true, y_pred)\n",
    "# â†’ Find weak classes (low precision/recall)\n",
    "\n",
    "# 3. Error inspection\n",
    "errors = y_pred != y_true\n",
    "visualize(X[errors][:20])  # Top 20 errors\n",
    "# â†’ Look for patterns (rotation, noise, confusions)\n",
    "\n",
    "# 4. Slice analysis\n",
    "for class_id in range(n_classes):\n",
    "    error_rate = errors[y_true == class_id].mean()\n",
    "# â†’ Identify problematic slices\n",
    "\n",
    "# 5. Save metrics\n",
    "save_metrics(metrics, f\"experiment_{name}.json\")\n",
    "# â†’ Track progress over time\n",
    "```\n",
    "\n",
    "### ðŸ“Š Key Metrics to Monitor\n",
    "\n",
    "| Metric | What It Tells You | When to Worry |\n",
    "|--------|-------------------|---------------|\n",
    "| **Accuracy** | Overall correctness | Doesn't show class imbalance |\n",
    "| **Precision** | False positive rate | Low precision = too many false alarms |\n",
    "| **Recall** | False negative rate | Low recall = missing many positives |\n",
    "| **F1 Score** | Balanced metric | Trade-off between precision/recall |\n",
    "| **Confusion Matrix** | Systematic confusions | Off-diagonal bright spots |\n",
    "| **Error Rate** | Failure rate | High error on specific slices |\n",
    "| **Confidence** | Model uncertainty | High-confidence errors = dangerous |\n",
    "\n",
    "### ðŸŽ¯ Action Items from Error Analysis\n",
    "\n",
    "**If you find...**\n",
    "- **Class imbalance**: Resample data or use weighted loss\n",
    "- **Systematic confusions** (e.g., 4â†”9): Add augmentation, collect more data for those pairs\n",
    "- **High error on specific class**: More training data, better features, check label quality\n",
    "- **Confident errors**: Model is overconfident â†’ calibration needed (temperature scaling)\n",
    "- **Ambiguous samples**: Consider rejecting low-confidence predictions in production\n",
    "- **Data quality issues**: Clean labels, remove corrupted samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e6ef0",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27710bce",
   "metadata": {},
   "source": [
    "### Exercise 1: Calibration Analysis\n",
    "\n",
    "Plot a calibration curve: For each confidence bin (0-0.1, 0.1-0.2, ..., 0.9-1.0), compute actual accuracy. Well-calibrated models have accuracy â‰ˆ confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bd7e81",
   "metadata": {},
   "source": [
    "### Exercise 2: Top-K Accuracy\n",
    "\n",
    "Compute top-2 and top-3 accuracy (prediction correct if true label in top-K predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b8292",
   "metadata": {},
   "source": [
    "### Exercise 3: Error Clustering\n",
    "\n",
    "Among the errors, find the 5 pairs of most similar images (by L2 distance). Do they have similar confusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ac6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b602b6b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solutions</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7adc2d5",
   "metadata": {},
   "source": [
    "### Solution 1: Calibration Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb13ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration analysis\n",
    "bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "bin_accuracies = []\n",
    "bin_confidences = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(len(bins) - 1):\n",
    "    mask = (confidences >= bins[i]) & (confidences < bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        bin_acc = (all_preds[mask] == all_labels[mask]).mean()\n",
    "        bin_conf = confidences[mask].mean()\n",
    "        bin_accuracies.append(bin_acc)\n",
    "        bin_confidences.append(bin_conf)\n",
    "        bin_counts.append(mask.sum())\n",
    "    else:\n",
    "        bin_accuracies.append(0)\n",
    "        bin_confidences.append(bin_centers[i])\n",
    "        bin_counts.append(0)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Calibration curve\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "axes[0].plot(bin_confidences, bin_accuracies, 'o-', linewidth=2, markersize=8, label='Model')\n",
    "axes[0].set_xlabel('Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Calibration Curve', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sample distribution\n",
    "axes[1].bar(bin_centers, bin_counts, width=0.08, alpha=0.7)\n",
    "axes[1].set_xlabel('Confidence', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Samples', fontsize=12)\n",
    "axes[1].set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"calibration_curve.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Compute Expected Calibration Error (ECE)\n",
    "ece = sum([(bin_counts[i] / len(all_labels)) * abs(bin_accuracies[i] - bin_confidences[i]) \n",
    "           for i in range(len(bins)-1)])\n",
    "print(f\"\\nExpected Calibration Error (ECE): {ece:.4f}\")\n",
    "print(\"(Lower is better, 0 = perfect calibration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a61ec61",
   "metadata": {},
   "source": [
    "### Solution 2: Top-K Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae86f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K accuracy\n",
    "def top_k_accuracy(probs, labels, k):\n",
    "    \"\"\"Compute top-K accuracy.\"\"\"\n",
    "    top_k_preds = np.argsort(probs, axis=1)[:, -k:]  # Top K predictions\n",
    "    correct = np.any(top_k_preds == labels[:, None], axis=1)\n",
    "    return correct.mean()\n",
    "\n",
    "top1_acc = top_k_accuracy(all_probs, all_labels, k=1)\n",
    "top2_acc = top_k_accuracy(all_probs, all_labels, k=2)\n",
    "top3_acc = top_k_accuracy(all_probs, all_labels, k=3)\n",
    "top5_acc = top_k_accuracy(all_probs, all_labels, k=5)\n",
    "\n",
    "print(\"\\nTop-K Accuracy:\")\n",
    "print(f\"  Top-1: {top1_acc:.4f}\")\n",
    "print(f\"  Top-2: {top2_acc:.4f}\")\n",
    "print(f\"  Top-3: {top3_acc:.4f}\")\n",
    "print(f\"  Top-5: {top5_acc:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "k_values = [1, 2, 3, 5]\n",
    "accuracies = [top1_acc, top2_acc, top3_acc, top5_acc]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, 'o-', linewidth=2, markersize=10)\n",
    "plt.xlabel('K (Top-K)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Top-K Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "plt.ylim([0.95, 1.0])\n",
    "plt.savefig(reports_dir / \"topk_accuracy.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâ†’ Top-2 includes {(top2_acc - top1_acc) * 100:.2f}% more samples than Top-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa494e",
   "metadata": {},
   "source": [
    "### Solution 3: Error Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6aa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get error images as vectors\n",
    "error_images = []\n",
    "for idx in error_indices:\n",
    "    img, _ = test_dataset[idx]\n",
    "    error_images.append(img.flatten().numpy())\n",
    "\n",
    "error_images = np.array(error_images)\n",
    "\n",
    "print(f\"Computing pairwise distances for {len(error_images)} errors...\")\n",
    "\n",
    "# Compute pairwise L2 distances (sample for speed)\n",
    "n_sample = min(200, len(error_images))\n",
    "sample_indices = np.random.choice(len(error_images), n_sample, replace=False)\n",
    "error_images_sample = error_images[sample_indices]\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "distances = cdist(error_images_sample, error_images_sample, metric='euclidean')\n",
    "\n",
    "# Find 5 most similar pairs (excluding diagonal)\n",
    "np.fill_diagonal(distances, np.inf)\n",
    "similar_pairs = []\n",
    "\n",
    "for _ in range(5):\n",
    "    min_idx = np.unravel_index(np.argmin(distances), distances.shape)\n",
    "    i, j = min_idx\n",
    "    dist = distances[i, j]\n",
    "    \n",
    "    # Map back to original error indices\n",
    "    orig_i = error_indices[sample_indices[i]]\n",
    "    orig_j = error_indices[sample_indices[j]]\n",
    "    \n",
    "    similar_pairs.append((orig_i, orig_j, dist))\n",
    "    \n",
    "    # Mark as used\n",
    "    distances[i, :] = np.inf\n",
    "    distances[:, i] = np.inf\n",
    "    distances[j, :] = np.inf\n",
    "    distances[:, j] = np.inf\n",
    "\n",
    "# Visualize similar error pairs\n",
    "fig, axes = plt.subplots(5, 2, figsize=(8, 16))\n",
    "\n",
    "for pair_idx, (idx1, idx2, dist) in enumerate(similar_pairs):\n",
    "    img1, true1 = test_dataset[idx1]\n",
    "    img2, true2 = test_dataset[idx2]\n",
    "    pred1 = all_preds[idx1]\n",
    "    pred2 = all_preds[idx2]\n",
    "    \n",
    "    axes[pair_idx, 0].imshow(img1.squeeze().numpy(), cmap='gray')\n",
    "    axes[pair_idx, 0].axis('off')\n",
    "    axes[pair_idx, 0].set_title(f\"True: {true1}, Pred: {pred1}\", fontsize=10)\n",
    "    \n",
    "    axes[pair_idx, 1].imshow(img2.squeeze().numpy(), cmap='gray')\n",
    "    axes[pair_idx, 1].axis('off')\n",
    "    axes[pair_idx, 1].set_title(f\"True: {true2}, Pred: {pred2}\\nDist: {dist:.2f}\", fontsize=10)\n",
    "\n",
    "plt.suptitle('Top 5 Most Similar Error Pairs', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"similar_errors.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Saved to {reports_dir / 'similar_errors.png'}\")\n",
    "print(\"\\nâ†’ Notice if similar-looking errors have similar confusions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33abd31",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e7ab0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You've now completed the core Module 06 notebooks! You know:\n",
    "1. **Reproducibility** - Deterministic training with seeds\n",
    "2. **Debugging** - Overfit tiny batch test\n",
    "3. **Optimization** - LR, schedules, regularization\n",
    "4. **Evaluation** - Confusion matrix, error analysis\n",
    "\n",
    "**Optional**: Notebook 05 on model export (TorchScript/ONNX) for deployment.\n",
    "\n",
    "**Remember the workflow**:\n",
    "```\n",
    "1. Overfit test â†’ 2. Train full model â†’ 3. Error analysis â†’ 4. Improve based on patterns\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
