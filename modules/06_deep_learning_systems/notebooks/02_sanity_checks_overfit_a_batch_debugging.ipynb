{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51857dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "repo_root = Path.cwd().parent.parent.parent\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "SimpleMLP = safe_import_from('06_deep_learning_systems.src.models', 'SimpleMLP')\n",
    "CNNMnist = safe_import_from('06_deep_learning_systems.src.models', 'CNNMnist')\n",
    "train_on_tiny_batch = safe_import_from('06_deep_learning_systems.src.trainer', 'train_on_tiny_batch')\n",
    "create_tiny_dataset = safe_import_from('06_deep_learning_systems.src.datasets', 'create_tiny_dataset')\n",
    "get_mnist_loaders = safe_import_from('06_deep_learning_systems.src.datasets', 'get_mnist_loaders')\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core.seeding', 'set_seed')\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "\n",
    "# Setup\n",
    "reports_dir = Path(\"../reports/notebook_02\")\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd4528",
   "metadata": {},
   "source": [
    "## Part 1: The Overfit Test - What It Means\n",
    "\n",
    "**Question**: Before debugging anything complex, can your model memorize 32 samples?\n",
    "\n",
    "**Why this matters**:\n",
    "- Tests that **gradients flow** (not zero/NaN)\n",
    "- Tests that **loss function** is correct\n",
    "- Tests that **data pipeline** works\n",
    "- Tests that **model has capacity**\n",
    "- Tests that **labels match inputs**\n",
    "\n",
    "**Expected outcome**: Loss ‚Üí 0.001 within 100-200 steps, 100% accuracy\n",
    "\n",
    "If this fails, **don't waste time tuning hyperparameters** - there's a bug!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f44869",
   "metadata": {},
   "source": [
    "## Part 2: Healthy Overfit - Reference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tiny dataset\n",
    "print(\"Creating tiny dataset (32 samples)...\")\n",
    "X, y = create_tiny_dataset(n_samples=32, n_features=10, n_classes=2, seed=42)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Classes: {y.unique()}\")\n",
    "print(f\"Class distribution: {[(y == c).sum().item() for c in y.unique()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba3a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with sufficient capacity\n",
    "model = SimpleMLP(input_dim=10, hidden_dims=[32, 32], output_dim=2)\n",
    "print(f\"\\nModel: {model.__class__.__name__}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Overfit\n",
    "print(\"\\nOverfitting tiny batch...\")\n",
    "model, losses = train_on_tiny_batch(\n",
    "    model, X, y,\n",
    "    num_steps=200,\n",
    "    lr=1e-2,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"\\nInitial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "print(f\"Reduction: {(1 - losses[-1]/losses[0])*100:.1f}%\")\n",
    "\n",
    "# Check accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X).argmax(dim=1)\n",
    "    accuracy = (preds == y).float().mean().item()\n",
    "print(f\"Final accuracy: {accuracy:.2%}\")\n",
    "\n",
    "if losses[-1] < 0.01 and accuracy == 1.0:\n",
    "    print(\"\\n‚úÖ HEALTHY: Model successfully overfitted!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Model didn't overfit perfectly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc361715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot healthy overfit curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2, color='steelblue')\n",
    "plt.axhline(0.01, color='red', linestyle='--', linewidth=2, label='Target (<0.01)')\n",
    "plt.xlabel('Step', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Healthy Overfit: Rapid Loss Collapse', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"healthy_overfit.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Saved to {reports_dir / 'healthy_overfit.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28da17",
   "metadata": {},
   "source": [
    "## Part 3: Debugging - When Overfit Fails\n",
    "\n",
    "Let's intentionally break things and learn how to diagnose each problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c9215",
   "metadata": {},
   "source": [
    "### Bug 1: Model Too Small (Insufficient Capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bug 1: Tiny model (1 hidden unit)\\n\")\n",
    "tiny_model = SimpleMLP(input_dim=10, hidden_dims=[1], output_dim=2)  # Only 1 unit!\n",
    "print(f\"Parameters: {sum(p.numel() for p in tiny_model.parameters())}\")\n",
    "\n",
    "tiny_model, tiny_losses = train_on_tiny_batch(\n",
    "    tiny_model, X, y,\n",
    "    num_steps=200,\n",
    "    lr=1e-2,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal loss: {tiny_losses[-1]:.4f}\")\n",
    "print(\"‚Üí Loss plateaus high - model lacks capacity to memorize\")\n",
    "print(\"Fix: Increase hidden_dims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d72be",
   "metadata": {},
   "source": [
    "### Bug 2: Learning Rate Too Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBug 2: Learning rate too low\\n\")\n",
    "model_lowlr = SimpleMLP(input_dim=10, hidden_dims=[32, 32], output_dim=2)\n",
    "\n",
    "model_lowlr, lowlr_losses = train_on_tiny_batch(\n",
    "    model_lowlr, X, y,\n",
    "    num_steps=200,\n",
    "    lr=1e-6,  # Too low!\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Final loss: {lowlr_losses[-1]:.4f}\")\n",
    "print(\"‚Üí Loss decreases very slowly\")\n",
    "print(\"Fix: Increase learning rate (try 1e-3 to 1e-2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5c76",
   "metadata": {},
   "source": [
    "### Bug 3: Wrong Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1431760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBug 3: Randomly shuffled labels (no signal)\\n\")\n",
    "y_random = torch.randint(0, 2, (32,))  # Random labels!\n",
    "\n",
    "model_random = SimpleMLP(input_dim=10, hidden_dims=[32, 32], output_dim=2)\n",
    "model_random, random_losses = train_on_tiny_batch(\n",
    "    model_random, X, y_random,\n",
    "    num_steps=200,\n",
    "    lr=1e-2,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Final loss: {random_losses[-1]:.4f}\")\n",
    "print(\"‚Üí Can still memorize, but takes longer\")\n",
    "print(\"Fix: Verify labels match inputs (inspect samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all failure modes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Healthy\n",
    "axes[0, 0].plot(losses, linewidth=2, color='green')\n",
    "axes[0, 0].axhline(0.01, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].set_title('‚úÖ Healthy: Rapid Collapse', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bug 1: Tiny model\n",
    "axes[0, 1].plot(tiny_losses, linewidth=2, color='orange')\n",
    "axes[0, 1].axhline(0.01, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].set_title('‚ùå Bug 1: Model Too Small', fontweight='bold')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Bug 2: LR too low\n",
    "axes[1, 0].plot(lowlr_losses, linewidth=2, color='blue')\n",
    "axes[1, 0].axhline(0.01, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].set_title('‚ùå Bug 2: LR Too Low', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bug 3: Random labels\n",
    "axes[1, 1].plot(random_losses, linewidth=2, color='purple')\n",
    "axes[1, 1].axhline(0.01, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_title('‚ö†Ô∏è Bug 3: Random Labels', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"failure_modes.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Saved to {reports_dir / 'failure_modes.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d57b51",
   "metadata": {},
   "source": [
    "## Part 4: Gradient Diagnostics\n",
    "\n",
    "When overfitting fails, check gradients first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(model, X, y):\n",
    "    \"\"\"Diagnostic function to check gradient health.\"\"\"\n",
    "    model.train()\n",
    "    output = model(X)\n",
    "    loss = nn.CrossEntropyLoss()(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"Gradient Check:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_mean = param.grad.mean().item()\n",
    "            grad_std = param.grad.std().item()\n",
    "            has_nan = torch.isnan(param.grad).any().item()\n",
    "            has_inf = torch.isinf(param.grad).any().item()\n",
    "            \n",
    "            status = \"‚úì\" if not (has_nan or has_inf) and grad_norm > 0 else \"‚úó\"\n",
    "            \n",
    "            print(f\"{status} {name:30s} norm={grad_norm:.6f} mean={grad_mean:.6f} std={grad_std:.6f}\")\n",
    "            \n",
    "            if has_nan:\n",
    "                print(f\"   ‚ö†Ô∏è NaN detected in {name}\")\n",
    "            if has_inf:\n",
    "                print(f\"   ‚ö†Ô∏è Inf detected in {name}\")\n",
    "            if grad_norm == 0:\n",
    "                print(f\"   ‚ö†Ô∏è Zero gradient in {name} (dead neuron?)\")\n",
    "        else:\n",
    "            print(f\"‚úó {name:30s} NO GRADIENT\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Test on healthy model\n",
    "model_test = SimpleMLP(input_dim=10, hidden_dims=[32, 32], output_dim=2)\n",
    "check_gradients(model_test, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb765ab",
   "metadata": {},
   "source": [
    "## Part 5: Data Inspection\n",
    "\n",
    "Always inspect your data before blaming the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real MNIST for inspection\n",
    "print(\"Loading MNIST for inspection...\")\n",
    "train_loader, _, _ = get_mnist_loaders(\n",
    "    data_dir=Path(\"../../../data\"),\n",
    "    batch_size=16,\n",
    "    val_split=0.1,\n",
    "    num_workers=0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Get one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nBatch info:\")\n",
    "print(f\"  Images shape: {images.shape}\")\n",
    "print(f\"  Labels shape: {labels.shape}\")\n",
    "print(f\"  Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"  Labels: {labels.tolist()}\")\n",
    "print(f\"  Unique labels: {labels.unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83879aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "    img = images[i].squeeze()  # Remove channel dim\n",
    "    label = labels[i].item()\n",
    "    \n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f'Label: {label}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Data Inspection: First 16 Samples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / \"data_inspection.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Saved to {reports_dir / 'data_inspection.png'}\")\n",
    "print(\"\\nChecklist:\")\n",
    "print(\"  [‚úì] Images look correct\")\n",
    "print(\"  [‚úì] Labels match visual content\")\n",
    "print(\"  [‚úì] Data is normalized\")\n",
    "print(\"  [‚úì] No obvious corruption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89519a",
   "metadata": {},
   "source": [
    "## Part 6: Create Debugging Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95578f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = \"\"\"# Deep Learning Debugging Checklist\n",
    "\n",
    "## When Training Fails (High Loss / No Learning)\n",
    "\n",
    "### 1. Can you overfit a tiny batch? (32-128 samples)\n",
    "- [ ] Loss goes to <0.01 in 100-200 steps?\n",
    "- [ ] 100% accuracy on the tiny batch?\n",
    "- If NO ‚Üí systematic debugging below\n",
    "\n",
    "### 2. Check Gradients\n",
    "- [ ] Gradients exist for all parameters?\n",
    "- [ ] No NaN or Inf in gradients?\n",
    "- [ ] Gradients are non-zero (not all dead neurons)?\n",
    "- [ ] Gradient magnitudes are reasonable (1e-5 to 1e-2)?\n",
    "\n",
    "### 3. Inspect Data\n",
    "- [ ] Visualize 10-20 samples - do they look correct?\n",
    "- [ ] Labels match the visual content?\n",
    "- [ ] Input range is reasonable (normalized)?\n",
    "- [ ] No obvious corruption or NaN values?\n",
    "- [ ] Class balance reasonable (not 99%/1%)?\n",
    "\n",
    "### 4. Verify Model & Loss\n",
    "- [ ] Model has sufficient capacity (not 1-2 neurons)?\n",
    "- [ ] Output shape matches expected (batch_size, num_classes)?\n",
    "- [ ] Loss function matches task (CrossEntropy for classification)?\n",
    "- [ ] No gradient blocking (e.g., detach(), .data)?\n",
    "\n",
    "### 5. Check Learning Rate\n",
    "- [ ] Not too high (loss explodes ‚Üí Inf)?\n",
    "- [ ] Not too low (loss barely decreases)?\n",
    "- [ ] Typical range: 1e-4 to 1e-2 for Adam\n",
    "\n",
    "### 6. Verify Optimizer\n",
    "- [ ] Optimizer has model.parameters()?\n",
    "- [ ] optimizer.zero_grad() called before backward()?\n",
    "- [ ] optimizer.step() called after backward()?\n",
    "\n",
    "### 7. Check Data Pipeline\n",
    "- [ ] DataLoader shuffles training data?\n",
    "- [ ] Transforms applied correctly?\n",
    "- [ ] Batch size not too small (<8) or too large?\n",
    "\n",
    "## Common Bugs & Fixes\n",
    "\n",
    "| Symptom | Likely Cause | Fix |\n",
    "|---------|--------------|-----|\n",
    "| Loss = NaN | Exploding gradients or bad data | Lower LR, check for Inf/NaN in data |\n",
    "| Loss plateaus high | Model too small or LR too low | Increase capacity or LR |\n",
    "| Loss very slow | LR too low | Increase LR by 10x |\n",
    "| Loss explodes | LR too high | Decrease LR by 10x |\n",
    "| Zero gradients | Dead neurons (e.g., ReLU) | Check activations, try LeakyReLU |\n",
    "| Can't overfit 32 samples | Fundamental bug | Systematic debugging above |\n",
    "\n",
    "## When to Use This Checklist\n",
    "\n",
    "1. **Starting a new model** ‚Üí Run overfit test first\n",
    "2. **Training fails** ‚Üí Work through checklist top to bottom\n",
    "3. **Before tuning hyperparameters** ‚Üí Verify overfit test passes\n",
    "4. **When reproducing papers** ‚Üí Overfit on their data first\n",
    "\n",
    "## Remember\n",
    "\n",
    "> \"If your model can't overfit 32 samples, don't waste time tuning hyperparameters. \n",
    "> There's a bug in your code.\"\n",
    "\n",
    "Generated: {date}\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "checklist = checklist.format(date=datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "with open(reports_dir / \"debugging_checklist.md\", 'w') as f:\n",
    "    f.write(checklist)\n",
    "\n",
    "print(f\"‚úì Saved checklist to {reports_dir / 'debugging_checklist.md'}\")\n",
    "print(\"\\n\" + checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b74f1f4",
   "metadata": {},
   "source": [
    "## Part 7: Save Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56490a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"experiment\": \"overfit_sanity_checks\",\n",
    "    \"healthy_overfit\": {\n",
    "        \"initial_loss\": float(losses[0]),\n",
    "        \"final_loss\": float(losses[-1]),\n",
    "        \"reduction_percent\": float((1 - losses[-1]/losses[0]) * 100),\n",
    "        \"steps\": len(losses),\n",
    "        \"final_accuracy\": float(accuracy),\n",
    "        \"passed\": bool(losses[-1] < 0.01 and accuracy == 1.0),\n",
    "    },\n",
    "    \"failure_modes\": {\n",
    "        \"tiny_model\": {\"final_loss\": float(tiny_losses[-1]), \"converged\": bool(tiny_losses[-1] < 0.01)},\n",
    "        \"low_lr\": {\"final_loss\": float(lowlr_losses[-1]), \"converged\": bool(lowlr_losses[-1] < 0.01)},\n",
    "        \"random_labels\": {\"final_loss\": float(random_losses[-1]), \"converged\": bool(random_losses[-1] < 0.01)},\n",
    "    },\n",
    "    \"key_insight\": \"Always test overfit on 32 samples before debugging anything else\",\n",
    "}\n",
    "\n",
    "with open(reports_dir / \"summary.json\", 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved summary to {reports_dir / 'summary.json'}\")\n",
    "print(\"\\nSummary:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c12f50",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### ‚úÖ The Golden Rule\n",
    "\n",
    "**\"Can your model overfit 32 samples in <200 steps?\"**\n",
    "- If YES ‚Üí Model is fundamentally working, tune hyperparameters\n",
    "- If NO ‚Üí There's a bug, don't waste time tuning\n",
    "\n",
    "### üîß Debugging Priority\n",
    "\n",
    "1. **Overfit test** (this notebook)\n",
    "2. **Check gradients** (are they non-zero, finite?)\n",
    "3. **Inspect data** (does it look correct?)\n",
    "4. **Verify loss & model** (correct for task?)\n",
    "5. **Then and only then** ‚Üí tune hyperparameters\n",
    "\n",
    "### ‚ö†Ô∏è Common Mistakes\n",
    "\n",
    "1. **Skipping the overfit test** ‚Üí waste hours debugging wrong things\n",
    "2. **Tuning hyperparameters when fundamentals are broken**\n",
    "3. **Not inspecting data visually**\n",
    "4. **Assuming gradients are fine without checking**\n",
    "\n",
    "### üìã Your Debugging Workflow\n",
    "\n",
    "```python\n",
    "# Step 1: Overfit test (ALWAYS FIRST)\n",
    "X_tiny, y_tiny = get_tiny_batch(32)\n",
    "model, losses = train_on_tiny_batch(model, X_tiny, y_tiny, steps=200)\n",
    "assert losses[-1] < 0.01, \"Overfit test failed - debug before continuing!\"\n",
    "\n",
    "# Step 2: If overfit fails ‚Üí check gradients\n",
    "check_gradients(model, X_tiny, y_tiny)\n",
    "\n",
    "# Step 3: Inspect data\n",
    "visualize_samples(X_tiny, y_tiny)\n",
    "\n",
    "# Step 4: Only after passing overfit ‚Üí train full dataset\n",
    "history = trainer.train(train_loader, val_loader)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05e34b",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1356910",
   "metadata": {},
   "source": [
    "### Exercise 1: Introduce and Fix a Bug\n",
    "\n",
    "Create a model that **fails** the overfit test due to a bug of your choice. Then diagnose and fix it using the checklist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ab754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Ideas: wrong loss function, zero learning rate, broken model, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94107823",
   "metadata": {},
   "source": [
    "### Exercise 2: Minimum Model Capacity\n",
    "\n",
    "Find the **smallest** model (fewest parameters) that can still overfit the 32-sample batch. Start with `[1]` hidden units and increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f21d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc0e0c4",
   "metadata": {},
   "source": [
    "### Exercise 3: Gradient Explosion\n",
    "\n",
    "Create a scenario where gradients **explode** (become Inf). Then fix it with gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73baea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use very high learning rate or deep network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c6f29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solutions</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce37d8a",
   "metadata": {},
   "source": [
    "### Solution 1: Wrong Loss Function Bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug: Using MSE instead of CrossEntropy for classification\n",
    "print(\"Creating bug: Wrong loss function (MSE for classification)\\n\")\n",
    "\n",
    "model_bug = SimpleMLP(10, [32, 32], 2)\n",
    "optimizer = torch.optim.Adam(model_bug.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()  # WRONG! Should be CrossEntropyLoss\n",
    "\n",
    "losses_bug = []\n",
    "for step in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = model_bug(X)\n",
    "    loss = criterion(output, y.float())  # MSE needs float\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses_bug.append(loss.item())\n",
    "\n",
    "print(f\"Final loss with MSE: {losses_bug[-1]:.4f}\")\n",
    "print(\"‚Üí Doesn't converge well!\\n\")\n",
    "\n",
    "# Fix: Use correct loss\n",
    "print(\"Fix: Using CrossEntropyLoss\\n\")\n",
    "model_fix = SimpleMLP(10, [32, 32], 2)\n",
    "model_fix, losses_fix = train_on_tiny_batch(model_fix, X, y, 200, 1e-2, \"cpu\")\n",
    "\n",
    "print(f\"Final loss with CrossEntropy: {losses_fix[-1]:.6f}\")\n",
    "print(\"‚úì Fixed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486f549",
   "metadata": {},
   "source": [
    "### Solution 2: Minimum Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be17f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum capacity\n",
    "hidden_sizes = [1, 2, 4, 8, 16, 32]\n",
    "results = []\n",
    "\n",
    "for hidden_dim in hidden_sizes:\n",
    "    model_test = SimpleMLP(10, [hidden_dim], 2)\n",
    "    model_test, test_losses = train_on_tiny_batch(model_test, X, y, 200, 1e-2, \"cpu\")\n",
    "    final_loss = test_losses[-1]\n",
    "    passed = final_loss < 0.01\n",
    "    results.append((hidden_dim, final_loss, passed))\n",
    "    print(f\"Hidden={hidden_dim:2d}: final_loss={final_loss:.6f} {'‚úì' if passed else '‚úó'}\")\n",
    "\n",
    "# Find minimum that works\n",
    "min_working = min([h for h, l, p in results if p])\n",
    "print(f\"\\n‚Üí Minimum capacity: {min_working} hidden units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7cca3",
   "metadata": {},
   "source": [
    "### Solution 3: Gradient Explosion & Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cause gradient explosion with very high LR\n",
    "print(\"Creating gradient explosion (LR=1.0)\\n\")\n",
    "model_explode = SimpleMLP(10, [32, 32], 2)\n",
    "optimizer = torch.optim.SGD(model_explode.parameters(), lr=1.0)  # Too high!\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "explosion_losses = []\n",
    "for step in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = model_explode(X)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check for explosion\n",
    "    grad_norm = sum(p.grad.norm().item()**2 for p in model_explode.parameters())**0.5\n",
    "    \n",
    "    optimizer.step()\n",
    "    explosion_losses.append(loss.item())\n",
    "    \n",
    "    print(f\"Step {step}: loss={loss.item():.4f}, grad_norm={grad_norm:.2f}\")\n",
    "    \n",
    "    if grad_norm > 1000:\n",
    "        print(\"‚Üí Gradients exploded!\\n\")\n",
    "        break\n",
    "\n",
    "# Fix with gradient clipping\n",
    "print(\"Fix: Gradient clipping (max_norm=1.0)\\n\")\n",
    "model_clip = SimpleMLP(10, [32, 32], 2)\n",
    "optimizer = torch.optim.SGD(model_clip.parameters(), lr=1.0)\n",
    "\n",
    "clip_losses = []\n",
    "for step in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model_clip(X)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model_clip.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    clip_losses.append(loss.item())\n",
    "\n",
    "print(f\"Final loss with clipping: {clip_losses[-1]:.4f}\")\n",
    "print(\"‚úì Stable training with gradient clipping!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba303590",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043fc40e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 03**: Optimization dynamics - compare optimizers, LR schedules, regularization\n",
    "- **Notebook 04**: Monitoring & error analysis - confusion matrices, worst errors\n",
    "\n",
    "**Remember**: The overfit test is your **first line of defense**. Run it before debugging anything else!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
