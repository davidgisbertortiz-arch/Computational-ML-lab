{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e31d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "repo_root = Path().resolve().parents[2]\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, confusion_matrix,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core.seeding', 'set_seed')\n",
    "load_data, get_feature_columns, split_data = safe_import_from(\n",
    "    '03_ml_tabular_foundations.src.data',\n",
    "    'load_data', 'get_feature_columns', 'split_data'\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "reports_dir = Path(\"../reports\")\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e8427",
   "metadata": {},
   "source": [
    "## 1. Why Baselines Matter\n",
    "\n",
    "**Baseline**: A simple model that provides a reference point.\n",
    "\n",
    "**Purpose**:\n",
    "- Establish minimum acceptable performance\n",
    "- Quantify value of complex models\n",
    "- Sanity check: complex model must beat baseline\n",
    "- Debug: if baseline fails, data may be broken\n",
    "\n",
    "**Types of baselines**:\n",
    "1. **Constant predictor**: Predict majority class (classification) or mean (regression)\n",
    "2. **Random predictor**: Random guessing\n",
    "3. **Linear model**: Logistic/linear regression (fast, interpretable)\n",
    "4. **Domain heuristic**: Physics-based rules (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "df = load_data()\n",
    "feature_cols = get_feature_columns(df)\n",
    "X = df[feature_cols].values\n",
    "y = df['is_signal'].values\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    X, y, test_size=0.2, val_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "print(f\"Class balance: {y.mean():.1%} signal, {1-y.mean():.1%} background\")\n",
    "print(f\"\\nSplit sizes: Train={len(y_train):,}, Val={len(y_val):,}, Test={len(y_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f0cb3",
   "metadata": {},
   "source": [
    "## 2. Baseline 1: Majority Class Predictor\n",
    "\n",
    "**Strategy**: Always predict the most common class.\n",
    "\n",
    "**When useful**: Establishes floor performance for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority class baseline\n",
    "majority_baseline = DummyClassifier(strategy='most_frequent')\n",
    "majority_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions (probabilities are always [0.9, 0.1] for background-heavy dataset)\n",
    "y_pred_train_maj = majority_baseline.predict(X_train)\n",
    "y_pred_val_maj = majority_baseline.predict(X_val)\n",
    "y_pred_test_maj = majority_baseline.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "acc_train_maj = accuracy_score(y_train, y_pred_train_maj)\n",
    "acc_val_maj = accuracy_score(y_val, y_pred_val_maj)\n",
    "acc_test_maj = accuracy_score(y_test, y_pred_test_maj)\n",
    "\n",
    "print(\"Baseline 1: Majority Class Predictor\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Strategy: Always predict class {majority_baseline.classes_[majority_baseline.class_prior_.argmax()]} (background)\")\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"  Train: {acc_train_maj:.1%}\")\n",
    "print(f\"  Val:   {acc_val_maj:.1%}\")\n",
    "print(f\"  Test:  {acc_test_maj:.1%}\")\n",
    "print(f\"\\nâš ï¸ Lesson: {acc_test_maj:.1%} accuracy by just guessing majority class!\")\n",
    "print(\"   â†’ Accuracy is MISLEADING for imbalanced data\")\n",
    "print(\"   â†’ Need better metrics: AUC-ROC, PR-AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8edf1",
   "metadata": {},
   "source": [
    "## 3. Baseline 2: Logistic Regression\n",
    "\n",
    "**Strategy**: Simple linear model (fast to train, interpretable).\n",
    "\n",
    "**Why it's a good baseline**:\n",
    "- Captures linear relationships\n",
    "- Provides probability estimates\n",
    "- Fast to train and predict\n",
    "- Interpretable coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eec978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression baseline (with proper pipeline)\n",
    "logistic_baseline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "logistic_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions (probabilities)\n",
    "y_pred_train_lr = logistic_baseline.predict_proba(X_train)[:, 1]\n",
    "y_pred_val_lr = logistic_baseline.predict_proba(X_val)[:, 1]\n",
    "y_pred_test_lr = logistic_baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute metrics\n",
    "def compute_classification_metrics(y_true, y_pred_proba, threshold=0.5):\n",
    "    \"\"\"Compute comprehensive classification metrics.\"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'F1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'AUC-ROC': roc_auc_score(y_true, y_pred_proba),\n",
    "        'PR-AUC': average_precision_score(y_true, y_pred_proba)\n",
    "    }\n",
    "\n",
    "metrics_train_lr = compute_classification_metrics(y_train, y_pred_train_lr)\n",
    "metrics_val_lr = compute_classification_metrics(y_val, y_pred_val_lr)\n",
    "metrics_test_lr = compute_classification_metrics(y_test, y_pred_test_lr)\n",
    "\n",
    "print(\"\\nBaseline 2: Logistic Regression\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<12} {'Train':<10} {'Val':<10} {'Test':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for metric in metrics_train_lr.keys():\n",
    "    print(f\"{metric:<12} {metrics_train_lr[metric]:<10.4f} \"\n",
    "          f\"{metrics_val_lr[metric]:<10.4f} {metrics_test_lr[metric]:<10.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Key Observations:\")\n",
    "print(f\"  â€¢ AUC-ROC = {metrics_test_lr['AUC-ROC']:.3f}: Model discriminates signal from background\")\n",
    "print(f\"  â€¢ PR-AUC = {metrics_test_lr['PR-AUC']:.3f}: Performance on minority class (signal)\")\n",
    "print(f\"  â€¢ Accuracy = {metrics_test_lr['Accuracy']:.3f}: Inflated by majority class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dfadee",
   "metadata": {},
   "source": [
    "## 4. Understanding Metrics for Imbalanced Data\n",
    "\n",
    "**Problem**: With 10% signal rate, predicting all background gives 90% accuracy!\n",
    "\n",
    "**Solution**: Use metrics that focus on discrimination ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54280970",
   "metadata": {},
   "source": [
    "### 4.1 ROC-AUC: Discrimination Ability\n",
    "\n",
    "**ROC Curve**: True Positive Rate vs. False Positive Rate at all thresholds.\n",
    "\n",
    "**AUC-ROC**:\n",
    "- Probability that model ranks random positive higher than random negative\n",
    "- Range: [0, 1], random = 0.5, perfect = 1.0\n",
    "- **Insensitive to class imbalance**\n",
    "\n",
    "**When to use**: Class balance doesn't strongly affect business goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_test_lr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(fpr, tpr, linewidth=2.5, label=f'Logistic (AUC={metrics_test_lr[\"AUC-ROC\"]:.3f})', \n",
    "        color='steelblue')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random (AUC=0.500)', alpha=0.6)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curve: Logistic Regression Baseline', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim([-0.02, 1.02])\n",
    "ax.set_ylim([-0.02, 1.02])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '03_roc_curve_baseline.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š ROC-AUC Interpretation:\")\n",
    "print(f\"  AUC = {metrics_test_lr['AUC-ROC']:.3f}\")\n",
    "print(f\"  â†’ {metrics_test_lr['AUC-ROC']*100:.1f}% chance model ranks signal higher than background\")\n",
    "print(f\"  â†’ Far better than random (0.500)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e9b45",
   "metadata": {},
   "source": [
    "### 4.2 PR-AUC: Performance on Minority Class\n",
    "\n",
    "**Precision-Recall Curve**: Precision vs. Recall at all thresholds.\n",
    "\n",
    "**PR-AUC**:\n",
    "- Average precision across recall levels\n",
    "- **Sensitive to class imbalance** (good for minority class focus)\n",
    "- Baseline = prevalence (10% in our case)\n",
    "\n",
    "**When to use**: Minority class is primary concern (fraud, disease, signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, y_pred_test_lr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(recall, precision, linewidth=2.5, \n",
    "        label=f'Logistic (PR-AUC={metrics_test_lr[\"PR-AUC\"]:.3f})', \n",
    "        color='coral')\n",
    "ax.axhline(y=y_test.mean(), color='k', linestyle='--', linewidth=1.5, \n",
    "           label=f'Random (PR-AUC={y_test.mean():.3f})', alpha=0.6)\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curve: Logistic Regression Baseline', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim([-0.02, 1.02])\n",
    "ax.set_ylim([-0.02, 1.02])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '03_pr_curve_baseline.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š PR-AUC Interpretation:\")\n",
    "print(f\"  PR-AUC = {metrics_test_lr['PR-AUC']:.3f}\")\n",
    "print(f\"  Baseline (random) = {y_test.mean():.3f} (prevalence)\")\n",
    "print(f\"  â†’ {(metrics_test_lr['PR-AUC'] - y_test.mean()) / y_test.mean() * 100:.0f}% improvement over random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5bd7f",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix: Understanding Errors\n",
    "\n",
    "**Confusion Matrix**: Breakdown of predictions into TP, TN, FP, FN.\n",
    "\n",
    "**Business interpretation**:\n",
    "- **False Positive (FP)**: Background misclassified as signal â†’ wasted resources\n",
    "- **False Negative (FN)**: Signal misclassified as background â†’ missed discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix at default threshold (0.5)\n",
    "y_pred_test_lr_binary = (y_pred_test_lr >= 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred_test_lr_binary)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(cm, cmap='Blues', alpha=0.8)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, f'{cm[i, j]:,}\\n({cm[i, j]/cm.sum()*100:.1f}%)',\n",
    "                      ha='center', va='center', fontsize=13, fontweight='bold',\n",
    "                      color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Pred: Background', 'Pred: Signal'], fontsize=11)\n",
    "ax.set_yticklabels(['True: Background', 'True: Signal'], fontsize=11)\n",
    "ax.set_title('Confusion Matrix (Threshold=0.5)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Count', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '03_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compute error rates\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fpr_val = fp / (fp + tn)\n",
    "fnr_val = fn / (fn + tp)\n",
    "\n",
    "print(\"ðŸ“Š Confusion Matrix Analysis:\")\n",
    "print(f\"  True Negatives (TN):  {tn:,} (correct background)\")\n",
    "print(f\"  False Positives (FP): {fp:,} (background â†’ signal)\")\n",
    "print(f\"  False Negatives (FN): {fn:,} (signal â†’ background)\")\n",
    "print(f\"  True Positives (TP):  {tp:,} (correct signal)\")\n",
    "print(f\"\\nError Rates:\")\n",
    "print(f\"  False Positive Rate: {fpr_val:.1%} (FP / all background)\")\n",
    "print(f\"  False Negative Rate: {fnr_val:.1%} (FN / all signal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a78360",
   "metadata": {},
   "source": [
    "## 6. Threshold Selection: Aligning Metrics with Decisions\n",
    "\n",
    "**Problem**: Default threshold (0.5) may not be optimal for business goal.\n",
    "\n",
    "**Example**: In particle physics, prefer high recall (catch all signals) even if precision suffers.\n",
    "\n",
    "**Solution**: Select threshold based on cost/benefit tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3a157",
   "metadata": {},
   "source": [
    "### 6.1 Scenario: Maximize Recall at 95% Precision\n",
    "\n",
    "**Goal**: Catch as many signal events as possible while maintaining 95% precision.\n",
    "\n",
    "**Why**: We can afford some background (cheap to filter later), but missing signal is expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3df3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold for target precision\n",
    "target_precision = 0.95\n",
    "\n",
    "# Compute precision/recall at all thresholds\n",
    "precision_all, recall_all, thresholds_all = precision_recall_curve(y_test, y_pred_test_lr)\n",
    "\n",
    "# Find threshold achieving target precision\n",
    "valid_idx = np.where(precision_all >= target_precision)[0]\n",
    "if len(valid_idx) > 0:\n",
    "    # Select threshold with highest recall among valid\n",
    "    best_idx = valid_idx[np.argmax(recall_all[valid_idx])]\n",
    "    optimal_threshold = thresholds_all[best_idx] if best_idx < len(thresholds_all) else 1.0\n",
    "    optimal_precision = precision_all[best_idx]\n",
    "    optimal_recall = recall_all[best_idx]\n",
    "else:\n",
    "    optimal_threshold = 1.0\n",
    "    optimal_precision = 1.0\n",
    "    optimal_recall = 0.0\n",
    "\n",
    "print(f\"Threshold Selection for P â‰¥ {target_precision:.0%}:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Optimal threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"  Achieved precision: {optimal_precision:.1%}\")\n",
    "print(f\"  Achieved recall: {optimal_recall:.1%}\")\n",
    "\n",
    "# Compare with default threshold\n",
    "y_pred_optimal = (y_pred_test_lr >= optimal_threshold).astype(int)\n",
    "metrics_optimal = {\n",
    "    'Precision': precision_score(y_test, y_pred_optimal),\n",
    "    'Recall': recall_score(y_test, y_pred_optimal),\n",
    "    'F1': f1_score(y_test, y_pred_optimal)\n",
    "}\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Default (0.5):  P={metrics_test_lr['Precision']:.3f}, R={metrics_test_lr['Recall']:.3f}, F1={metrics_test_lr['F1']:.3f}\")\n",
    "print(f\"  Optimal ({optimal_threshold:.3f}): P={metrics_optimal['Precision']:.3f}, R={metrics_optimal['Recall']:.3f}, F1={metrics_optimal['F1']:.3f}\")\n",
    "print(f\"\\nðŸ’¡ Trade-off: Sacrificed {(metrics_test_lr['Recall'] - metrics_optimal['Recall'])*100:.1f} percentage points recall\")\n",
    "print(f\"             to gain {(metrics_optimal['Precision'] - metrics_test_lr['Precision'])*100:.1f} percentage points precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd958033",
   "metadata": {},
   "source": [
    "## 7. Bootstrap Confidence Intervals\n",
    "\n",
    "**Problem**: Single metric value doesn't capture uncertainty.\n",
    "\n",
    "**Solution**: Bootstrap to estimate confidence interval.\n",
    "\n",
    "**Method**:\n",
    "1. Resample test set with replacement\n",
    "2. Compute metric on each resample\n",
    "3. Calculate 95% CI from distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap confidence intervals for AUC-ROC\n",
    "def bootstrap_metric(y_true, y_pred, metric_fn, n_bootstrap=1000, alpha=0.05):\n",
    "    \"\"\"Compute bootstrap confidence interval for a metric.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    scores = []\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        idx = np.random.randint(0, n_samples, n_samples)\n",
    "        y_true_boot = y_true[idx]\n",
    "        y_pred_boot = y_pred[idx]\n",
    "        \n",
    "        # Compute metric\n",
    "        if len(np.unique(y_true_boot)) > 1:  # Need both classes\n",
    "            score = metric_fn(y_true_boot, y_pred_boot)\n",
    "            scores.append(score)\n",
    "    \n",
    "    # Compute confidence interval\n",
    "    lower = np.percentile(scores, alpha/2 * 100)\n",
    "    upper = np.percentile(scores, (1 - alpha/2) * 100)\n",
    "    mean = np.mean(scores)\n",
    "    \n",
    "    return mean, lower, upper, scores\n",
    "\n",
    "# Bootstrap AUC-ROC\n",
    "auc_mean, auc_lower, auc_upper, auc_dist = bootstrap_metric(\n",
    "    y_test, y_pred_test_lr, roc_auc_score, n_bootstrap=1000\n",
    ")\n",
    "\n",
    "# Bootstrap PR-AUC\n",
    "pr_auc_mean, pr_auc_lower, pr_auc_upper, pr_auc_dist = bootstrap_metric(\n",
    "    y_test, y_pred_test_lr, average_precision_score, n_bootstrap=1000\n",
    ")\n",
    "\n",
    "print(\"Bootstrap 95% Confidence Intervals (n=1000):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"AUC-ROC:  {auc_mean:.4f} [{auc_lower:.4f}, {auc_upper:.4f}]\")\n",
    "print(f\"PR-AUC:   {pr_auc_mean:.4f} [{pr_auc_lower:.4f}, {pr_auc_upper:.4f}]\")\n",
    "\n",
    "# Visualize bootstrap distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.hist(auc_dist, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax1.axvline(auc_mean, color='red', linewidth=2, label=f'Mean={auc_mean:.4f}')\n",
    "ax1.axvline(auc_lower, color='orange', linewidth=2, linestyle='--', label=f'95% CI')\n",
    "ax1.axvline(auc_upper, color='orange', linewidth=2, linestyle='--')\n",
    "ax1.set_xlabel('AUC-ROC', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Bootstrap Distribution: AUC-ROC', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.hist(pr_auc_dist, bins=30, alpha=0.7, color='coral', edgecolor='black')\n",
    "ax2.axvline(pr_auc_mean, color='red', linewidth=2, label=f'Mean={pr_auc_mean:.4f}')\n",
    "ax2.axvline(pr_auc_lower, color='orange', linewidth=2, linestyle='--', label=f'95% CI')\n",
    "ax2.axvline(pr_auc_upper, color='orange', linewidth=2, linestyle='--')\n",
    "ax2.set_xlabel('PR-AUC', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Bootstrap Distribution: PR-AUC', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '03_bootstrap_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Interpretation:\")\n",
    "print(f\"  â€¢ We are 95% confident true AUC-ROC is in [{auc_lower:.4f}, {auc_upper:.4f}]\")\n",
    "print(f\"  â€¢ Width of CI: {auc_upper - auc_lower:.4f}\")\n",
    "print(f\"  â€¢ Narrow CI â†’ stable estimate (sufficient test data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d99845",
   "metadata": {},
   "source": [
    "## 8. Metrics Summary Table\n",
    "\n",
    "Consolidate all metrics for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics summary\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'Model': ['Majority Baseline', 'Logistic Regression'],\n",
    "    'Accuracy': [acc_test_maj, metrics_test_lr['Accuracy']],\n",
    "    'Precision': [0.0, metrics_test_lr['Precision']],\n",
    "    'Recall': [0.0, metrics_test_lr['Recall']],\n",
    "    'F1': [0.0, metrics_test_lr['F1']],\n",
    "    'AUC-ROC': [0.5, metrics_test_lr['AUC-ROC']],\n",
    "    'PR-AUC': [y_test.mean(), metrics_test_lr['PR-AUC']]\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ“Š Baseline Model Comparison (Test Set):\")\n",
    "print(\"=\" * 60)\n",
    "print(metrics_summary.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "metrics_summary.to_csv(reports_dir / '03_baseline_metrics.csv', index=False)\n",
    "print(f\"\\nâœ… Metrics saved to: {reports_dir / '03_baseline_metrics.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ccbab",
   "metadata": {},
   "source": [
    "## 9. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bb2ef",
   "metadata": {},
   "source": [
    "**Exercise 1**: Choose metrics for a business scenario\n",
    "\n",
    "You're building a medical diagnosis model for a rare disease (1% prevalence):\n",
    "- **False Negative**: Patient has disease but test says no â†’ missed treatment (HIGH COST)\n",
    "- **False Positive**: Patient doesn't have disease but test says yes â†’ unnecessary treatment (LOW COST)\n",
    "\n",
    "Which metrics should you prioritize? What threshold strategy would you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a960959",
   "metadata": {},
   "source": [
    "**Exercise 2**: Compute optimal threshold for custom cost function\n",
    "\n",
    "Given:\n",
    "- Cost of False Negative = $1000 (missed signal discovery)\n",
    "- Cost of False Positive = $10 (wasted detector readout)\n",
    "\n",
    "Write code to find the threshold that minimizes total cost on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation here:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98044067",
   "metadata": {},
   "source": [
    "**Exercise 3**: When does RÂ² fail?\n",
    "\n",
    "Consider a regression problem predicting house prices. You have two models:\n",
    "- Model A: RÂ² = 0.85, predictions always within 20% of true price\n",
    "- Model B: RÂ² = 0.90, but occasionally predicts negative prices\n",
    "\n",
    "Which model would you deploy? Why? What additional metrics would you check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96eaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb8d44",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cbe98",
   "metadata": {},
   "source": [
    "**Solution 1**: Medical diagnosis metrics\n",
    "\n",
    "**Prioritized metrics**:\n",
    "\n",
    "1. **Primary: Recall (Sensitivity)** âœ…\n",
    "   - Goal: Catch all positive cases (minimize FN)\n",
    "   - Willing to accept more FP to avoid missing disease\n",
    "   \n",
    "2. **Secondary: Precision (PPV)** \n",
    "   - Control false alarm rate\n",
    "   - But lower priority than recall\n",
    "   \n",
    "3. **Threshold strategy**:\n",
    "   - Set threshold to achieve **high recall (â‰¥95%)**\n",
    "   - Accept lower precision (e.g., 30-50%)\n",
    "   - Follow-up with confirmatory tests for positives\n",
    "   \n",
    "4. **Why NOT accuracy or AUC-ROC**:\n",
    "   - Accuracy: Misleading with 1% prevalence (99% by predicting all negative)\n",
    "   - AUC-ROC: Doesn't capture asymmetric costs\n",
    "   - **Use PR-AUC instead** (focuses on minority class)\n",
    "\n",
    "**Example threshold selection**:\n",
    "```python\n",
    "# Find threshold giving 95% recall\n",
    "target_recall = 0.95\n",
    "precision_all, recall_all, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "idx = np.where(recall_all >= target_recall)[0]\n",
    "optimal_threshold = thresholds[idx[-1]]  # Lowest threshold with â‰¥95% recall\n",
    "```\n",
    "\n",
    "**Clinical decision rule**:\n",
    "- Threshold low (e.g., 0.1) â†’ catch all cases\n",
    "- Positive screening â†’ confirmatory diagnostic test\n",
    "- Better to over-test than miss disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bcbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1 (demonstration)\n",
    "print(\"Medical Diagnosis Strategy:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Scenario: Rare disease (1% prevalence)\")\n",
    "print(\"  Cost(FN) >> Cost(FP)\")\n",
    "print(\"\")\n",
    "print(\"Recommended Approach:\")\n",
    "print(\"  1. Primary metric: Recall (â‰¥95%)\")\n",
    "print(\"  2. Secondary metric: Precision (acceptable if >30%)\")\n",
    "print(\"  3. Use PR-AUC for model selection\")\n",
    "print(\"  4. Set low threshold for high recall\")\n",
    "print(\"  5. Confirmatory test for all positives\")\n",
    "print(\"\")\n",
    "print(\"Why this works:\")\n",
    "print(\"  â€¢ Screening catches all cases (high recall)\")\n",
    "print(\"  â€¢ Confirmatory test filters false positives\")\n",
    "print(\"  â€¢ Acceptable workflow: screen 1000 â†’ 100 positive â†’ 10 confirm â†’ 10 treat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4954e",
   "metadata": {},
   "source": [
    "**Solution 2**: Optimal threshold for custom cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2\n",
    "def find_optimal_threshold_by_cost(y_true, y_pred_proba, cost_fn, cost_fp):\n",
    "    \"\"\"Find threshold minimizing total cost.\"\"\"\n",
    "    # Try all unique thresholds\n",
    "    thresholds = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    costs = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        total_cost = fn * cost_fn + fp * cost_fp\n",
    "        costs.append(total_cost)\n",
    "    \n",
    "    # Find threshold with minimum cost\n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_cost = costs[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold, optimal_cost, costs\n",
    "\n",
    "# Find optimal threshold\n",
    "cost_fn = 1000  # Cost of False Negative\n",
    "cost_fp = 10    # Cost of False Positive\n",
    "\n",
    "optimal_thresh, optimal_cost, all_costs = find_optimal_threshold_by_cost(\n",
    "    y_test, y_pred_test_lr, cost_fn, cost_fp\n",
    ")\n",
    "\n",
    "# Evaluate at optimal threshold\n",
    "y_pred_cost_optimal = (y_pred_test_lr >= optimal_thresh).astype(int)\n",
    "cm_optimal = confusion_matrix(y_test, y_pred_cost_optimal)\n",
    "tn, fp, fn, tp = cm_optimal.ravel()\n",
    "\n",
    "print(\"Optimal Threshold by Cost Minimization:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Cost parameters:\")\n",
    "print(f\"  Cost(FN) = ${cost_fn:,}\")\n",
    "print(f\"  Cost(FP) = ${cost_fp:,}\")\n",
    "print(f\"  Cost ratio (FN/FP) = {cost_fn/cost_fp:.0f}:1\")\n",
    "print(f\"\\nOptimal threshold: {optimal_thresh:.4f}\")\n",
    "print(f\"Total cost at optimal: ${optimal_cost:,.0f}\")\n",
    "print(f\"\\nConfusion matrix at optimal threshold:\")\n",
    "print(f\"  TP={tp:,}, FP={fp:,}, TN={tn:,}, FN={fn:,}\")\n",
    "print(f\"  Total cost = {fn}Ã—${cost_fn:,} + {fp}Ã—${cost_fp:,} = ${optimal_cost:,.0f}\")\n",
    "\n",
    "# Compare with default threshold (0.5)\n",
    "y_pred_default = (y_pred_test_lr >= 0.5).astype(int)\n",
    "cm_default = confusion_matrix(y_test, y_pred_default)\n",
    "tn_d, fp_d, fn_d, tp_d = cm_default.ravel()\n",
    "cost_default = fn_d * cost_fn + fp_d * cost_fp\n",
    "\n",
    "print(f\"\\nComparison with default (0.5):\")\n",
    "print(f\"  FN={fn_d}, FP={fp_d}\")\n",
    "print(f\"  Total cost = ${cost_default:,.0f}\")\n",
    "print(f\"  Savings = ${cost_default - optimal_cost:,.0f} ({(cost_default - optimal_cost)/cost_default*100:.1f}%)\")\n",
    "\n",
    "# Visualize cost vs threshold\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "thresholds_plot = np.linspace(0, 1, 1000)\n",
    "ax.plot(thresholds_plot, all_costs, linewidth=2, color='steelblue')\n",
    "ax.axvline(optimal_thresh, color='red', linewidth=2, linestyle='--', \n",
    "           label=f'Optimal={optimal_thresh:.3f}')\n",
    "ax.axvline(0.5, color='orange', linewidth=2, linestyle='--', alpha=0.7,\n",
    "           label='Default=0.5')\n",
    "ax.set_xlabel('Threshold', fontsize=12)\n",
    "ax.set_ylabel('Total Cost ($)', fontsize=12)\n",
    "ax.set_title('Cost Minimization: Finding Optimal Threshold', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '03_cost_optimization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d735184d",
   "metadata": {},
   "source": [
    "**Solution 3**: When RÂ² fails\n",
    "\n",
    "**Answer**: Deploy Model A despite lower RÂ².\n",
    "\n",
    "**Reasoning**:\n",
    "\n",
    "1. **RÂ² limitations**:\n",
    "   - Measures correlation, not prediction quality\n",
    "   - Doesn't penalize impossible predictions (negative prices)\n",
    "   - Can be high even with systematically biased predictions\n",
    "   \n",
    "2. **Model B red flags**:\n",
    "   - **Negative price predictions are physically impossible** âŒ\n",
    "   - Indicates model doesn't respect domain constraints\n",
    "   - May have extrapolation issues\n",
    "   - Unreliable for production deployment\n",
    "   \n",
    "3. **Model A advantages**:\n",
    "   - **Predictions always plausible** (within 20% of true) âœ…\n",
    "   - Lower RÂ² but more robust\n",
    "   - Predictable error bounds\n",
    "   - Safe for production\n",
    "\n",
    "**Additional metrics to check**:\n",
    "\n",
    "```python\n",
    "# 1. Mean Absolute Error (MAE)\n",
    "#    â†’ Interpretable: average $ error\n",
    "mae = np.mean(np.abs(y_pred - y_true))\n",
    "\n",
    "# 2. Mean Absolute Percentage Error (MAPE)\n",
    "#    â†’ Relative error (% of true value)\n",
    "mape = np.mean(np.abs((y_pred - y_true) / y_true)) * 100\n",
    "\n",
    "# 3. Max Error\n",
    "#    â†’ Worst-case prediction error\n",
    "max_error = np.max(np.abs(y_pred - y_true))\n",
    "\n",
    "# 4. Residual distribution\n",
    "#    â†’ Check for systematic bias, outliers\n",
    "residuals = y_pred - y_true\n",
    "plt.hist(residuals, bins=50)\n",
    "\n",
    "# 5. Prediction bounds check\n",
    "#    â†’ Verify all predictions are plausible\n",
    "assert (y_pred >= 0).all(), \"Negative prices detected!\"\n",
    "assert (y_pred <= max_plausible_price).all(), \"Implausibly high prices!\"\n",
    "```\n",
    "\n",
    "**Decision rule for production**:\n",
    "- RÂ² as model selection metric (among valid models)\n",
    "- Domain constraints as hard requirements\n",
    "- MAE/MAPE for interpretability\n",
    "- Max error for risk assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3 (code demonstration)\n",
    "print(\"When RÂ² Fails: Model Selection Example\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nScenario: House price prediction\")\n",
    "print(\"  Model A: RÂ²=0.85, all predictions within 20% of true\")\n",
    "print(\"  Model B: RÂ²=0.90, but sometimes predicts negative prices\")\n",
    "print(\"\\nDecision: Deploy Model A âœ…\")\n",
    "print(\"\\nReasoning:\")\n",
    "print(\"  1. RÂ² doesn't capture domain constraints\")\n",
    "print(\"  2. Negative prices violate physical reality\")\n",
    "print(\"  3. Model B unreliable for production\")\n",
    "print(\"  4. Model A has predictable errors\")\n",
    "print(\"\\nLesson: High RÂ² â‰  Good model\")\n",
    "print(\"        Always check:\")\n",
    "print(\"          â€¢ Domain constraints (price â‰¥ 0)\")\n",
    "print(\"          â€¢ Residual distribution\")\n",
    "print(\"          â€¢ Max error magnitude\")\n",
    "print(\"          â€¢ Prediction plausibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428b042",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Notebook Complete!\n",
    "\n",
    "**What you learned**:\n",
    "1. âœ… Establish baselines (majority class, logistic regression)\n",
    "2. âœ… Choose metrics aligned with business goals\n",
    "3. âœ… Understand metric limitations (accuracy, RÂ²)\n",
    "4. âœ… Select thresholds based on cost/benefit\n",
    "5. âœ… Compute confidence intervals via bootstrap\n",
    "\n",
    "**Outputs saved**:\n",
    "- `reports/03_roc_curve_baseline.png`\n",
    "- `reports/03_pr_curve_baseline.png`\n",
    "- `reports/03_confusion_matrix.png`\n",
    "- `reports/03_bootstrap_distributions.png`\n",
    "- `reports/03_baseline_metrics.csv`\n",
    "- `reports/03_cost_optimization.png`\n",
    "\n",
    "**Key takeaways**:\n",
    "- Baselines establish minimum performance\n",
    "- Accuracy misleads with imbalanced data\n",
    "- AUC-ROC (discrimination) vs. PR-AUC (minority class focus)\n",
    "- Threshold selection requires domain knowledge\n",
    "- Bootstrap CIs quantify uncertainty\n",
    "\n",
    "**Next notebook**: `04_gbm_model_and_hyperparameter_search.ipynb` â€” Train a gradient boosting model and tune hyperparameters systematically."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
