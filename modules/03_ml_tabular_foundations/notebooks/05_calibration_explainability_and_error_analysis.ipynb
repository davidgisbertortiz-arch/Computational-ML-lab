{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "repo_root = Path().resolve().parents[2]\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core.seeding', 'set_seed')\n",
    "load_data, get_feature_columns, split_data = safe_import_from(\n",
    "    '03_ml_tabular_foundations.src.data',\n",
    "    'load_data', 'get_feature_columns', 'split_data'\n",
    ")\n",
    "create_lightgbm_pipeline, calibrate_model = safe_import_from(\n",
    "    '03_ml_tabular_foundations.src.models',\n",
    "    'create_lightgbm_pipeline', 'calibrate_model'\n",
    ")\n",
    "expected_calibration_error, compute_permutation_importance = safe_import_from(\n",
    "    '03_ml_tabular_foundations.src.eval',\n",
    "    'expected_calibration_error', 'compute_permutation_importance'\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "reports_dir = Path(\"../reports\")\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806cf7e",
   "metadata": {},
   "source": [
    "## 1. Why Calibration Matters\n",
    "\n",
    "**Problem**: Raw classifier outputs are **scores**, not probabilities.\n",
    "\n",
    "**Example**: Model outputs 0.7 for 100 samples:\n",
    "- **Well-calibrated**: ~70 are actually positive\n",
    "- **Poorly-calibrated**: Only 50 are positive (overconfident)\n",
    "\n",
    "**When calibration is critical**:\n",
    "- Medical diagnosis (need reliable risk estimates)\n",
    "- Cost-sensitive decisions (FP/FN tradeoffs)\n",
    "- Ensembling (combining models by probability)\n",
    "- User-facing systems (displaying confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "df = load_data()\n",
    "feature_cols = get_feature_columns(df)\n",
    "X = df[feature_cols].values\n",
    "y = df['is_signal'].values\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(\n",
    "    X, y, test_size=0.2, val_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "\n",
    "# Train uncalibrated model\n",
    "print(\"\\nTraining uncalibrated GBM...\")\n",
    "pipeline_uncalibrated = create_lightgbm_pipeline(\n",
    "    n_estimators=500, learning_rate=0.05, max_depth=7, \n",
    "    num_leaves=63, random_state=42\n",
    ")\n",
    "pipeline_uncalibrated.fit(X_train, y_train)\n",
    "\n",
    "y_pred_uncal = pipeline_uncalibrated.predict_proba(X_test)[:, 1]\n",
    "print(f\"âœ… Uncalibrated model trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211ff90",
   "metadata": {},
   "source": [
    "## 2. Reliability Diagram\n",
    "\n",
    "**Reliability diagram**: Predicted probability vs. observed frequency.\n",
    "\n",
    "**Interpretation**:\n",
    "- **Diagonal line**: Perfect calibration (predicted = observed)\n",
    "- **Above diagonal**: Underconfident (predicting lower than actual)\n",
    "- **Below diagonal**: Overconfident (predicting higher than actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reliability diagram\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_uncal, n_bins=10, strategy='uniform')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot reliability curve\n",
    "ax.plot(prob_pred, prob_true, 'o-', linewidth=2.5, markersize=10, \n",
    "        label='Uncalibrated Model', color='coral')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration', alpha=0.6)\n",
    "\n",
    "ax.set_xlabel('Predicted Probability', fontsize=12)\n",
    "ax.set_ylabel('Observed Frequency', fontsize=12)\n",
    "ax.set_title('Reliability Diagram (Before Calibration)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_reliability_before.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compute Expected Calibration Error (ECE)\n",
    "ece_uncal = expected_calibration_error(y_test, y_pred_uncal, n_bins=10)\n",
    "\n",
    "print(f\"ðŸ“Š Calibration Analysis (Before):\")\n",
    "print(f\"  Expected Calibration Error (ECE): {ece_uncal:.4f}\")\n",
    "if ece_uncal > 0.10:\n",
    "    print(\"  âš ï¸ Poor calibration (ECE > 0.10)\")\n",
    "elif ece_uncal > 0.05:\n",
    "    print(\"  âš ï¸ Moderate miscalibration (ECE > 0.05)\")\n",
    "else:\n",
    "    print(\"  âœ… Well-calibrated (ECE < 0.05)\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Interpretation:\")\n",
    "if prob_true[-1] < prob_pred[-1]:\n",
    "    print(\"  â€¢ Model is OVERCONFIDENT (predicts higher than reality)\")\n",
    "else:\n",
    "    print(\"  â€¢ Model is UNDERCONFIDENT (predicts lower than reality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31855c4a",
   "metadata": {},
   "source": [
    "## 3. Calibration Methods\n",
    "\n",
    "**Common methods**:\n",
    "1. **Platt Scaling**: Fit logistic regression on predictions\n",
    "2. **Isotonic Regression**: Fit non-parametric monotonic function\n",
    "3. **Temperature Scaling**: Divide logits by temperature parameter\n",
    "\n",
    "**We'll use Platt scaling** (fast, works well for tree-based models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate model using validation set\n",
    "print(\"Calibrating model using Platt scaling...\")\n",
    "pipeline_calibrated = calibrate_model(\n",
    "    pipeline_uncalibrated, X_val, y_val, method='platt'\n",
    ")\n",
    "\n",
    "y_pred_cal = pipeline_calibrated.predict_proba(X_test)[:, 1]\n",
    "print(\"âœ… Model calibrated\")\n",
    "\n",
    "# Compute reliability diagram for calibrated model\n",
    "prob_true_cal, prob_pred_cal = calibration_curve(y_test, y_pred_cal, n_bins=10, strategy='uniform')\n",
    "ece_cal = expected_calibration_error(y_test, y_pred_cal, n_bins=10)\n",
    "\n",
    "print(f\"\\nðŸ“Š Calibration Analysis (After):\")\n",
    "print(f\"  Expected Calibration Error (ECE): {ece_cal:.4f}\")\n",
    "print(f\"  Improvement: {(ece_uncal - ece_cal):.4f} ({(ece_uncal - ece_cal)/ece_uncal*100:.1f}% reduction)\")\n",
    "\n",
    "# Plot both reliability diagrams\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Before calibration\n",
    "ax1.plot(prob_pred, prob_true, 'o-', linewidth=2.5, markersize=10, color='coral', label='Uncalibrated')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.6, label='Perfect')\n",
    "ax1.set_xlabel('Predicted Probability', fontsize=11)\n",
    "ax1.set_ylabel('Observed Frequency', fontsize=11)\n",
    "ax1.set_title(f'Before Calibration (ECE={ece_uncal:.4f})', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# After calibration\n",
    "ax2.plot(prob_pred_cal, prob_true_cal, 's-', linewidth=2.5, markersize=10, color='steelblue', label='Calibrated')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.6, label='Perfect')\n",
    "ax2.set_xlabel('Predicted Probability', fontsize=11)\n",
    "ax2.set_ylabel('Observed Frequency', fontsize=11)\n",
    "ax2.set_title(f'After Calibration (ECE={ece_cal:.4f})', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_reliability_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Check discrimination is preserved\n",
    "auc_uncal = roc_auc_score(y_test, y_pred_uncal)\n",
    "auc_cal = roc_auc_score(y_test, y_pred_cal)\n",
    "\n",
    "print(f\"\\nðŸ” Discrimination Check:\")\n",
    "print(f\"  Uncalibrated AUC: {auc_uncal:.4f}\")\n",
    "print(f\"  Calibrated AUC:   {auc_cal:.4f}\")\n",
    "print(f\"  Change: {(auc_cal - auc_uncal):.4f}\")\n",
    "print(\"  âœ… Calibration preserves discrimination (AUC unchanged)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb06227f",
   "metadata": {},
   "source": [
    "## 4. Explainability: Permutation Importance\n",
    "\n",
    "**Goal**: Understand which features drive predictions.\n",
    "\n",
    "**Method**: Permutation importance\n",
    "- Shuffle feature i\n",
    "- Measure drop in performance\n",
    "- Importance = drop in AUC\n",
    "\n",
    "**Advantages over tree importance**:\n",
    "- Model-agnostic (works for any model)\n",
    "- Captures feature interactions\n",
    "- Doesn't bias towards high-cardinality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance\n",
    "print(\"Computing permutation importance (this may take ~30s)...\")\n",
    "perm_importance = compute_permutation_importance(\n",
    "    pipeline_calibrated, X_test, y_test, n_repeats=10, random_state=42\n",
    ")\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': perm_importance['importances_mean'],\n",
    "    'Std': perm_importance['importances_std']\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"âœ… Permutation importance computed\")\n",
    "\n",
    "# Plot top 10 features\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "top_n = 10\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "ax.barh(range(top_n), top_features['Importance'].values, \n",
    "        xerr=top_features['Std'].values, alpha=0.8,\n",
    "        color='steelblue', edgecolor='black', linewidth=1.5, capsize=5)\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(top_features['Feature'].values, fontsize=10)\n",
    "ax.set_xlabel('Permutation Importance (Drop in AUC)', fontsize=12)\n",
    "ax.set_title(f'Top {top_n} Features by Permutation Importance', fontsize=13, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_permutation_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Top 5 Most Important Features:\")\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    print(f\"  {row['Feature']:<20} {row['Importance']:.4f} Â± {row['Std']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Interpretation:\")\n",
    "print(f\"  â€¢ Top feature: {importance_df.iloc[0]['Feature']} (likely m_inv, Higgs mass peak)\")\n",
    "print(f\"  â€¢ Error bars show stability across permutations\")\n",
    "print(f\"  â€¢ Features with Importance â‰ˆ 0 can be dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b86716",
   "metadata": {},
   "source": [
    "## 5. Error Analysis: Confusion Matrix Breakdown\n",
    "\n",
    "**Goal**: Understand where the model fails.\n",
    "\n",
    "**Strategy**:\n",
    "1. Compute confusion matrix\n",
    "2. Analyze False Positives vs. False Negatives\n",
    "3. Identify systematic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffdf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix at optimal threshold (using default 0.5 for simplicity)\n",
    "y_pred_binary = (y_pred_cal >= 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(cm, cmap='Blues', alpha=0.8)\n",
    "\n",
    "# Add text annotations\n",
    "labels = [['TN\\n(Correct Background)', 'FP\\n(Background â†’ Signal)'],\n",
    "          ['FN\\n(Signal â†’ Background)', 'TP\\n(Correct Signal)']]\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, f'{cm[i, j]:,}\\n{labels[i][j]}',\n",
    "                      ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "                      color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Pred: Background (0)', 'Pred: Signal (1)'], fontsize=11)\n",
    "ax.set_yticklabels(['True: Background (0)', 'True: Signal (1)'], fontsize=11)\n",
    "ax.set_title('Confusion Matrix (Calibrated Model, Threshold=0.5)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Error analysis\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "total = cm.sum()\n",
    "\n",
    "print(f\"ðŸ“Š Error Breakdown:\")\n",
    "print(f\"  True Negatives (TN):  {tn:,} ({tn/total*100:.1f}%)\")\n",
    "print(f\"  False Positives (FP): {fp:,} ({fp/total*100:.1f}%) â† Background misclassified as signal\")\n",
    "print(f\"  False Negatives (FN): {fn:,} ({fn/total*100:.1f}%) â† Signal misclassified as background\")\n",
    "print(f\"  True Positives (TP):  {tp:,} ({tp/total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ” Error Rates:\")\n",
    "print(f\"  Precision: {tp/(tp+fp):.1%} (of predicted signals, {tp/(tp+fp):.1%} are real)\")\n",
    "print(f\"  Recall:    {tp/(tp+fn):.1%} (of true signals, {tp/(tp+fn):.1%} were caught)\")\n",
    "print(f\"  FPR:       {fp/(fp+tn):.1%} (of backgrounds, {fp/(fp+tn):.1%} misclassified)\")\n",
    "print(f\"  FNR:       {fn/(fn+tp):.1%} (of signals, {fn/(fn+tp):.1%} missed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d362e3",
   "metadata": {},
   "source": [
    "## 6. Slice Analysis: Performance by Subgroup\n",
    "\n",
    "**Goal**: Find subgroups where model fails.\n",
    "\n",
    "**Strategy**: Bin continuous features, measure performance per bin.\n",
    "\n",
    "**Example**: Does model fail on low-momentum events? High-mass events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice analysis: Performance by feature bins\n",
    "slice_feature = 'm_inv'  # Invariant mass (most important feature)\n",
    "slice_feature_idx = feature_cols.index(slice_feature)\n",
    "\n",
    "# Create bins\n",
    "n_bins = 5\n",
    "bins = np.linspace(X_test[:, slice_feature_idx].min(), \n",
    "                   X_test[:, slice_feature_idx].max(), n_bins+1)\n",
    "bin_labels = [f'{bins[i]:.1f}-{bins[i+1]:.1f}' for i in range(n_bins)]\n",
    "\n",
    "# Assign samples to bins\n",
    "bin_indices = np.digitize(X_test[:, slice_feature_idx], bins) - 1\n",
    "bin_indices = np.clip(bin_indices, 0, n_bins-1)\n",
    "\n",
    "# Compute metrics per bin\n",
    "slice_metrics = []\n",
    "for i in range(n_bins):\n",
    "    mask = bin_indices == i\n",
    "    if mask.sum() > 10:  # Need enough samples\n",
    "        slice_auc = roc_auc_score(y_test[mask], y_pred_cal[mask])\n",
    "        slice_metrics.append({\n",
    "            'Bin': bin_labels[i],\n",
    "            'N_samples': mask.sum(),\n",
    "            'Signal_rate': y_test[mask].mean(),\n",
    "            'AUC': slice_auc\n",
    "        })\n",
    "\n",
    "slice_df = pd.DataFrame(slice_metrics)\n",
    "\n",
    "# Plot slice performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# AUC by bin\n",
    "ax1.bar(range(len(slice_df)), slice_df['AUC'].values, alpha=0.8, \n",
    "        color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "ax1.axhline(auc_cal, color='red', linestyle='--', linewidth=2, label=f'Overall AUC={auc_cal:.3f}')\n",
    "ax1.set_xticks(range(len(slice_df)))\n",
    "ax1.set_xticklabels(slice_df['Bin'].values, rotation=45, ha='right')\n",
    "ax1.set_ylabel('AUC-ROC', fontsize=12)\n",
    "ax1.set_xlabel(f'{slice_feature} Bins (GeV)', fontsize=12)\n",
    "ax1.set_title(f'Performance by {slice_feature} Range', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3, axis='y')\n",
    "ax1.set_ylim([0.85, 1.0])\n",
    "\n",
    "# Sample distribution\n",
    "ax2.bar(range(len(slice_df)), slice_df['N_samples'].values, alpha=0.8,\n",
    "        color='coral', edgecolor='black', linewidth=1.5)\n",
    "ax2.set_xticks(range(len(slice_df)))\n",
    "ax2.set_xticklabels(slice_df['Bin'].values, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax2.set_xlabel(f'{slice_feature} Bins (GeV)', fontsize=12)\n",
    "ax2.set_title('Sample Distribution Across Bins', fontsize=13, fontweight='bold')\n",
    "ax2.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_slice_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“Š Slice Analysis ({slice_feature}):\")\n",
    "print(slice_df.to_string(index=False))\n",
    "\n",
    "# Identify worst-performing slice\n",
    "worst_slice = slice_df.loc[slice_df['AUC'].idxmin()]\n",
    "print(f\"\\nâš ï¸ Worst-performing slice:\")\n",
    "print(f\"  Bin: {worst_slice['Bin']} GeV\")\n",
    "print(f\"  AUC: {worst_slice['AUC']:.4f} (vs. overall {auc_cal:.4f})\")\n",
    "print(f\"  Samples: {worst_slice['N_samples']}\")\n",
    "print(f\"\\nðŸ’¡ Mitigation: Collect more data in this range or engineer features specific to this regime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af4577",
   "metadata": {},
   "source": [
    "## 7. Top Mistakes: Inspecting Misclassified Examples\n",
    "\n",
    "**Goal**: Understand why specific examples fail.\n",
    "\n",
    "**Strategy**: Find hardest false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aadb117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top mistakes\n",
    "# False Positives: background with high predicted probability\n",
    "fp_mask = (y_test == 0) & (y_pred_cal > 0.5)\n",
    "fp_scores = y_pred_cal[fp_mask]\n",
    "fp_indices = np.where(fp_mask)[0]\n",
    "\n",
    "# Sort by confidence (most confident mistakes)\n",
    "top_fp_idx = fp_indices[np.argsort(fp_scores)[-5:]]  # Top 5 most confident FPs\n",
    "\n",
    "# False Negatives: signal with low predicted probability\n",
    "fn_mask = (y_test == 1) & (y_pred_cal < 0.5)\n",
    "fn_scores = y_pred_cal[fn_mask]\n",
    "fn_indices = np.where(fn_mask)[0]\n",
    "\n",
    "# Sort by confidence (most confident mistakes)\n",
    "top_fn_idx = fn_indices[np.argsort(fn_scores)[:5]]  # Top 5 most confident FNs\n",
    "\n",
    "print(\"ðŸ” Top Mistakes Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nTop 5 False Positives (Background predicted as Signal):\")\n",
    "print(f\"{'Index':<8} {'Pred Prob':<12} {'Feature Summary'}\")\n",
    "print(\"-\" * 60)\n",
    "for idx in top_fp_idx:\n",
    "    pred_prob = y_pred_cal[idx]\n",
    "    # Show key feature values\n",
    "    m_inv_val = X_test[idx, feature_cols.index('m_inv')]\n",
    "    missing_et_val = X_test[idx, feature_cols.index('missing_E_T')]\n",
    "    print(f\"{idx:<8} {pred_prob:<12.4f} m_inv={m_inv_val:.1f}, missing_E_T={missing_et_val:.1f}\")\n",
    "\n",
    "print(\"\\nTop 5 False Negatives (Signal predicted as Background):\")\n",
    "print(f\"{'Index':<8} {'Pred Prob':<12} {'Feature Summary'}\")\n",
    "print(\"-\" * 60)\n",
    "for idx in top_fn_idx:\n",
    "    pred_prob = y_pred_cal[idx]\n",
    "    m_inv_val = X_test[idx, feature_cols.index('m_inv')]\n",
    "    missing_et_val = X_test[idx, feature_cols.index('missing_E_T')]\n",
    "    print(f\"{idx:<8} {pred_prob:<12.4f} m_inv={m_inv_val:.1f}, missing_E_T={missing_et_val:.1f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Patterns in Mistakes:\")\n",
    "print(\"  â€¢ False Positives: Background events that 'look like' signal\")\n",
    "print(\"    - May have high m_inv (near Higgs mass) by chance\")\n",
    "print(\"    - Difficult to distinguish from true signal\")\n",
    "print(\"  â€¢ False Negatives: Signal events that 'look like' background\")\n",
    "print(\"    - May have low m_inv (off-peak)\")\n",
    "print(\"    - Low missing E_T (leptons not well-isolated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8c570",
   "metadata": {},
   "source": [
    "## 8. Summary Report: Failure Modes & Mitigations\n",
    "\n",
    "Synthesize error analysis into actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate error analysis summary\n",
    "error_summary = f\"\"\"\n",
    "# Error Analysis Summary\n",
    "\n",
    "## Model Performance\n",
    "- **Test AUC**: {auc_cal:.4f}\n",
    "- **Test PR-AUC**: {average_precision_score(y_test, y_pred_cal):.4f}\n",
    "- **Calibration (ECE)**: {ece_cal:.4f} (improved from {ece_uncal:.4f})\n",
    "\n",
    "## Error Breakdown\n",
    "- **False Positives**: {fp:,} ({fp/total*100:.1f}%)\n",
    "  - Background events misclassified as signal\n",
    "  - **Cost**: Wasted detector readout time, follow-up analysis\n",
    "  \n",
    "- **False Negatives**: {fn:,} ({fn/total*100:.1f}%)\n",
    "  - Signal events missed\n",
    "  - **Cost**: Missed physics discoveries\n",
    "\n",
    "## Identified Failure Modes\n",
    "\n",
    "### 1. Off-Peak Signal Events (False Negatives)\n",
    "- **Pattern**: Signal with m_inv far from 125 GeV\n",
    "- **Frequency**: ~{fn/(fn+tp)*100:.0f}% of all signal\n",
    "- **Mitigation**: \n",
    "  - Add feature: distance to Higgs mass peak\n",
    "  - Collect more off-peak training data\n",
    "  - Ensemble with model trained on off-peak region\n",
    "\n",
    "### 2. Background with Signal-Like Features (False Positives)\n",
    "- **Pattern**: Background with high m_inv, high missing E_T\n",
    "- **Frequency**: ~{fp/(fp+tn)*100:.0f}% of all background\n",
    "- **Mitigation**:\n",
    "  - Add discriminating features (jet substructure, angular correlations)\n",
    "  - Increase threshold for high-stakes decisions\n",
    "  - Use ensemble disagreement as uncertainty signal\n",
    "\n",
    "### 3. Low-Momentum Events (Slice Analysis)\n",
    "- **Pattern**: Worse performance at {worst_slice['Bin']} GeV\n",
    "- **Impact**: AUC drops to {worst_slice['AUC']:.4f} (vs. {auc_cal:.4f} overall)\n",
    "- **Mitigation**:\n",
    "  - Stratified sampling to balance momentum bins\n",
    "  - Dedicated model for low-momentum regime\n",
    "  - Feature engineering for kinematics at threshold\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **Production Deployment**:\n",
    "   - Use calibrated model (ECE < 0.05)\n",
    "   - Set threshold based on cost analysis (FP vs. FN tradeoff)\n",
    "   - Monitor slice performance (retrain if drift detected)\n",
    "\n",
    "2. **Model Improvements**:\n",
    "   - Collect more data in failure slices\n",
    "   - Engineer features for off-peak signal\n",
    "   - Ensemble multiple models (momentum-binned)\n",
    "\n",
    "3. **Monitoring**:\n",
    "   - Track calibration drift (ECE over time)\n",
    "   - Monitor slice performance separately\n",
    "   - Alert if FN rate increases (missed discoveries)\n",
    "\"\"\"\n",
    "\n",
    "# Save summary\n",
    "with open(reports_dir / '05_error_analysis_summary.md', 'w') as f:\n",
    "    f.write(error_summary)\n",
    "\n",
    "print(\"ðŸ“‹ Error Analysis Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(error_summary)\n",
    "print(f\"\\nâœ… Summary saved to: {reports_dir / '05_error_analysis_summary.md'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7f476",
   "metadata": {},
   "source": [
    "## 9. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149be4a",
   "metadata": {},
   "source": [
    "**Exercise 1**: Diagnose calibration issues\n",
    "\n",
    "You train a model and get this reliability diagram:\n",
    "- Bin 0-0.1: Predicted=0.05, Observed=0.02\n",
    "- Bin 0.1-0.2: Predicted=0.15, Observed=0.10\n",
    "- ...\n",
    "- Bin 0.9-1.0: Predicted=0.95, Observed=0.70\n",
    "\n",
    "What does this tell you? How would you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34545a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d78a7",
   "metadata": {},
   "source": [
    "**Exercise 2**: Feature importance discrepancy\n",
    "\n",
    "You compute two types of feature importance:\n",
    "- **Tree importance**: `feature_A` = 0.3, `feature_B` = 0.1\n",
    "- **Permutation importance**: `feature_A` = 0.05, `feature_B` = 0.25\n",
    "\n",
    "Why do they disagree? Which should you trust?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e452190",
   "metadata": {},
   "source": [
    "**Exercise 3**: Design an error mitigation strategy\n",
    "\n",
    "Your fraud detection model has:\n",
    "- High FP rate on small transactions (<$10)\n",
    "- High FN rate on new merchants (< 30 days old)\n",
    "\n",
    "Propose 3 concrete mitigations (data, features, or modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe55d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ed90f",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00a58f",
   "metadata": {},
   "source": [
    "**Solution 1**: Diagnose calibration issues\n",
    "\n",
    "**Analysis**: Model is **systematically overconfident**.\n",
    "\n",
    "**Evidence**:\n",
    "- At high confidence (0.9-1.0), predicts 95% but only 70% are positive\n",
    "- Consistent pattern across bins: predicted > observed\n",
    "\n",
    "**Root causes**:\n",
    "1. **Tree-based models** (GBMs) tend to be overconfident\n",
    "   - Split decisions are deterministic\n",
    "   - Leaf predictions don't capture uncertainty\n",
    "   \n",
    "2. **Class imbalance** amplifies overconfidence\n",
    "   - Rare positive class â†’ high thresholds\n",
    "   - Model \"reaches\" for positives\n",
    "\n",
    "3. **Overfitting** in high-confidence region\n",
    "   - Memorized training patterns\n",
    "   - Doesn't generalize to test distribution\n",
    "\n",
    "**Fixes**:\n",
    "\n",
    "1. **Platt Scaling** (first choice):\n",
    "   ```python\n",
    "   from sklearn.calibration import CalibratedClassifierCV\n",
    "   calibrated = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "   calibrated.fit(X_val, y_val)  # Use separate validation set\n",
    "   ```\n",
    "\n",
    "2. **Isotonic Regression** (if non-monotonic):\n",
    "   ```python\n",
    "   calibrated = CalibratedClassifierCV(model, method='isotonic', cv='prefit')\n",
    "   ```\n",
    "\n",
    "3. **Reduce overconfidence in training**:\n",
    "   - Early stopping (prevent overfitting)\n",
    "   - Increase regularization (reg_alpha, reg_lambda)\n",
    "   - Use ensembles (average predictions â†’ more conservative)\n",
    "\n",
    "4. **Temperature scaling** (for neural networks):\n",
    "   ```python\n",
    "   # Find temperature T that minimizes log loss on validation set\n",
    "   # Divide logits by T before softmax\n",
    "   ```\n",
    "\n",
    "**Verification**: ECE should drop from ~0.25 to <0.05 after calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de896ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1 (code demonstration)\n",
    "print(\"Calibration Diagnosis\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSymptoms:\")\n",
    "print(\"  Predicted=0.95, Observed=0.70 â† OVERCONFIDENT\")\n",
    "print(\"  Pattern: Predicted > Observed across all bins\")\n",
    "print(\"\\nRoot causes:\")\n",
    "print(\"  1. Tree-based models are naturally overconfident\")\n",
    "print(\"  2. Class imbalance amplifies issue\")\n",
    "print(\"  3. Overfitting in high-confidence region\")\n",
    "print(\"\\nFixes (prioritized):\")\n",
    "print(\"  1. Platt scaling (logistic recalibration)\")\n",
    "print(\"  2. Isotonic regression (non-parametric)\")\n",
    "print(\"  3. Temperature scaling (for neural networks)\")\n",
    "print(\"  4. Reduce overfitting (early stopping, regularization)\")\n",
    "print(\"\\nExpected result:\")\n",
    "print(\"  ECE: 0.25 â†’ 0.03 (90% reduction)\")\n",
    "print(\"  Reliability curve aligns with diagonal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b75b1",
   "metadata": {},
   "source": [
    "**Solution 2**: Feature importance discrepancy\n",
    "\n",
    "**Why they disagree**:\n",
    "\n",
    "**Tree importance (Gain-based)**:\n",
    "- Measures: Total reduction in loss when feature is used to split\n",
    "- **Bias**: Favors high-cardinality features (more split opportunities)\n",
    "- **Issue**: Doesn't capture feature interactions or test-time importance\n",
    "- `feature_A` has high gain â†’ used frequently in splits\n",
    "\n",
    "**Permutation importance**:\n",
    "- Measures: Drop in test performance when feature is shuffled\n",
    "- **Unbiased**: Reflects true predictive power at test time\n",
    "- **Issue**: Computationally expensive (need to rescore data)\n",
    "- `feature_B` has high permutation importance â†’ critical for test predictions\n",
    "\n",
    "**Interpretation**:\n",
    "```\n",
    "Tree Importance:  A=0.3,  B=0.1\n",
    "Perm Importance:  A=0.05, B=0.25\n",
    "\n",
    "â†’ Feature A: Used frequently in training (many splits),\n",
    "              but NOT critical for test predictions (low permutation drop)\n",
    "              \n",
    "â†’ Feature B: Used less in training (fewer splits),\n",
    "              but CRITICAL for test predictions (high permutation drop)\n",
    "```\n",
    "\n",
    "**Why this happens**:\n",
    "1. **Correlated features**: A is correlated with other features\n",
    "   - Tree uses A for splits, but other features can substitute\n",
    "   - When A is permuted, other features compensate\n",
    "   \n",
    "2. **Feature interactions**: B interacts with other features\n",
    "   - Not used alone (low gain), but critical in combination\n",
    "   - When B is permuted, interactions break â†’ performance drops\n",
    "\n",
    "**Which to trust**: **Permutation importance** âœ…\n",
    "\n",
    "**Use case**:\n",
    "- **Feature selection**: Drop features with low permutation importance\n",
    "- **Explainability**: Report permutation importance to stakeholders\n",
    "- **Tree importance**: Use for debugging training (which features are used)\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Feature A: Age (correlated with income)\n",
    "# Feature B: Fraud flag (rare, but decisive when present)\n",
    "\n",
    "# Tree uses Age frequently (continuous, many splits)\n",
    "# But Fraud flag is more predictive (permutation test reveals)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2 (code demonstration)\n",
    "print(\"Feature Importance Discrepancy\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  Tree Importance:  A=0.3,  B=0.1\")\n",
    "print(\"  Perm Importance:  A=0.05, B=0.25\")\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"  Feature A:\")\n",
    "print(\"    â€¢ High tree importance (used in many splits)\")\n",
    "print(\"    â€¢ Low permutation importance (not critical at test time)\")\n",
    "print(\"    â€¢ Likely correlated with other features (redundant)\")\n",
    "print(\"  Feature B:\")\n",
    "print(\"    â€¢ Low tree importance (few splits)\")\n",
    "print(\"    â€¢ High permutation importance (critical for predictions)\")\n",
    "print(\"    â€¢ Participates in feature interactions\")\n",
    "print(\"\\nWhich to trust: Permutation Importance âœ…\")\n",
    "print(\"  â†’ Reflects true test-time predictive power\")\n",
    "print(\"  â†’ Use for feature selection and explainability\")\n",
    "print(\"\\nTree importance is useful for:\")\n",
    "print(\"  â€¢ Debugging training (which features model uses)\")\n",
    "print(\"  â€¢ Understanding split decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704554ec",
   "metadata": {},
   "source": [
    "**Solution 3**: Error mitigation strategy\n",
    "\n",
    "**Problem**:\n",
    "- **High FP** on small transactions (<$10)\n",
    "- **High FN** on new merchants (< 30 days old)\n",
    "\n",
    "**Mitigation Strategies**:\n",
    "\n",
    "### 1. Data Collection (Address Distribution Gaps)\n",
    "\n",
    "**Small transactions**:\n",
    "```python\n",
    "# Problem: Imbalanced data (most fraud is high-value)\n",
    "# Solution: Collect more small-value fraud examples\n",
    "\n",
    "# Upsample small-value frauds in training\n",
    "small_fraud_mask = (df['amount'] < 10) & (df['is_fraud'] == 1)\n",
    "df_balanced = pd.concat([df, df[small_fraud_mask].sample(n=1000, replace=True)])\n",
    "\n",
    "# Or: Use SMOTE (synthetic oversampling)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy={1: target_count})\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "**New merchants**:\n",
    "```python\n",
    "# Problem: Cold start (no history for new merchants)\n",
    "# Solution: Collect labeled data specifically for new merchants\n",
    "\n",
    "# Prioritize manual review for new merchants\n",
    "# Build dataset: 10K transactions from new merchants with labels\n",
    "```\n",
    "\n",
    "### 2. Feature Engineering (Domain-Specific Signals)\n",
    "\n",
    "**Small transactions**:\n",
    "```python\n",
    "# Add features capturing \"small transaction patterns\"\n",
    "df['is_small_transaction'] = (df['amount'] < 10).astype(int)\n",
    "df['small_tx_velocity'] = df.groupby('user_id')['is_small_transaction'].transform('sum')  # How many small tx recently?\n",
    "df['amount_deviation'] = np.abs(df['amount'] - df.groupby('user_id')['amount'].transform('mean'))  # Unusual amount?\n",
    "\n",
    "# Interaction: Small amount + high velocity â†’ suspicious\n",
    "df['small_high_velocity'] = (df['is_small_transaction'] & (df['small_tx_velocity'] > 5)).astype(int)\n",
    "```\n",
    "\n",
    "**New merchants**:\n",
    "```python\n",
    "# Add features capturing \"new merchant risk\"\n",
    "df['merchant_age_days'] = (df['timestamp'] - df['merchant_first_seen']).dt.days\n",
    "df['merchant_tx_count'] = df.groupby('merchant_id')['transaction_id'].transform('count')\n",
    "df['merchant_fraud_rate'] = df.groupby('merchant_id')['is_fraud'].transform('mean')  # Historical fraud rate\n",
    "\n",
    "# Cold start: Use merchant category as proxy\n",
    "df['merchant_category_risk'] = df['merchant_category'].map(category_risk_dict)\n",
    "```\n",
    "\n",
    "### 3. Modeling (Subgroup-Specific Models)\n",
    "\n",
    "**Small transactions**:\n",
    "```python\n",
    "# Option A: Separate model for small transactions\n",
    "model_small = train_model(df[df['amount'] < 10])\n",
    "model_large = train_model(df[df['amount'] >= 10])\n",
    "\n",
    "# Route transactions to appropriate model\n",
    "def predict(tx):\n",
    "    if tx['amount'] < 10:\n",
    "        return model_small.predict(tx)\n",
    "    else:\n",
    "        return model_large.predict(tx)\n",
    "```\n",
    "\n",
    "```python\n",
    "# Option B: Adjust threshold for small transactions\n",
    "threshold_default = 0.5\n",
    "threshold_small = 0.3  # Lower threshold â†’ catch more fraud (accept higher FP)\n",
    "\n",
    "if tx['amount'] < 10:\n",
    "    threshold = threshold_small\n",
    "else:\n",
    "    threshold = threshold_default\n",
    "```\n",
    "\n",
    "**New merchants**:\n",
    "```python\n",
    "# Option A: Conservative threshold for new merchants\n",
    "if merchant_age < 30:\n",
    "    threshold = 0.2  # Low threshold â†’ route more to manual review\n",
    "    \n",
    "# Option B: Require 2FA for new merchants\n",
    "if merchant_age < 30 and fraud_score > 0.3:\n",
    "    require_2fa(transaction)\n",
    "```\n",
    "\n",
    "```python\n",
    "# Option C: Ensemble with heuristic\n",
    "rule_based_score = check_new_merchant_rules(tx)  # Heuristics\n",
    "ml_score = model.predict_proba(tx)[1]\n",
    "final_score = 0.7 * ml_score + 0.3 * rule_based_score  # Blend\n",
    "```\n",
    "\n",
    "**Summary**:\n",
    "1. **Data**: Upsample minority failure modes\n",
    "2. **Features**: Engineer domain-specific signals\n",
    "3. **Modeling**: Subgroup-specific models or thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3 (code template)\n",
    "print(\"Error Mitigation Strategy\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nProblem:\")\n",
    "print(\"  â€¢ High FP on small transactions (<$10)\")\n",
    "print(\"  â€¢ High FN on new merchants (< 30 days old)\")\n",
    "print(\"\\nStrategy 1: Data Collection\")\n",
    "print(\"  â€¢ Upsample small-value frauds (SMOTE)\")\n",
    "print(\"  â€¢ Prioritize labeling for new merchants\")\n",
    "print(\"\\nStrategy 2: Feature Engineering\")\n",
    "print(\"  Small transactions:\")\n",
    "print(\"    â€¢ Add: small_tx_velocity, amount_deviation\")\n",
    "print(\"    â€¢ Interaction: small_amount Ã— high_velocity\")\n",
    "print(\"  New merchants:\")\n",
    "print(\"    â€¢ Add: merchant_age_days, merchant_tx_count\")\n",
    "print(\"    â€¢ Proxy: merchant_category_risk\")\n",
    "print(\"\\nStrategy 3: Modeling\")\n",
    "print(\"  Small transactions:\")\n",
    "print(\"    â€¢ Option A: Separate model for <$10\")\n",
    "print(\"    â€¢ Option B: Lower threshold (0.5 â†’ 0.3)\")\n",
    "print(\"  New merchants:\")\n",
    "print(\"    â€¢ Option A: Conservative threshold (0.5 â†’ 0.2)\")\n",
    "print(\"    â€¢ Option B: Require 2FA for new merchants\")\n",
    "print(\"    â€¢ Option C: Ensemble ML + heuristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139762d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Notebook Complete!\n",
    "\n",
    "**What you learned**:\n",
    "1. âœ… Calibrate probability predictions (Platt scaling, ECE)\n",
    "2. âœ… Explain predictions with permutation importance\n",
    "3. âœ… Perform slice analysis (identify subgroup failures)\n",
    "4. âœ… Analyze top mistakes (FP/FN patterns)\n",
    "5. âœ… Design error mitigation strategies\n",
    "\n",
    "**Outputs saved**:\n",
    "- `reports/05_reliability_before.png`\n",
    "- `reports/05_reliability_comparison.png`\n",
    "- `reports/05_permutation_importance.png`\n",
    "- `reports/05_confusion_matrix.png`\n",
    "- `reports/05_slice_analysis.png`\n",
    "- `reports/05_error_analysis_summary.md`\n",
    "\n",
    "**Key takeaways**:\n",
    "- Calibration matters for decision-making (use Platt scaling)\n",
    "- Permutation importance > tree importance for explainability\n",
    "- Slice analysis reveals hidden failure modes\n",
    "- Error analysis drives feature engineering and data collection\n",
    "\n",
    "**Portfolio value**: This notebook demonstrates production ML thinkingâ€”not just metrics, but **actionable insights** for improvement.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Congratulations!\n",
    "\n",
    "You've completed the full tabular ML curriculum:\n",
    "1. âœ… EDA & data quality\n",
    "2. âœ… Splitting, CV & leakage prevention\n",
    "3. âœ… Baselines & metrics\n",
    "4. âœ… GBM training & hyperparameter tuning\n",
    "5. âœ… Calibration, explainability & error analysis\n",
    "\n",
    "**Next steps**:\n",
    "- Apply to your own datasets\n",
    "- Extend with SHAP for instance-level explanations\n",
    "- Deploy model with monitoring (MLflow, Evidently)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
