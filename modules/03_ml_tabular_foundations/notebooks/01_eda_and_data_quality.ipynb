{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "repo_root = Path().resolve().parents[2]\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core.seeding', 'set_seed')\n",
    "load_data, get_feature_columns = safe_import_from(\n",
    "    '03_ml_tabular_foundations.src.data',\n",
    "    'load_data', 'get_feature_columns'\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "reports_dir = Path(\"../reports\")\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cc45e",
   "metadata": {},
   "source": [
    "## 1. Problem Framing\n",
    "\n",
    "**Dataset**: Synthetic particle collision events (100K samples)\n",
    "\n",
    "**Task**: Binary classification to identify signal events (rare particle decays) from background noise.\n",
    "\n",
    "**Physics Context**:\n",
    "- Signal rate: ~10% (class imbalance typical in HEP)\n",
    "- Features: 16 kinematic variables (momentum, energy, angles, mass)\n",
    "- No time-ordering (each event is independent)\n",
    "\n",
    "**What Makes This Hard**:\n",
    "1. Class imbalance (1:9 signal:background)\n",
    "2. Correlated features (physics constraints)\n",
    "3. Rare events ‚Üí need high precision to avoid false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_data()\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"  Total samples: {len(df):,}\")\n",
    "print(f\"  Features: {len(get_feature_columns(df))}\")\n",
    "print(f\"  Target: 'is_signal' (binary)\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['is_signal'].value_counts())\n",
    "print(f\"\\nSignal rate: {df['is_signal'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2a3e9",
   "metadata": {},
   "source": [
    "## 2. Data Schema Validation\n",
    "\n",
    "**Professional ML Mindset**: Always validate your data schema before analysis.\n",
    "\n",
    "**Checks**:\n",
    "- Column names match expectation\n",
    "- Data types are correct\n",
    "- No unexpected nulls\n",
    "- Value ranges are plausible (physics constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema validation\n",
    "expected_features = {\n",
    "    'p_T', 'eta', 'phi', 'E_total', 'm_inv', 'missing_E_T',\n",
    "    'n_jets', 'b_tag_score', 'lepton_iso', 'delta_R', 'm_T',\n",
    "    'E_ratio', 'sphericity', 'aplanarity', 'centrality', 'H_T'\n",
    "}\n",
    "expected_target = 'is_signal'\n",
    "\n",
    "actual_cols = set(df.columns)\n",
    "feature_cols = get_feature_columns(df)\n",
    "\n",
    "print(\"Schema Validation:\")\n",
    "print(f\"‚úÖ All expected features present: {expected_features == set(feature_cols)}\")\n",
    "print(f\"‚úÖ Target column present: {expected_target in df.columns}\")\n",
    "\n",
    "# Data type checks\n",
    "print(f\"\\n‚úÖ All features numeric: {df[feature_cols].select_dtypes(include=[np.number]).shape[1] == len(feature_cols)}\")\n",
    "print(f\"‚úÖ Target is binary: {set(df[expected_target].unique()) == {0, 1}}\")\n",
    "\n",
    "# Missingness check\n",
    "missing = df.isnull().sum()\n",
    "print(f\"\\n‚úÖ No missing values: {missing.sum() == 0}\")\n",
    "\n",
    "# Physics sanity checks\n",
    "print(\"\\nPhysics Constraints:\")\n",
    "print(f\"  p_T ‚â• 0: {(df['p_T'] >= 0).all()} ‚úÖ\")\n",
    "print(f\"  E_total ‚â• 0: {(df['E_total'] >= 0).all()} ‚úÖ\")\n",
    "print(f\"  0 ‚â§ b_tag_score ‚â§ 1: {((df['b_tag_score'] >= 0) & (df['b_tag_score'] <= 1)).all()} ‚úÖ\")\n",
    "print(f\"  -œÄ ‚â§ phi ‚â§ œÄ: {((df['phi'] >= -np.pi) & (df['phi'] <= np.pi)).all()} ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275deedd",
   "metadata": {},
   "source": [
    "## 3. Target Distribution Analysis\n",
    "\n",
    "**Critical First Step**: Understand your target distribution.\n",
    "\n",
    "For classification:\n",
    "- Class balance affects metric choice\n",
    "- Imbalance affects model training (need stratified sampling)\n",
    "- Baseline accuracy = majority class rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar plot\n",
    "counts = df['is_signal'].value_counts().sort_index()\n",
    "ax1.bar(['Background', 'Signal'], counts.values, alpha=0.7, \n",
    "        color=['steelblue', 'coral'], edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Class Distribution', fontsize=13, fontweight='bold')\n",
    "ax1.grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(counts.values):\n",
    "    ax1.text(i, v + 1000, f'{v:,}\\n({v/len(df):.1%})', \n",
    "             ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(counts.values, labels=['Background', 'Signal'], autopct='%1.1f%%',\n",
    "        colors=['steelblue', 'coral'], startangle=90, \n",
    "        wedgeprops={'edgecolor': 'black', 'linewidth': 1.5})\n",
    "ax2.set_title('Class Proportions', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '01_target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(f\"  ‚Ä¢ Imbalanced dataset: {df['is_signal'].mean():.1%} signal\")\n",
    "print(f\"  ‚Ä¢ Baseline accuracy (predict majority): {df['is_signal'].value_counts().max() / len(df):.1%}\")\n",
    "print(f\"  ‚Ä¢ ‚ö†Ô∏è Implications:\")\n",
    "print(f\"    - Need stratified splits to maintain class balance\")\n",
    "print(f\"    - Accuracy is misleading (use AUC-ROC, PR-AUC)\")\n",
    "print(f\"    - May need class weights or resampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9da8f",
   "metadata": {},
   "source": [
    "## 4. Feature Type Analysis\n",
    "\n",
    "**Categorize features** by type for appropriate preprocessing:\n",
    "- Continuous: normalize/standardize\n",
    "- Categorical: one-hot encode\n",
    "- Ordinal: encode preserving order\n",
    "- Counts: may need log-transform or binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58638071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature type analysis\n",
    "feature_types = {\n",
    "    'continuous': ['p_T', 'eta', 'phi', 'E_total', 'm_inv', 'missing_E_T', \n",
    "                   'delta_R', 'm_T', 'H_T'],\n",
    "    'bounded_continuous': ['b_tag_score', 'lepton_iso', 'E_ratio', \n",
    "                           'sphericity', 'aplanarity', 'centrality'],\n",
    "    'discrete_count': ['n_jets']\n",
    "}\n",
    "\n",
    "print(\"Feature Types:\")\n",
    "for ftype, features in feature_types.items():\n",
    "    print(f\"\\n{ftype.upper()} ({len(features)}):\")\n",
    "    for f in features:\n",
    "        print(f\"  ‚Ä¢ {f}\")\n",
    "\n",
    "# Distribution statistics\n",
    "print(\"\\n\\nDistribution Summary:\")\n",
    "print(df[feature_cols].describe().T[['mean', 'std', 'min', 'max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057aca0a",
   "metadata": {},
   "source": [
    "## 5. Missing Value Analysis\n",
    "\n",
    "**Best Practice**: Analyze missing patterns, don't just impute blindly.\n",
    "\n",
    "Questions to ask:\n",
    "- Are missings random or systematic?\n",
    "- Do missing indicators correlate with target?\n",
    "- Should we impute or use as a feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7976255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(\"Missing Value Summary:\")\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"\\n‚úÖ No missing values detected\")\n",
    "    print(\"   ‚Üí This is unusual for real-world data but expected for synthetic datasets\")\n",
    "    print(\"   ‚Üí In production, monitor for sudden changes in missingness patterns\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Total missing: {missing_df['Missing Count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c553b",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection\n",
    "\n",
    "**Purpose**: Identify extreme values that may indicate:\n",
    "- Data quality issues\n",
    "- Rare but valid events\n",
    "- Need for robust scaling/capping\n",
    "\n",
    "**Method**: Use IQR (Interquartile Range) method for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d149cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method\n",
    "def detect_outliers_iqr(data, multiplier=1.5):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    outliers = (data < lower_bound) | (data > upper_bound)\n",
    "    return outliers.sum(), lower_bound, upper_bound\n",
    "\n",
    "outlier_summary = []\n",
    "for col in feature_cols:\n",
    "    n_outliers, lower, upper = detect_outliers_iqr(df[col])\n",
    "    outlier_summary.append({\n",
    "        'Feature': col,\n",
    "        'Outliers': n_outliers,\n",
    "        'Outlier %': f\"{n_outliers/len(df)*100:.2f}%\",\n",
    "        'Lower Bound': f\"{lower:.2f}\",\n",
    "        'Upper Bound': f\"{upper:.2f}\"\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values('Outliers', ascending=False)\n",
    "print(\"Outlier Summary (Top 10):\")\n",
    "print(outlier_df.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize outliers for top feature\n",
    "top_feature = outlier_df.iloc[0]['Feature']\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.boxplot(df[top_feature], vert=False, widths=0.5)\n",
    "ax.set_xlabel(top_feature, fontsize=12)\n",
    "ax.set_title(f'Outlier Detection: {top_feature}', fontsize=13, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '01_outliers_boxplot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Interpretation:\")\n",
    "print(f\"  ‚Ä¢ Feature with most outliers: {top_feature}\")\n",
    "print(f\"  ‚Ä¢ In physics, 'outliers' may be real high-energy events\")\n",
    "print(f\"  ‚Ä¢ ‚ö†Ô∏è Decision: Keep outliers (physics-motivated), but use robust scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b4ec2",
   "metadata": {},
   "source": [
    "## 7. Feature Distributions by Class\n",
    "\n",
    "**Goal**: Understand which features discriminate between signal and background.\n",
    "\n",
    "**Good discriminators**:\n",
    "- Show clear separation between classes\n",
    "- Have different means/medians\n",
    "- Exhibit different shapes\n",
    "\n",
    "**Poor discriminators**:\n",
    "- Overlap heavily between classes\n",
    "- Similar distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c573e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by class (top 6 features)\n",
    "key_features = ['m_inv', 'missing_E_T', 'b_tag_score', 'lepton_iso', 'p_T', 'E_total']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    signal_data = df[df['is_signal'] == 1][feature]\n",
    "    background_data = df[df['is_signal'] == 0][feature]\n",
    "    \n",
    "    # Overlapping histograms\n",
    "    ax.hist(background_data, bins=50, alpha=0.5, label='Background', \n",
    "            color='steelblue', density=True, edgecolor='black', linewidth=0.5)\n",
    "    ax.hist(signal_data, bins=50, alpha=0.5, label='Signal', \n",
    "            color='coral', density=True, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel(feature, fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(feature, fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9, loc='upper right')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '01_feature_distributions_by_class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Visual Assessment:\")\n",
    "print(\"  ‚Ä¢ m_inv: Signal has clear peak at 125 GeV (Higgs mass) ‚úÖ Strong discriminator\")\n",
    "print(\"  ‚Ä¢ missing_E_T: Signal has higher missing energy ‚úÖ Good discriminator\")\n",
    "print(\"  ‚Ä¢ b_tag_score: Signal enriched in high b-tag ‚úÖ Good discriminator\")\n",
    "print(\"  ‚Ä¢ lepton_iso: Signal has better isolated leptons ‚úÖ Good discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9630869",
   "metadata": {},
   "source": [
    "## 8. Feature Correlations\n",
    "\n",
    "**Purpose**: Identify multicollinearity and feature relationships.\n",
    "\n",
    "**Why it matters**:\n",
    "- High correlation ‚Üí redundant features (consider dropping)\n",
    "- Physics correlations (e.g., E ‚àù p) are expected\n",
    "- Unexpected correlations may indicate leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df[feature_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')\n",
    "\n",
    "ax.set_xticks(np.arange(len(feature_cols)))\n",
    "ax.set_yticks(np.arange(len(feature_cols)))\n",
    "ax.set_xticklabels(feature_cols, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(feature_cols, fontsize=9)\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Correlation', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '01_correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr_threshold = 0.7\n",
    "high_corr_pairs = []\n",
    "for i in range(len(feature_cols)):\n",
    "    for j in range(i+1, len(feature_cols)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > high_corr_threshold:\n",
    "            high_corr_pairs.append((feature_cols[i], feature_cols[j], corr_val))\n",
    "\n",
    "print(f\"\\nHighly Correlated Pairs (|r| > {high_corr_threshold}):\")\n",
    "if high_corr_pairs:\n",
    "    for f1, f2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "        print(f\"  ‚Ä¢ {f1} <-> {f2}: r = {corr:.3f}\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No highly correlated pairs (good for linear models)\")\n",
    "    \n",
    "print(\"\\n‚ö†Ô∏è Note: Moderate correlations (0.3-0.7) are expected in physics data\")\n",
    "print(\"   E.g., E_total and p_T are physically related via E¬≤ = (pc)¬≤ + (mc¬≤)¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544aca6f",
   "metadata": {},
   "source": [
    "## 9. Potential Data Leakage Detection\n",
    "\n",
    "**Critical Check**: Identify features that may leak target information.\n",
    "\n",
    "**Red flags**:\n",
    "- Perfect or near-perfect correlation with target\n",
    "- Features computed using target\n",
    "- Future information in past predictions\n",
    "- Identifiers that proxy for target\n",
    "\n",
    "**Our dataset**: Synthetic, so leakage unlikely, but let's check systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leakage detection: correlation with target\n",
    "from sklearn.metrics import mutual_info_classif\n",
    "\n",
    "# Compute correlations and mutual information with target\n",
    "target_correlations = df[feature_cols].corrwith(df['is_signal']).abs().sort_values(ascending=False)\n",
    "\n",
    "# Mutual information (handles non-linear relationships)\n",
    "mi_scores = mutual_info_classif(df[feature_cols], df['is_signal'], random_state=42)\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Mutual Info': mi_scores\n",
    "}).sort_values('Mutual Info', ascending=False)\n",
    "\n",
    "print(\"Feature-Target Relationship Strength:\")\n",
    "print(\"\\nTop 5 by Absolute Correlation:\")\n",
    "print(target_correlations.head())\n",
    "\n",
    "print(\"\\nTop 5 by Mutual Information:\")\n",
    "print(mi_df.head().to_string(index=False))\n",
    "\n",
    "# Leakage red flags\n",
    "leakage_threshold = 0.9\n",
    "suspicious_features = target_correlations[target_correlations > leakage_threshold]\n",
    "\n",
    "if len(suspicious_features) > 0:\n",
    "    print(f\"\\nüö® LEAKAGE ALERT: Features with |corr| > {leakage_threshold}:\")\n",
    "    for feat, corr in suspicious_features.items():\n",
    "        print(f\"  ‚Ä¢ {feat}: {corr:.4f}\")\n",
    "    print(\"  ‚Üí These features may be computed using the target!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No obvious leakage detected (no |corr| > {leakage_threshold})\")\n",
    "    print(\"  ‚Üí But always review feature engineering pipeline manually\")\n",
    "\n",
    "print(\"\\nüìã Leakage Checklist:\")\n",
    "print(\"  1. ‚úÖ No features with perfect correlation to target\")\n",
    "print(\"  2. ‚úÖ All features are kinematic (physics-based, not target-derived)\")\n",
    "print(\"  3. ‚úÖ No temporal leakage (events are independent)\")\n",
    "print(\"  4. ‚úÖ No identifiers that might proxy for target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe11377",
   "metadata": {},
   "source": [
    "## 10. Summary Report\n",
    "\n",
    "Generate a compact EDA report for stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40549878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "eda_summary = {\n",
    "    'dataset': {\n",
    "        'name': 'Particle Collision Classification',\n",
    "        'total_samples': len(df),\n",
    "        'n_features': len(feature_cols),\n",
    "        'target': 'is_signal (binary)',\n",
    "        'signal_rate': f\"{df['is_signal'].mean():.1%}\",\n",
    "        'class_balance': '10% signal / 90% background (imbalanced)'\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'missing_values': missing_df['Missing Count'].sum(),\n",
    "        'duplicate_rows': df.duplicated().sum(),\n",
    "        'schema_valid': True,\n",
    "        'outliers_detected': outlier_df['Outliers'].sum()\n",
    "    },\n",
    "    'feature_insights': {\n",
    "        'top_discriminators': ['m_inv', 'missing_E_T', 'b_tag_score', 'lepton_iso'],\n",
    "        'highly_correlated_pairs': len(high_corr_pairs),\n",
    "        'leakage_suspects': len(suspicious_features)\n",
    "    },\n",
    "    'recommendations': {\n",
    "        'split_strategy': 'Stratified train/val/test (maintain 10% signal rate)',\n",
    "        'preprocessing': 'StandardScaler (robust to outliers)',\n",
    "        'metrics': 'AUC-ROC, PR-AUC (not accuracy due to imbalance)',\n",
    "        'modeling': 'Gradient boosting (handles feature interactions)',\n",
    "        'calibration': 'Required (check reliability diagram)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save report\n",
    "with open(reports_dir / '01_eda_summary.json', 'w') as f:\n",
    "    json.dump(eda_summary, f, indent=2)\n",
    "\n",
    "print(\"üìä EDA Summary Report\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: {eda_summary['dataset']['name']}\")\n",
    "print(f\"  Samples: {eda_summary['dataset']['total_samples']:,}\")\n",
    "print(f\"  Features: {eda_summary['dataset']['n_features']}\")\n",
    "print(f\"  Signal rate: {eda_summary['dataset']['signal_rate']}\")\n",
    "\n",
    "print(f\"\\nData Quality:\")\n",
    "print(f\"  ‚úÖ Missing values: {eda_summary['data_quality']['missing_values']}\")\n",
    "print(f\"  ‚úÖ Duplicate rows: {eda_summary['data_quality']['duplicate_rows']}\")\n",
    "print(f\"  ‚úÖ Schema valid: {eda_summary['data_quality']['schema_valid']}\")\n",
    "print(f\"  ‚ö†Ô∏è Outliers detected: {eda_summary['data_quality']['outliers_detected']:,} (expected in physics)\")\n",
    "\n",
    "print(f\"\\nTop Discriminators:\")\n",
    "for feat in eda_summary['feature_insights']['top_discriminators']:\n",
    "    print(f\"  ‚Ä¢ {feat}\")\n",
    "\n",
    "print(f\"\\nüîë Key Recommendations:\")\n",
    "for key, value in eda_summary['recommendations'].items():\n",
    "    print(f\"  ‚Ä¢ {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Report saved to: {reports_dir / '01_eda_summary.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900cd191",
   "metadata": {},
   "source": [
    "## 11. Exercises\n",
    "\n",
    "Complete these exercises to reinforce your learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e36cf9",
   "metadata": {},
   "source": [
    "**Exercise 1**: Identify potential leakage columns\n",
    "\n",
    "Given a hypothetical dataset with these columns:\n",
    "- `transaction_id` (unique identifier)\n",
    "- `user_age`, `user_income` (demographics)\n",
    "- `transaction_amount` (purchase amount)\n",
    "- `fraud_investigation_opened` (binary: was fraud investigation started?)\n",
    "- **Target**: `is_fraud` (binary: was transaction fraudulent?)\n",
    "\n",
    "Which column(s) would cause data leakage? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ea0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da7911",
   "metadata": {},
   "source": [
    "**Exercise 2**: Propose a split strategy\n",
    "\n",
    "For our particle collision dataset:\n",
    "1. What split ratio would you use (train/val/test)?\n",
    "2. Should you use stratification? Why or why not?\n",
    "3. Should you shuffle before splitting? Why or why not?\n",
    "4. Write pseudocode for the splitting logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ecb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c656f",
   "metadata": {},
   "source": [
    "**Exercise 3**: Detect outliers vs. valid rare events\n",
    "\n",
    "In particle physics, extremely high `p_T` (transverse momentum) events are rare but physically valid.\n",
    "\n",
    "How would you distinguish between:\n",
    "1. **Data quality outliers** (measurement errors, detector glitches)\n",
    "2. **Valid rare events** (high-energy physics processes)\n",
    "\n",
    "What checks would you implement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11663b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here:\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd481e6d",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db19ef8",
   "metadata": {},
   "source": [
    "**Solution 1**: Leakage columns\n",
    "\n",
    "**Leakage column**: `fraud_investigation_opened`\n",
    "\n",
    "**Why**: This column is **computed after** the target is known. Banks only open fraud investigations after detecting fraudulent activity. Using this feature gives the model perfect hindsight.\n",
    "\n",
    "**Other potential issues**:\n",
    "- `transaction_id`: Could leak if IDs are assigned sequentially and fraud rate changes over time\n",
    "- `user_age`, `user_income`: ‚úÖ Safe (demographics known before transaction)\n",
    "- `transaction_amount`: ‚úÖ Safe (observed at transaction time)\n",
    "\n",
    "**General rule**: Ask \"Was this feature available at prediction time?\" If no ‚Üí leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1 (code demonstration)\n",
    "print(\"Leakage Check:\")\n",
    "print(\"  fraud_investigation_opened ‚Üí TARGET: ‚ùå LEAKAGE\")\n",
    "print(\"  Reason: Investigation happens AFTER fraud detection\")\n",
    "print(\"\")\n",
    "print(\"Safe Features:\")\n",
    "print(\"  user_age ‚Üí Target: ‚úÖ Known before transaction\")\n",
    "print(\"  user_income ‚Üí Target: ‚úÖ Known before transaction\")\n",
    "print(\"  transaction_amount ‚Üí Target: ‚úÖ Observed at transaction time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031692c4",
   "metadata": {},
   "source": [
    "**Solution 2**: Split strategy\n",
    "\n",
    "```python\n",
    "# Recommended split strategy for particle collision dataset\n",
    "\n",
    "# 1. Split ratio: 60/20/20 (train/val/test)\n",
    "train_size = 0.60  # 60K samples for training\n",
    "val_size = 0.20    # 20K samples for validation (hyperparameter tuning, calibration)\n",
    "test_size = 0.20   # 20K samples for final evaluation (touch once!)\n",
    "\n",
    "# 2. Stratification: YES\n",
    "# Reason: Class imbalance (10% signal). Without stratification,\n",
    "# random split might give 9% signal in train and 11% in test ‚Üí biased evaluation.\n",
    "stratify = True\n",
    "\n",
    "# 3. Shuffling: YES\n",
    "# Reason: No temporal ordering in particle collisions (events are i.i.d.).\n",
    "# Shuffling ensures random split, not based on data collection order.\n",
    "shuffle = True\n",
    "\n",
    "# Pseudocode:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# \n",
    "# # First split: separate test set\n",
    "# train_val, test = train_test_split(data, test_size=0.2, \n",
    "#                                     stratify=data['is_signal'], \n",
    "#                                     shuffle=True, random_state=42)\n",
    "# \n",
    "# # Second split: separate validation from train\n",
    "# train, val = train_test_split(train_val, test_size=0.25,  # 0.25 * 0.8 = 0.2\n",
    "#                               stratify=train_val['is_signal'],\n",
    "#                               shuffle=True, random_state=42)\n",
    "```\n",
    "\n",
    "**Critical**: Fix `random_state=42` for reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2 (code verification)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Demonstrate stratified split\n",
    "train_val, test = train_test_split(df, test_size=0.2, \n",
    "                                    stratify=df['is_signal'], \n",
    "                                    random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.25, \n",
    "                              stratify=train_val['is_signal'], \n",
    "                              random_state=42)\n",
    "\n",
    "print(\"Split Verification:\")\n",
    "print(f\"Train: {len(train):,} samples, {train['is_signal'].mean():.1%} signal\")\n",
    "print(f\"Val:   {len(val):,} samples, {val['is_signal'].mean():.1%} signal\")\n",
    "print(f\"Test:  {len(test):,} samples, {test['is_signal'].mean():.1%} signal\")\n",
    "print(\"\\n‚úÖ Stratification preserved class balance across all splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e447e6",
   "metadata": {},
   "source": [
    "**Solution 3**: Outliers vs. valid rare events\n",
    "\n",
    "**Strategy**: Use domain knowledge + statistical checks.\n",
    "\n",
    "**Approach**:\n",
    "\n",
    "1. **Physics Constraints**:\n",
    "   ```python\n",
    "   # Hard constraints (if violated ‚Üí data error)\n",
    "   assert (df['E_total'] >= 0).all(), \"Energy cannot be negative\"\n",
    "   assert (df['p_T'] >= 0).all(), \"Momentum cannot be negative\"\n",
    "   assert (df['phi'].between(-np.pi, np.pi)).all(), \"Angle out of range\"\n",
    "   ```\n",
    "\n",
    "2. **Extreme Value Analysis**:\n",
    "   ```python\n",
    "   # Flag events beyond 5-sigma (extreme but rare)\n",
    "   z_scores = (df['p_T'] - df['p_T'].mean()) / df['p_T'].std()\n",
    "   extreme_events = df[np.abs(z_scores) > 5]\n",
    "   \n",
    "   # Manual review: Are these:\n",
    "   # - High-energy physics processes (valid)\n",
    "   # - Detector saturation (error)\n",
    "   # - Pile-up contamination (error)\n",
    "   ```\n",
    "\n",
    "3. **Consistency Checks**:\n",
    "   ```python\n",
    "   # Check energy-momentum relationship: E¬≤ ‚âà (pc)¬≤ + (mc¬≤)¬≤\n",
    "   # If violated significantly ‚Üí measurement error\n",
    "   energy_momentum_consistent = check_energy_momentum_relation(df)\n",
    "   ```\n",
    "\n",
    "4. **Decision Rule**:\n",
    "   - **Keep** if passes physics constraints + consistency checks\n",
    "   - **Flag** if extreme + inconsistent (review manually)\n",
    "   - **Cap** if using in production (e.g., `p_T_capped = min(p_T, 99th_percentile)`)\n",
    "\n",
    "**In this dataset**: All events pass physics checks ‚Üí keep all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a727221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3 (implementation)\n",
    "def check_outliers_vs_rare_events(df):\n",
    "    \"\"\"Distinguish outliers from valid rare events.\"\"\"\n",
    "    \n",
    "    # Physics constraint checks\n",
    "    constraints_passed = {\n",
    "        'E_total ‚â• 0': (df['E_total'] >= 0).all(),\n",
    "        'p_T ‚â• 0': (df['p_T'] >= 0).all(),\n",
    "        'missing_E_T ‚â• 0': (df['missing_E_T'] >= 0).all(),\n",
    "        '0 ‚â§ b_tag_score ‚â§ 1': ((df['b_tag_score'] >= 0) & (df['b_tag_score'] <= 1)).all(),\n",
    "        '-œÄ ‚â§ phi ‚â§ œÄ': ((df['phi'] >= -np.pi) & (df['phi'] <= np.pi)).all()\n",
    "    }\n",
    "    \n",
    "    print(\"Physics Constraint Checks:\")\n",
    "    for constraint, passed in constraints_passed.items():\n",
    "        status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "        print(f\"  {status} {constraint}\")\n",
    "    \n",
    "    # Extreme value detection\n",
    "    z_threshold = 5\n",
    "    extreme_features = []\n",
    "    for col in ['p_T', 'E_total', 'missing_E_T']:\n",
    "        z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "        n_extreme = (z_scores > z_threshold).sum()\n",
    "        if n_extreme > 0:\n",
    "            extreme_features.append((col, n_extreme))\n",
    "    \n",
    "    print(f\"\\nExtreme Value Analysis (>{z_threshold}œÉ):\")\n",
    "    if extreme_features:\n",
    "        for feat, count in extreme_features:\n",
    "            print(f\"  ‚ö†Ô∏è {feat}: {count} extreme events ({count/len(df)*100:.2f}%)\")\n",
    "        print(\"  ‚Üí Decision: Keep (physically valid high-energy events)\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ No extreme outliers detected\")\n",
    "    \n",
    "    return all(constraints_passed.values())\n",
    "\n",
    "# Run check\n",
    "valid = check_outliers_vs_rare_events(df)\n",
    "print(f\"\\n{'‚úÖ' if valid else '‚ùå'} Dataset passes all physics constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea63627",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Notebook Complete!\n",
    "\n",
    "**What you learned**:\n",
    "1. ‚úÖ Systematic EDA workflow (schema ‚Üí target ‚Üí features ‚Üí quality)\n",
    "2. ‚úÖ Data quality checks (missing values, outliers, constraints)\n",
    "3. ‚úÖ Leakage detection (correlation with target, temporal logic)\n",
    "4. ‚úÖ Feature analysis (distributions, correlations, discriminators)\n",
    "5. ‚úÖ Split strategy design (stratification, shuffling, reproducibility)\n",
    "\n",
    "**Outputs saved**:\n",
    "- `reports/01_target_distribution.png`\n",
    "- `reports/01_outliers_boxplot.png`\n",
    "- `reports/01_feature_distributions_by_class.png`\n",
    "- `reports/01_correlation_matrix.png`\n",
    "- `reports/01_eda_summary.json`\n",
    "\n",
    "**Next notebook**: `02_splitting_cv_and_leakage.ipynb` ‚Äî Learn proper train/val/test splits and demonstrate leakage with wrong pipelines."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
