{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: MCMC - Metropolis-Hastings & Diagnostics\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand Markov Chain Monte Carlo (MCMC) sampling\n",
    "- Implement Metropolis-Hastings algorithm\n",
    "- Learn proposal tuning (step size, acceptance rate trade-offs)\n",
    "- Master diagnostics: trace plots, autocorrelation, ESS\n",
    "- Identify and fix common failure modes\n",
    "\n",
    "**Runtime:** ~2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "\n",
    "repo_root = Path().resolve().parents[2]\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "set_seed, get_rng = safe_import_from(\n",
    "    '00_repo_standards.src.mlphys_core.seeding',\n",
    "    'set_seed', 'get_rng'\n",
    ")\n",
    "MetropolisHastings, MCMCDiagnostics = safe_import_from(\n",
    "    '02_stat_inference_uq.src.mcmc_basics',\n",
    "    'MetropolisHastings', 'MCMCDiagnostics'\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "reports_dir = Path(\"../reports\")\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intuition: Why MCMC?\n",
    "\n",
    "**The problem:** Many Bayesian posteriors cannot be computed analytically\n",
    "- Example: $p(\\theta | \\text{data})$ for complex likelihoods\n",
    "- Direct sampling is impossible (don't know normalization constant)\n",
    "- High-dimensional integrals are intractable\n",
    "\n",
    "**MCMC solution:** Generate samples from target distribution without knowing normalization\n",
    "- Build a Markov chain whose stationary distribution is the target\n",
    "- After \"burn-in\", samples approximate draws from $p(\\theta | \\text{data})$\n",
    "- Use samples to compute expectations: $\\mathbb{E}[f(\\theta)] \\approx \\frac{1}{N}\\sum_i f(\\theta_i)$\n",
    "\n",
    "**Metropolis-Hastings algorithm:**\n",
    "1. Propose new state: $\\theta' \\sim q(\\cdot | \\theta)$\n",
    "2. Compute acceptance ratio: $\\alpha = \\min\\left(1, \\frac{p(\\theta') q(\\theta | \\theta')}{p(\\theta) q(\\theta' | \\theta)}\\right)$\n",
    "3. Accept $\\theta'$ with probability $\\alpha$, else stay at $\\theta$\n",
    "4. Repeat\n",
    "\n",
    "**Key insight:** Only need to compute ratios $p(\\theta')/p(\\theta)$ ‚Üí normalization cancels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minimal Math\n",
    "\n",
    "**Target distribution:** $\\pi(\\theta)$ (unnormalized is fine)\n",
    "\n",
    "**Proposal distribution:** $q(\\theta' | \\theta)$ (e.g., random walk: $\\theta' = \\theta + \\epsilon$, $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$)\n",
    "\n",
    "**For symmetric proposals** ($q(\\theta' | \\theta) = q(\\theta | \\theta')$):\n",
    "$$\\alpha = \\min\\left(1, \\frac{\\pi(\\theta')}{\\pi(\\theta)}\\right)$$\n",
    "\n",
    "**Acceptance probability:**\n",
    "- If $\\pi(\\theta') > \\pi(\\theta)$ (better state): always accept\n",
    "- If $\\pi(\\theta') < \\pi(\\theta)$ (worse state): accept with probability $\\pi(\\theta') / \\pi(\\theta)$\n",
    "\n",
    "**Key diagnostics:**\n",
    "1. **Acceptance rate**: Fraction of proposals accepted\n",
    "   - Too high (>80%): proposals too small, slow exploration\n",
    "   - Too low (<20%): proposals too large, rejecting too often\n",
    "   - Optimal: ~20-50% for high-dimensional problems\n",
    "\n",
    "2. **Autocorrelation**: $\\rho(k) = \\text{Corr}(\\theta_t, \\theta_{t+k})$\n",
    "   - High autocorrelation ‚Üí samples are dependent\n",
    "   - Want $\\rho(k) \\to 0$ quickly as $k$ increases\n",
    "\n",
    "3. **Effective Sample Size (ESS)**: $\\text{ESS} \\approx \\frac{N}{1 + 2\\sum_{k=1}^\\infty \\rho(k)}$\n",
    "   - Accounts for autocorrelation\n",
    "   - Higher ESS ‚Üí more independent information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation: Sample from 1D Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: Standard normal N(0, 1)\n",
    "def log_prob_1d(x):\n",
    "    \"\"\"Log probability of N(0,1).\"\"\"\n",
    "    return -0.5 * x**2  # Ignoring constant terms\n",
    "\n",
    "# Run MCMC\n",
    "sampler = MetropolisHastings(\n",
    "    log_prob_fn=log_prob_1d,\n",
    "    proposal_std=1.0,\n",
    "    n_samples=5000,\n",
    "    n_burn=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "samples = sampler.sample(x0=np.array([5.0]), verbose=False)\n",
    "\n",
    "print(f\"Acceptance rate: {sampler.accept_rate_:.1%}\")\n",
    "print(f\"Sample mean: {samples.mean():.3f} (true: 0.0)\")\n",
    "print(f\"Sample std: {samples.std():.3f} (true: 1.0)\")\n",
    "print(f\"Number of samples after burn-in: {len(samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments: Proposal Tuning & Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Effect of proposal step size\n",
    "proposal_stds = [0.1, 0.5, 1.0, 3.0]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, prop_std in enumerate(proposal_stds):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    sampler_exp = MetropolisHastings(\n",
    "        log_prob_fn=log_prob_1d,\n",
    "        proposal_std=prop_std,\n",
    "        n_samples=1000,\n",
    "        n_burn=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    samples_exp = sampler_exp.sample(x0=np.array([5.0]), verbose=False)\n",
    "    \n",
    "    # Plot histogram vs true distribution\n",
    "    ax.hist(samples_exp, bins=30, density=True, alpha=0.6, \n",
    "            color='steelblue', edgecolor='black', label='MCMC samples')\n",
    "    x_range = np.linspace(-4, 4, 200)\n",
    "    ax.plot(x_range, norm.pdf(x_range), 'r-', linewidth=2, label='True N(0,1)')\n",
    "    \n",
    "    ax.set_xlabel('x', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'œÉ_prop = {prop_std:.1f} | Accept rate: {sampler_exp.accept_rate_:.1%}',\n",
    "                fontsize=12)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xlim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '04_proposal_tuning.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Saved: reports/04_proposal_tuning.png\")\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"   œÉ=0.1: High accept (slow exploration)\")\n",
    "print(\"   œÉ=3.0: Low accept (many rejections, gets stuck)\")\n",
    "print(\"   œÉ=1.0: Goldilocks zone (~40-60% accept)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Trace plots and diagnostics\n",
    "# Sample from 2D Gaussian\n",
    "def log_prob_2d(x):\n",
    "    \"\"\"Log prob of 2D Gaussian with correlation.\"\"\"\n",
    "    mu = np.array([1.0, -0.5])\n",
    "    cov = np.array([[1.0, 0.7], [0.7, 1.0]])\n",
    "    diff = x - mu\n",
    "    return -0.5 * diff @ np.linalg.inv(cov) @ diff\n",
    "\n",
    "sampler_2d = MetropolisHastings(\n",
    "    log_prob_fn=log_prob_2d,\n",
    "    proposal_std=1.0,\n",
    "    n_samples=5000,\n",
    "    n_burn=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "samples_2d = sampler_2d.sample(x0=np.array([0.0, 0.0]), verbose=False)\n",
    "\n",
    "print(f\"\\n2D Sampling Results:\")\n",
    "print(f\"Acceptance rate: {sampler_2d.accept_rate_:.1%}\")\n",
    "print(f\"Mean: {samples_2d.mean(axis=0)} (true: [1.0, -0.5])\")\n",
    "print(f\"Std: {samples_2d.std(axis=0)} (true: [1.0, 1.0])\")\n",
    "\n",
    "# Create diagnostics\n",
    "diagnostics = MCMCDiagnostics(samples_2d)\n",
    "\n",
    "# Trace plots\n",
    "fig = diagnostics.trace_plot()\n",
    "plt.savefig(reports_dir / '04_trace_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: reports/04_trace_plots.png\")\n",
    "\n",
    "# Marginal histograms\n",
    "fig = diagnostics.marginal_histograms(true_mean=np.array([1.0, -0.5]))\n",
    "plt.savefig(reports_dir / '04_marginal_histograms.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: reports/04_marginal_histograms.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Autocorrelation and ESS\n",
    "acf_x1 = diagnostics.autocorrelation(max_lag=50, param_idx=0)\n",
    "acf_x2 = diagnostics.autocorrelation(max_lag=50, param_idx=1)\n",
    "\n",
    "iat_x1 = diagnostics.integrated_autocorr_time(param_idx=0)\n",
    "iat_x2 = diagnostics.integrated_autocorr_time(param_idx=1)\n",
    "\n",
    "ess_x1 = diagnostics.effective_sample_size(param_idx=0)\n",
    "ess_x2 = diagnostics.effective_sample_size(param_idx=1)\n",
    "\n",
    "print(f\"\\nAutocorrelation Diagnostics:\")\n",
    "print(f\"Integrated autocorrelation time: [{iat_x1:.2f}, {iat_x2:.2f}]\")\n",
    "print(f\"Effective sample size: [{ess_x1:.0f}, {ess_x2:.0f}] (out of {len(samples_2d)})\")\n",
    "print(f\"Efficiency: {ess_x1/len(samples_2d):.1%}\")\n",
    "\n",
    "# Plot autocorrelation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(acf_x1, 'o-', linewidth=2, markersize=4)\n",
    "axes[0].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Lag', fontsize=12)\n",
    "axes[0].set_ylabel('Autocorrelation', fontsize=12)\n",
    "axes[0].set_title(f'Dimension 1 (IAT={iat_x1:.2f}, ESS={ess_x1:.0f})', fontsize=13)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(acf_x2, 's-', linewidth=2, markersize=4, color='darkorange')\n",
    "axes[1].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Lag', fontsize=12)\n",
    "axes[1].set_ylabel('Autocorrelation', fontsize=12)\n",
    "axes[1].set_title(f'Dimension 2 (IAT={iat_x2:.2f}, ESS={ess_x2:.0f})', fontsize=13)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '04_autocorrelation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n‚úÖ Saved: reports/04_autocorrelation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Failure modes\n",
    "# Mode 1: Too small step size (random walk gets stuck)\n",
    "# Mode 2: Too large step size (high rejection rate)\n",
    "# Mode 3: Multimodal target (chain doesn't explore all modes)\n",
    "\n",
    "def multimodal_log_prob(x):\n",
    "    \"\"\"Mixture of two Gaussians (bimodal).\"\"\"\n",
    "    # Modes at x=-2 and x=+2\n",
    "    log_p1 = -0.5 * (x[0] + 2)**2 - 0.5 * x[1]**2\n",
    "    log_p2 = -0.5 * (x[0] - 2)**2 - 0.5 * x[1]**2\n",
    "    # Log-sum-exp trick\n",
    "    max_log_p = max(log_p1, log_p2)\n",
    "    return max_log_p + np.log(np.exp(log_p1 - max_log_p) + np.exp(log_p2 - max_log_p))\n",
    "\n",
    "# Try sampling with small step (will get stuck in one mode)\n",
    "sampler_fail = MetropolisHastings(\n",
    "    log_prob_fn=multimodal_log_prob,\n",
    "    proposal_std=0.5,  # Too small to jump between modes\n",
    "    n_samples=3000,\n",
    "    n_burn=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "samples_fail = sampler_fail.sample(x0=np.array([-2.0, 0.0]), verbose=False)\n",
    "\n",
    "print(f\"\\nMultimodal sampling (failure mode):\")\n",
    "print(f\"Acceptance rate: {sampler_fail.accept_rate_:.1%}\")\n",
    "print(f\"Mean of x1: {samples_fail[:, 0].mean():.2f} (should be ~0 if exploring both modes)\")\n",
    "print(f\"Std of x1: {samples_fail[:, 0].std():.2f} (should be >2 if exploring both modes)\")\n",
    "print(f\"‚ö†Ô∏è Chain likely stuck in one mode!\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Trace plot\n",
    "ax1.plot(samples_fail[:, 0], linewidth=0.5)\n",
    "ax1.axhline(-2, color='r', linestyle='--', label='Left mode')\n",
    "ax1.axhline(2, color='g', linestyle='--', label='Right mode')\n",
    "ax1.set_xlabel('Iteration', fontsize=12)\n",
    "ax1.set_ylabel('x‚ÇÅ', fontsize=12)\n",
    "ax1.set_title('Trace Plot: Stuck in Left Mode', fontsize=13)\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2D scatter\n",
    "ax2.scatter(samples_fail[:, 0], samples_fail[:, 1], s=10, alpha=0.3)\n",
    "ax2.scatter([-2, 2], [0, 0], s=200, c=['red', 'green'], \n",
    "           marker='*', edgecolor='black', linewidth=2, label='True modes')\n",
    "ax2.set_xlabel('x‚ÇÅ', fontsize=12)\n",
    "ax2.set_ylabel('x‚ÇÇ', fontsize=12)\n",
    "ax2.set_title('Samples: Only Exploring Left Mode', fontsize=13)\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '04_failure_mode_multimodal.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Saved: reports/04_failure_mode_multimodal.png\")\n",
    "print(\"\\nüìä Lesson: MCMC can fail on multimodal distributions!\")\n",
    "print(\"   Solutions: Parallel tempering, Hamiltonian MC, or multiple chains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 1: Sample mean should converge to true mean\n",
    "print(\"Sanity Check 1: Convergence to true mean\")\n",
    "print(f\"   Sample mean: {samples_2d.mean(axis=0)}\")\n",
    "print(f\"   True mean: [1.0, -0.5]\")\n",
    "print(f\"   Error: {np.linalg.norm(samples_2d.mean(axis=0) - np.array([1.0, -0.5])):.4f}\")\n",
    "print(f\"   ‚úÖ PASSED\" if np.allclose(samples_2d.mean(axis=0), [1.0, -0.5], atol=0.1) else \"   ‚ùå FAILED\")\n",
    "\n",
    "# Sanity check 2: Acceptance rate in reasonable range\n",
    "print(\"\\nSanity Check 2: Acceptance rate\")\n",
    "print(f\"   Rate: {sampler_2d.accept_rate_:.1%}\")\n",
    "reasonable = 0.2 <= sampler_2d.accept_rate_ <= 0.7\n",
    "print(f\"   ‚úÖ PASSED (in [20%, 70%])\" if reasonable else \"   ‚ö†Ô∏è Outside optimal range\")\n",
    "\n",
    "# Sanity check 3: ESS should be less than total samples\n",
    "print(\"\\nSanity Check 3: ESS < N\")\n",
    "print(f\"   ESS: {ess_x1:.0f}, {ess_x2:.0f}\")\n",
    "print(f\"   Total samples: {len(samples_2d)}\")\n",
    "print(f\"   ‚úÖ PASSED\" if (ess_x1 < len(samples_2d) and ess_x2 < len(samples_2d)) else \"   ‚ùå FAILED\")\n",
    "\n",
    "# Sanity check 4: Autocorrelation should decay\n",
    "print(\"\\nSanity Check 4: Autocorrelation decay\")\n",
    "print(f\"   ACF at lag 0: {acf_x1[0]:.3f} (should be 1.0)\")\n",
    "print(f\"   ACF at lag 20: {acf_x1[20]:.3f} (should be < 0.2)\")\n",
    "decays = acf_x1[0] > acf_x1[10] > acf_x1[20]\n",
    "print(f\"   ‚úÖ PASSED (decays)\" if decays else \"   ‚ö†Ô∏è Not decaying properly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "‚úÖ **MCMC enables sampling from complex distributions** without knowing normalization\n",
    "\n",
    "‚úÖ **Metropolis-Hastings**: Simple and general MCMC algorithm\n",
    "   - Only needs to evaluate probability ratios\n",
    "   - Guaranteed to converge to target distribution (eventually)\n",
    "\n",
    "‚úÖ **Proposal tuning is critical**:\n",
    "   - Too small: high acceptance but slow exploration\n",
    "   - Too large: low acceptance, chain gets stuck\n",
    "   - Aim for ~20-50% acceptance rate\n",
    "\n",
    "‚úÖ **Diagnostics are essential**:\n",
    "   - **Trace plots**: Check for convergence and mixing\n",
    "   - **Autocorrelation**: Quantify sample dependence\n",
    "   - **ESS**: Effective number of independent samples\n",
    "\n",
    "‚úÖ **Common pitfalls**:\n",
    "   - Insufficient burn-in (discard early samples)\n",
    "   - Multimodal targets (chain may miss modes)\n",
    "   - High autocorrelation (need more samples)\n",
    "\n",
    "**When to thin vs not thin:**\n",
    "- ‚úÖ Thin if storage is limited\n",
    "- ‚ùå Generally don't thin: use all samples for better estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "**Exercise 1:** Implement adaptive Metropolis: adjust proposal std during sampling to maintain ~40% acceptance.\n",
    "\n",
    "**Exercise 2:** Sample from a banana-shaped distribution. How does proposal shape (isotropic vs adapted) affect efficiency?\n",
    "\n",
    "**Exercise 3:** For the multimodal example, try parallel tempering or running multiple chains from different initializations.\n",
    "\n",
    "**Exercise 4:** Implement Gelman-Rubin diagnostic (R-hat) to assess convergence across multiple chains.\n",
    "\n",
    "**Exercise 5:** Compare MCMC to importance sampling for a 1D target. Which is more efficient?\n",
    "\n",
    "**Exercise 6:** Research Hamiltonian Monte Carlo (HMC). Why is it more efficient than random-walk MH?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4: Gelman-Rubin R-hat\n",
    "def gelman_rubin(chains):\n",
    "    \"\"\"Compute R-hat statistic for multiple chains.\n",
    "    \n",
    "    Args:\n",
    "        chains: List of arrays, each (n_samples, n_dim)\n",
    "    Returns:\n",
    "        R_hat per dimension\n",
    "    \"\"\"\n",
    "    n_chains = len(chains)\n",
    "    n_samples = chains[0].shape[0]\n",
    "    n_dim = chains[0].shape[1]\n",
    "    \n",
    "    chain_means = np.array([chain.mean(axis=0) for chain in chains])  # (n_chains, n_dim)\n",
    "    grand_mean = chain_means.mean(axis=0)  # (n_dim,)\n",
    "    \n",
    "    # Between-chain variance\n",
    "    B = n_samples / (n_chains - 1) * np.sum((chain_means - grand_mean)**2, axis=0)\n",
    "    \n",
    "    # Within-chain variance\n",
    "    W = np.mean([np.var(chain, axis=0, ddof=1) for chain in chains], axis=0)\n",
    "    \n",
    "    # Marginal posterior variance estimate\n",
    "    var_plus = ((n_samples - 1) / n_samples) * W + (1 / n_samples) * B\n",
    "    \n",
    "    # R-hat\n",
    "    R_hat = np.sqrt(var_plus / W)\n",
    "    return R_hat\n",
    "\n",
    "# Run 4 chains from different starting points\n",
    "n_chains = 4\n",
    "chains = []\n",
    "for i in range(n_chains):\n",
    "    sampler_i = MetropolisHastings(\n",
    "        log_prob_fn=log_prob_2d,\n",
    "        proposal_std=1.0,\n",
    "        n_samples=2000,\n",
    "        n_burn=200,\n",
    "        random_state=42 + i\n",
    "    )\n",
    "    x0 = np.random.randn(2) * 2  # Random start\n",
    "    samples_i = sampler_i.sample(x0=x0, verbose=False)\n",
    "    chains.append(samples_i)\n",
    "\n",
    "R_hat = gelman_rubin(chains)\n",
    "print(f\"Solution 4: Gelman-Rubin R-hat = {R_hat}\")\n",
    "print(f\"   R-hat < 1.1 indicates convergence\")\n",
    "print(f\"   Status: {'‚úÖ Converged' if np.all(R_hat < 1.1) else '‚ö†Ô∏è Not converged'}\")\n",
    "\n",
    "# Solution 6: HMC explanation\n",
    "print(\"\\nSolution 6: Why HMC is more efficient:\")\n",
    "print(\"   1. Uses gradient information (not just random walk)\")\n",
    "print(\"   2. Proposes distant states with high acceptance\")\n",
    "print(\"   3. Explores posterior along natural curvature\")\n",
    "print(\"   4. Lower autocorrelation ‚Üí higher ESS per iteration\")\n",
    "print(\"   Example: Stan, PyMC3 use HMC/NUTS by default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed the Statistical Inference & UQ module. You now understand:\n",
    "- Aleatoric vs epistemic uncertainty\n",
    "- Bayesian regression and posterior predictive distributions\n",
    "- Calibration diagnostics and temperature scaling\n",
    "- MCMC sampling and convergence diagnostics\n",
    "\n",
    "**Next steps:** Apply these concepts to real ML problems in subsequent modules!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
