{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, brier_score_loss\n",
    "\n",
    "# Add repo root\n",
    "repo_root = Path().resolve().parents[2]\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "# Import utilities\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core.seeding', 'set_seed')\n",
    "reliability_diagram, expected_calibration_error, TemperatureScaling = safe_import_from(\n",
    "    '02_stat_inference_uq.src.calibration',\n",
    "    'reliability_diagram', 'expected_calibration_error', 'TemperatureScaling'\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "reports_dir = Path(\"../reports\")\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b5dac",
   "metadata": {},
   "source": [
    "## 1. Intuition: What is Calibration?\n",
    "\n",
    "**Definition:** A probabilistic classifier is **calibrated** if its predicted probabilities match empirical frequencies.\n",
    "\n",
    "**Example:**\n",
    "- If a model predicts 80% confidence for 100 predictions, ~80 should be correct\n",
    "- **Well-calibrated**: Among predictions with confidence 0.7, exactly 70% are correct\n",
    "- **Overconfident**: Among predictions with confidence 0.9, only 70% are correct\n",
    "- **Underconfident**: Among predictions with confidence 0.6, actually 80% are correct\n",
    "\n",
    "**Why it matters:**\n",
    "- **Decision-making**: If a medical model says \"95% chance of disease\", you want that to be accurate!\n",
    "- **Cost-sensitive applications**: Need reliable probabilities to set thresholds\n",
    "- **Modern deep networks are often overconfident** (high accuracy but poor calibration)\n",
    "\n",
    "**Key insight:** High accuracy ‚â† good calibration!\n",
    "- A model can be 95% accurate but severely miscalibrated\n",
    "- Temperature scaling fixes calibration without changing predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a8ff3",
   "metadata": {},
   "source": [
    "## 2. Minimal Math: Calibration Metrics\n",
    "\n",
    "### Reliability Diagram\n",
    "- **Bin predictions** by confidence: $[0.0, 0.1), [0.1, 0.2), ..., [0.9, 1.0]$\n",
    "- For bin $B_m$: \n",
    "  - Mean predicted probability: $\\bar{p}_m = \\frac{1}{|B_m|} \\sum_{i \\in B_m} p_i$\n",
    "  - Empirical accuracy: $\\bar{y}_m = \\frac{1}{|B_m|} \\sum_{i \\in B_m} y_i$\n",
    "- **Perfect calibration**: $\\bar{p}_m = \\bar{y}_m$ for all bins\n",
    "\n",
    "### Expected Calibration Error (ECE)\n",
    "$$\\text{ECE} = \\sum_{m=1}^M \\frac{|B_m|}{n} \\left| \\bar{p}_m - \\bar{y}_m \\right|$$\n",
    "\n",
    "**Interpretation:** Weighted average calibration gap across bins\n",
    "\n",
    "### Temperature Scaling\n",
    "- **Pre-calibration logits**: $z$ (model outputs before softmax)\n",
    "- **Post-calibration**: $p_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$\n",
    "- **Temperature $T$**: Learned on validation set to minimize NLL\n",
    "  - $T > 1$ ‚Üí softer probabilities (less confident)\n",
    "  - $T < 1$ ‚Üí sharper probabilities (more confident)\n",
    "  - $T = 1$ ‚Üí original probabilities\n",
    "\n",
    "**Key property:** Temperature scaling **preserves accuracy** (argmax unchanged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff62de4",
   "metadata": {},
   "source": [
    "## 3. Implementation: Generate Overconfident Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d95c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=3,\n",
    "    n_classes=2,\n",
    "    flip_y=0.05,  # 5% label noise\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split: train / validation (for temp scaling) / test\n",
    "X_train, X_temp_test, y_train, y_temp_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp_test, y_temp_test, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset sizes: train={len(y_train)}, val={len(y_val)}, test={len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model that tends to be overconfident (small neural network)\n",
    "# We'll use a shallow MLP with L2 regularization that produces overconfident predictions\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(50, 25),\n",
    "    activation='relu',\n",
    "    max_iter=500,\n",
    "    alpha=0.001,  # Small L2 penalty\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_prob_train = model.predict_proba(X_train)[:, 1]\n",
    "y_prob_val = model.predict_proba(X_val)[:, 1]\n",
    "y_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate accuracy\n",
    "acc_test = accuracy_score(y_test, model.predict(X_test))\n",
    "print(f\"\\nTest accuracy: {acc_test:.1%}\")\n",
    "print(f\"Average predicted probability: {y_prob_test.mean():.3f}\")\n",
    "print(f\"Actual positive rate: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135ea81",
   "metadata": {},
   "source": [
    "## 4. Experiments: Diagnosing Miscalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae57bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Reliability diagram BEFORE calibration\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "bin_centers, bin_accs, bin_counts = reliability_diagram(\n",
    "    y_test, y_prob_test, n_bins=10, strategy='uniform', ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('Reliability Diagram (BEFORE Temperature Scaling)', fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '03_reliability_before.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compute ECE\n",
    "ece_before = expected_calibration_error(y_test, y_prob_test, n_bins=10)\n",
    "print(f\"\\n‚úÖ ECE before calibration: {ece_before:.4f}\")\n",
    "print(f\"   (Lower is better; 0 = perfect calibration)\")\n",
    "print(\"\\nüìä Observation: Bars below diagonal => overconfident predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Apply temperature scaling\n",
    "temp_scaler = TemperatureScaling()\n",
    "\n",
    "# Fit temperature on validation set (NOT test set!)\n",
    "# Need to get logits, not probabilities\n",
    "# For sklearn, we'll work with probabilities and approximate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Simple temperature scaling for binary case\n",
    "# We'll use a custom implementation since sklearn doesn't expose logits easily\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def calibrate_binary(y_true, y_prob):\n",
    "    \"\"\"Find optimal temperature for binary classification.\"\"\"\n",
    "    def nll(T):\n",
    "        # Clip to avoid log(0)\n",
    "        p_calibrated = np.clip(y_prob ** (1/T) / (y_prob**(1/T) + (1-y_prob)**(1/T)), 1e-7, 1-1e-7)\n",
    "        return -np.mean(y_true * np.log(p_calibrated) + (1-y_true) * np.log(1-p_calibrated))\n",
    "    \n",
    "    result = minimize_scalar(nll, bounds=(0.1, 10.0), method='bounded')\n",
    "    return result.x\n",
    "\n",
    "# Find optimal temperature\n",
    "T_opt = calibrate_binary(y_val, y_prob_val)\n",
    "print(f\"Optimal temperature: T = {T_opt:.3f}\")\n",
    "print(f\"   T > 1 means model was overconfident (soften predictions)\")\n",
    "print(f\"   T < 1 means model was underconfident (sharpen predictions)\")\n",
    "\n",
    "# Apply calibration to test set\n",
    "y_prob_calibrated = np.clip(\n",
    "    y_prob_test ** (1/T_opt) / (y_prob_test**(1/T_opt) + (1-y_prob_test)**(1/T_opt)),\n",
    "    1e-7, 1-1e-7\n",
    ")\n",
    "\n",
    "print(f\"\\nAverage probability before: {y_prob_test.mean():.3f}\")\n",
    "print(f\"Average probability after: {y_prob_calibrated.mean():.3f}\")\n",
    "print(f\"Actual positive rate: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e4cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Reliability diagram AFTER calibration\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "bin_centers_cal, bin_accs_cal, bin_counts_cal = reliability_diagram(\n",
    "    y_test, y_prob_calibrated, n_bins=10, strategy='uniform', ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('Reliability Diagram (AFTER Temperature Scaling)', fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '03_reliability_after.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "ece_after = expected_calibration_error(y_test, y_prob_calibrated, n_bins=10)\n",
    "print(f\"\\n‚úÖ ECE after calibration: {ece_after:.4f}\")\n",
    "print(f\"   Improvement: {(ece_before - ece_after)/ece_before * 100:.1f}% reduction\")\n",
    "print(\"\\nüìä Observation: Bars now closer to diagonal => better calibrated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Side-by-side comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Before\n",
    "reliability_diagram(y_test, y_prob_test, n_bins=10, strategy='uniform', ax=ax1)\n",
    "ax1.set_title(f'BEFORE (ECE={ece_before:.4f})', fontsize=14, weight='bold')\n",
    "\n",
    "# After\n",
    "reliability_diagram(y_test, y_prob_calibrated, n_bins=10, strategy='uniform', ax=ax2)\n",
    "ax2.set_title(f'AFTER (ECE={ece_after:.4f})', fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '03_calibration_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Saved: reports/03_calibration_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77baa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: Comprehensive metrics comparison\n",
    "def compute_metrics(y_true, y_prob, y_pred):\n",
    "    \"\"\"Compute accuracy, NLL, Brier, ECE.\"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    # NLL (negative log-likelihood)\n",
    "    y_prob_clipped = np.clip(y_prob, 1e-7, 1-1e-7)\n",
    "    nll = -np.mean(y_true * np.log(y_prob_clipped) + (1-y_true) * np.log(1-y_prob_clipped))\n",
    "    # Brier score\n",
    "    brier = np.mean((y_prob - y_true)**2)\n",
    "    # ECE\n",
    "    ece = expected_calibration_error(y_true, y_prob, n_bins=10)\n",
    "    return {'accuracy': acc, 'nll': nll, 'brier': brier, 'ece': ece}\n",
    "\n",
    "# Before calibration\n",
    "y_pred_before = (y_prob_test >= 0.5).astype(int)\n",
    "metrics_before = compute_metrics(y_test, y_prob_test, y_pred_before)\n",
    "\n",
    "# After calibration\n",
    "y_pred_after = (y_prob_calibrated >= 0.5).astype(int)\n",
    "metrics_after = compute_metrics(y_test, y_prob_calibrated, y_pred_after)\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METRICS COMPARISON: BEFORE vs AFTER CALIBRATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Before':>15} {'After':>15} {'Change':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for metric in ['accuracy', 'nll', 'brier', 'ece']:\n",
    "    before = metrics_before[metric]\n",
    "    after = metrics_after[metric]\n",
    "    change = after - before\n",
    "    symbol = '‚Üì' if change < 0 else ('‚Üë' if change > 0 else '=')\n",
    "    print(f\"{metric.upper():<20} {before:>15.4f} {after:>15.4f} {symbol:>2} {abs(change):>7.4f}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(\"Key observations:\")\n",
    "print(\"  - Accuracy UNCHANGED (temperature scaling preserves argmax)\")\n",
    "print(\"  - NLL IMPROVED (better probability estimates)\")\n",
    "print(\"  - Brier IMPROVED (closer to true probabilities)\")\n",
    "print(\"  - ECE IMPROVED (better calibrated)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save metrics to file\n",
    "import json\n",
    "metrics_dict = {\n",
    "    'before': {k: float(v) for k, v in metrics_before.items()},\n",
    "    'after': {k: float(v) for k, v in metrics_after.items()},\n",
    "    'temperature': float(T_opt)\n",
    "}\n",
    "with open(reports_dir / '03_calibration_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Saved: reports/03_calibration_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e871a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6: Threshold decision impact\n",
    "# Show how calibration affects decisions at different thresholds\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 17)\n",
    "precision_before = []\n",
    "recall_before = []\n",
    "precision_after = []\n",
    "recall_after = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Before\n",
    "    y_pred_th_before = (y_prob_test >= threshold).astype(int)\n",
    "    tp_before = np.sum((y_pred_th_before == 1) & (y_test == 1))\n",
    "    fp_before = np.sum((y_pred_th_before == 1) & (y_test == 0))\n",
    "    fn_before = np.sum((y_pred_th_before == 0) & (y_test == 1))\n",
    "    \n",
    "    prec_before = tp_before / (tp_before + fp_before) if (tp_before + fp_before) > 0 else 0\n",
    "    rec_before = tp_before / (tp_before + fn_before) if (tp_before + fn_before) > 0 else 0\n",
    "    precision_before.append(prec_before)\n",
    "    recall_before.append(rec_before)\n",
    "    \n",
    "    # After\n",
    "    y_pred_th_after = (y_prob_calibrated >= threshold).astype(int)\n",
    "    tp_after = np.sum((y_pred_th_after == 1) & (y_test == 1))\n",
    "    fp_after = np.sum((y_pred_th_after == 1) & (y_test == 0))\n",
    "    fn_after = np.sum((y_pred_th_after == 0) & (y_test == 1))\n",
    "    \n",
    "    prec_after = tp_after / (tp_after + fp_after) if (tp_after + fp_after) > 0 else 0\n",
    "    rec_after = tp_after / (tp_after + fn_after) if (tp_after + fn_after) > 0 else 0\n",
    "    precision_after.append(prec_after)\n",
    "    recall_after.append(rec_after)\n",
    "\n",
    "# Plot precision-recall curves\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "ax.plot(recall_before, precision_before, 'o-', linewidth=2, markersize=6,\n",
    "        label='Before calibration', alpha=0.8)\n",
    "ax.plot(recall_after, precision_after, 's-', linewidth=2, markersize=6,\n",
    "        label='After calibration', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall at Different Thresholds', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '03_threshold_decisions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Saved: reports/03_threshold_decisions.png\")\n",
    "print(\"\\nüìä Insight: Calibrated probabilities lead to more reliable threshold tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5685a2",
   "metadata": {},
   "source": [
    "## 5. Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982bdbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 1: Temperature > 1 for overconfident models\n",
    "print(\"Sanity Check 1: Temperature value\")\n",
    "print(f\"   T = {T_opt:.3f}\")\n",
    "if T_opt > 1.0:\n",
    "    print(\"   ‚úÖ T > 1: Model was overconfident (as expected for neural networks)\")\n",
    "elif T_opt < 1.0:\n",
    "    print(\"   ‚ö†Ô∏è T < 1: Model was underconfident (unusual)\")\n",
    "else:\n",
    "    print(\"   T = 1: Model was already well-calibrated\")\n",
    "\n",
    "# Sanity check 2: ECE should decrease\n",
    "print(\"\\nSanity Check 2: ECE reduction\")\n",
    "print(f\"   ECE before: {ece_before:.4f}\")\n",
    "print(f\"   ECE after: {ece_after:.4f}\")\n",
    "print(f\"   ‚úÖ PASSED\" if ece_after < ece_before else \"   ‚ùå FAILED\")\n",
    "\n",
    "# Sanity check 3: Accuracy should be approximately preserved\n",
    "print(\"\\nSanity Check 3: Accuracy preservation\")\n",
    "print(f\"   Accuracy before: {metrics_before['accuracy']:.4f}\")\n",
    "print(f\"   Accuracy after: {metrics_after['accuracy']:.4f}\")\n",
    "print(f\"   Difference: {abs(metrics_after['accuracy'] - metrics_before['accuracy']):.4f}\")\n",
    "acc_preserved = np.isclose(metrics_before['accuracy'], metrics_after['accuracy'], atol=0.01)\n",
    "print(f\"   ‚úÖ PASSED (accuracy preserved)\" if acc_preserved else \"   ‚ö†Ô∏è Small change\")\n",
    "\n",
    "# Sanity check 4: Perfect calibration should have ECE ‚âà 0\n",
    "print(\"\\nSanity Check 4: Perfect calibration test\")\n",
    "# Create perfectly calibrated predictions\n",
    "y_perfect = np.random.rand(1000)\n",
    "y_labels = (np.random.rand(1000) < y_perfect).astype(int)\n",
    "ece_perfect = expected_calibration_error(y_labels, y_perfect, n_bins=10)\n",
    "print(f\"   ECE for perfectly calibrated data: {ece_perfect:.4f}\")\n",
    "print(f\"   ‚úÖ PASSED (ECE ‚âà 0)\" if ece_perfect < 0.05 else \"   ‚ö†Ô∏è Check implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d635d55",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "‚úÖ **Calibration**: Predicted probabilities should match empirical frequencies\n",
    "\n",
    "‚úÖ **Modern neural networks are often overconfident** ‚Üí need post-hoc calibration\n",
    "\n",
    "‚úÖ **Reliability diagram**: Visual tool to diagnose miscalibration\n",
    "   - Perfect calibration: bars align with diagonal\n",
    "   - Below diagonal: overconfident\n",
    "   - Above diagonal: underconfident\n",
    "\n",
    "‚úÖ **ECE quantifies miscalibration**: Lower is better (0 = perfect)\n",
    "\n",
    "‚úÖ **Temperature scaling**: Simple, effective post-hoc calibration\n",
    "   - $T > 1$ ‚Üí soften overconfident predictions\n",
    "   - Preserves accuracy (argmax unchanged)\n",
    "   - Only requires validation set\n",
    "\n",
    "‚úÖ **Metrics improve**: NLL, Brier score, ECE all benefit from calibration\n",
    "\n",
    "**Common pitfalls:**\n",
    "- ‚ùå Confusing accuracy with calibration (can have high accuracy but poor calibration)\n",
    "- ‚ùå Using test set to fit temperature (causes overfitting! use separate validation set)\n",
    "- ‚ùå Applying calibration when not needed (simple models like logistic regression are often well-calibrated)\n",
    "- ‚ùå Forgetting that calibration doesn't improve discrimination (ROC-AUC unchanged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84118b92",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "**Exercise 1:** Train a very overconfident model (e.g., increase hidden layer sizes, reduce regularization). What is the optimal temperature?\n",
    "\n",
    "**Exercise 2:** Implement Maximum Calibration Error (MCE): $\\max_m |\\bar{p}_m - \\bar{y}_m|$. How does it differ from ECE?\n",
    "\n",
    "**Exercise 3:** Create artificially underconfident predictions by adding noise to probabilities. Verify that $T < 1$ after calibration.\n",
    "\n",
    "**Exercise 4:** Compare temperature scaling with Platt scaling (logistic regression on top of model scores). Which is simpler? Which works better?\n",
    "\n",
    "**Exercise 5:** For a multiclass problem (3+ classes), extend temperature scaling. What changes?\n",
    "\n",
    "**Exercise 6:** Plot Brier score decomposition: $\\text{Brier} = \\text{Reliability} + \\text{Resolution} - \\text{Uncertainty}$. How does calibration affect each term?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06095443",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b610b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Very overconfident model\n",
    "set_seed(42)\n",
    "overconfident_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(200, 100, 50),  # Larger network\n",
    "    activation='relu',\n",
    "    max_iter=500,\n",
    "    alpha=0.0001,  # Very small regularization\n",
    "    random_state=42\n",
    ")\n",
    "overconfident_model.fit(X_train, y_train)\n",
    "y_prob_overconf_val = overconfident_model.predict_proba(X_val)[:, 1]\n",
    "T_overconf = calibrate_binary(y_val, y_prob_overconf_val)\n",
    "print(f\"Solution 1: Overconfident model optimal T = {T_overconf:.3f}\")\n",
    "print(f\"   Even higher T than before (model is more overconfident)\")\n",
    "\n",
    "# Solution 2: Maximum Calibration Error (MCE)\n",
    "def maximum_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"Compute MCE: maximum calibration gap.\"\"\"\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_prob, bins[1:-1])\n",
    "    \n",
    "    max_gap = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        if np.sum(mask) > 0:\n",
    "            bin_acc = np.mean(y_true[mask])\n",
    "            bin_conf = np.mean(y_prob[mask])\n",
    "            gap = abs(bin_conf - bin_acc)\n",
    "            max_gap = max(max_gap, gap)\n",
    "    return max_gap\n",
    "\n",
    "mce_before = maximum_calibration_error(y_test, y_prob_test, n_bins=10)\n",
    "mce_after = maximum_calibration_error(y_test, y_prob_calibrated, n_bins=10)\n",
    "print(f\"\\nSolution 2: MCE comparison\")\n",
    "print(f\"   MCE before: {mce_before:.4f}\")\n",
    "print(f\"   MCE after: {mce_after:.4f}\")\n",
    "print(f\"   Difference: MCE focuses on worst bin, ECE averages all bins\")\n",
    "\n",
    "# Solution 3: Underconfident predictions\n",
    "# Add noise to pull predictions toward 0.5\n",
    "y_prob_underconf = 0.5 + 0.3 * (y_prob_val - 0.5)  # Shrink toward 0.5\n",
    "T_underconf = calibrate_binary(y_val, y_prob_underconf)\n",
    "print(f\"\\nSolution 3: Underconfident T = {T_underconf:.3f}\")\n",
    "print(f\"   T < 1 as expected (need to sharpen predictions)\")\n",
    "\n",
    "# Solution 5: Multiclass temperature scaling\n",
    "print(\"\\nSolution 5: Multiclass extension\")\n",
    "print(\"   For K classes with logits z_i:\")\n",
    "print(\"   p_i = exp(z_i/T) / sum_j exp(z_j/T)\")\n",
    "print(\"   Single scalar T applied to all logits\")\n",
    "print(\"   Fit T by minimizing negative log-likelihood on validation set\")\n",
    "print(\"   Preserves class rankings (argmax unchanged)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688b93b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next:** [04_mcmc_metropolis_hastings_diagnostics.ipynb](04_mcmc_metropolis_hastings_diagnostics.ipynb) - Learn how to sample from complex distributions using MCMC"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
