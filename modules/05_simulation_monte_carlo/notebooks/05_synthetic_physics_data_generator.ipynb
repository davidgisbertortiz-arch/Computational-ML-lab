{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "# Import generators\n",
    "(generate_brownian_motion, generate_ou_process, generate_levy_flight,\n",
    " PhysicsDataGenerator, add_correlated_noise) = safe_import_from(\n",
    "    '05_simulation_monte_carlo.src.synthetic_generators',\n",
    "    'generate_brownian_motion', 'generate_ou_process', 'generate_levy_flight',\n",
    "    'PhysicsDataGenerator', 'add_correlated_noise'\n",
    ")\n",
    "\n",
    "# Setup\n",
    "np.random.seed(42)\n",
    "reports_dir = Path('../reports')\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (14, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0effc",
   "metadata": {},
   "source": [
    "## 1. Why Synthetic Data?\n",
    "\n",
    "**Advantages:**\n",
    "- ✓ **Known ground truth** → can measure true error, not just validation error\n",
    "- ✓ **Control complexity** → gradually increase difficulty to debug models\n",
    "- ✓ **No data collection cost** → infinite training data\n",
    "- ✓ **Reproducible** → same experiments, different researchers\n",
    "- ✓ **Test edge cases** → simulate rare events, outliers\n",
    "\n",
    "**When to use:**\n",
    "- Algorithm development and debugging\n",
    "- Benchmarking new methods\n",
    "- Understanding model failure modes\n",
    "- Ablation studies (isolate specific effects)\n",
    "\n",
    "**When NOT to use:**\n",
    "- Final production models (always validate on real data!)\n",
    "- When synthetic ≠ real distribution (domain shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1b3be",
   "metadata": {},
   "source": [
    "## 2. Stochastic Processes\n",
    "\n",
    "### 2.1 Brownian Motion (Wiener Process)\n",
    "\n",
    "$$dX_t = \\mu dt + \\sigma dW_t$$\n",
    "\n",
    "- **Drift:** $\\mu$ (constant trend)\n",
    "- **Diffusion:** $\\sigma$ (random fluctuations)\n",
    "- **Applications:** Stock prices, particle diffusion, sensor noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Brownian motion paths\n",
    "t, paths_bm = generate_brownian_motion(\n",
    "    t_max=10.0,\n",
    "    n_steps=1000,\n",
    "    mu=0.1,  # Positive drift\n",
    "    sigma=1.0,\n",
    "    n_paths=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Sample paths\n",
    "for i in range(paths_bm.shape[0]):\n",
    "    ax1.plot(t, paths_bm[i], alpha=0.7, linewidth=1.5)\n",
    "ax1.axhline(0, color='black', linestyle='--', alpha=0.3)\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Position')\n",
    "ax1.set_title('Brownian Motion (μ=0.1, σ=1.0)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Distribution at final time\n",
    "t_final, paths_many = generate_brownian_motion(10.0, 1000, 0.1, 1.0, n_paths=5000, seed=42)\n",
    "final_values = paths_many[:, -1]\n",
    "ax2.hist(final_values, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(np.mean(final_values), color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Mean={np.mean(final_values):.2f}')\n",
    "ax2.set_xlabel('Final Position')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Distribution at t=10 (5000 paths)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_brownian_motion.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Expected mean at t=10: μ*t = {0.1 * 10:.2f}\")\n",
    "print(f\"Observed mean: {np.mean(final_values):.2f}\")\n",
    "print(f\"Expected std: σ*sqrt(t) = {1.0 * np.sqrt(10):.2f}\")\n",
    "print(f\"Observed std: {np.std(final_values):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b34d9",
   "metadata": {},
   "source": [
    "### 2.2 Ornstein-Uhlenbeck Process (Mean-Reverting)\n",
    "\n",
    "$$dX_t = \\theta(\\mu - X_t)dt + \\sigma dW_t$$\n",
    "\n",
    "- **Mean reversion:** $\\theta$ (speed of return to $\\mu$)\n",
    "- **Applications:** Interest rates, commodity prices, temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different mean reversion rates\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "thetas = [0.5, 2.0, 5.0]\n",
    "\n",
    "for ax, theta in zip(axes, thetas):\n",
    "    t_ou, paths_ou = generate_ou_process(\n",
    "        t_max=10.0,\n",
    "        n_steps=1000,\n",
    "        theta=theta,\n",
    "        mu=0.0,\n",
    "        sigma=1.0,\n",
    "        x0=3.0,  # Start far from mean\n",
    "        n_paths=10,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    for i in range(paths_ou.shape[0]):\n",
    "        ax.plot(t_ou, paths_ou[i], alpha=0.5, linewidth=1)\n",
    "    ax.axhline(0, color='red', linestyle='--', linewidth=2, label='Mean μ=0')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Position')\n",
    "    ax.set_title(f'OU Process (θ={theta})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_ou_process.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Larger θ → faster mean reversion\")\n",
    "print(\"✓ Useful for modeling systems with restoring forces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d149b5a",
   "metadata": {},
   "source": [
    "### 2.3 Lévy Flights (Heavy-Tailed Jumps)\n",
    "\n",
    "$$X_{t+1} = X_t + \\Delta X, \\quad \\Delta X \\sim \\text{Lévy}(\\alpha)$$\n",
    "\n",
    "- **Stability parameter:** $\\alpha \\in (0, 2]$ \n",
    "  - $\\alpha = 2$: Gaussian (Brownian motion)\n",
    "  - $\\alpha = 1$: Cauchy distribution\n",
    "- **Applications:** Anomalous diffusion, animal foraging, financial crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb09a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different stability parameters\n",
    "alphas = [0.5, 1.0, 1.5, 2.0]\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, alpha in zip(axes, alphas):\n",
    "    t_levy, paths_levy = generate_levy_flight(\n",
    "        n_steps=1000,\n",
    "        alpha=alpha,\n",
    "        scale=0.1,\n",
    "        n_paths=5,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    for i in range(paths_levy.shape[0]):\n",
    "        ax.plot(t_levy, paths_levy[i], alpha=0.7, linewidth=1)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Position')\n",
    "    ax.set_title(f'Lévy Flight (α={alpha})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_levy_flights.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Smaller α → heavier tails → larger jumps\")\n",
    "print(\"✓ α=2 recovers Brownian motion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477d112",
   "metadata": {},
   "source": [
    "## 3. Physics-Inspired Regression Problems\n",
    "\n",
    "Generate datasets with known functional forms for ML benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0aeef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = PhysicsDataGenerator(seed=42)\n",
    "\n",
    "# Problem 1: Damped Harmonic Oscillator\n",
    "# x(t) = A*exp(-γt)*cos(ωt + φ)\n",
    "X_damped, y_damped, desc_damped, truth_damped = generator.damped_oscillator(\n",
    "    n_samples=200,\n",
    "    noise_level=0.05\n",
    ")\n",
    "\n",
    "# Problem 2: Projectile Motion\n",
    "# y = v₀*sin(θ)*t - 0.5*g*t²\n",
    "X_proj, y_proj, desc_proj, truth_proj = generator.projectile(\n",
    "    n_samples=200,\n",
    "    noise_level=0.05\n",
    ")\n",
    "\n",
    "# Problem 3: Heat Diffusion\n",
    "# T(x,t) = T₀*erf(x/(2*sqrt(α*t)))\n",
    "X_heat, y_heat, desc_heat, truth_heat = generator.heat_diffusion(\n",
    "    n_samples=200,\n",
    "    noise_level=0.05\n",
    ")\n",
    "\n",
    "# Plot all three\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "problems = [\n",
    "    (X_damped, y_damped, truth_damped, \"Damped Oscillator\"),\n",
    "    (X_proj, y_proj, truth_proj, \"Projectile Motion\"),\n",
    "    (X_heat, y_heat, truth_heat, \"Heat Diffusion\")\n",
    "]\n",
    "\n",
    "for ax, (X, y, truth, title) in zip(axes, problems):\n",
    "    # Noisy observations\n",
    "    ax.scatter(X[:, 0], y, alpha=0.5, s=20, label='Noisy data')\n",
    "    # Ground truth\n",
    "    X_sorted = X[np.argsort(X[:, 0])]\n",
    "    y_true = truth(X_sorted)\n",
    "    ax.plot(X_sorted[:, 0], y_true, 'r-', linewidth=2, label='Ground truth')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Target')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_physics_regression_problems.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Each problem has:\")\n",
    "print(\"  - Features X (input variables)\")\n",
    "print(\"  - Targets y (noisy observations)\")\n",
    "print(\"  - Ground truth function (for true error measurement)\")\n",
    "print(\"  - Description (physical interpretation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bdc03a",
   "metadata": {},
   "source": [
    "## 4. ML Workflow Example: Testing a Model\n",
    "\n",
    "Let's use synthetic data to test a simple regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083dd5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate data\n",
    "X, y, desc, ground_truth = generator.pendulum_energy(n_samples=500, noise_level=0.1)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Measure error (test set)\n",
    "test_mse = np.mean((y_test - y_pred)**2)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "# Measure TRUE error (ground truth)\n",
    "y_true = ground_truth(X_test)\n",
    "true_mse = np.mean((y_true - y_pred)**2)\n",
    "true_rmse = np.sqrt(true_mse)\n",
    "\n",
    "# Noise level (difference between true and observed)\n",
    "noise_mse = np.mean((y_test - y_true)**2)\n",
    "\n",
    "print(f\"Problem: {desc}\\n\")\n",
    "print(f\"Test RMSE (with noise): {test_rmse:.4f}\")\n",
    "print(f\"True RMSE (vs ground truth): {true_rmse:.4f}\")\n",
    "print(f\"Noise level: {np.sqrt(noise_mse):.4f}\\n\")\n",
    "print(f\"✓ Synthetic data lets us separate:\")\n",
    "print(f\"  - Model error (true RMSE)\")\n",
    "print(f\"  - Noise contribution (test RMSE - true RMSE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd25de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Predictions vs observations\n",
    "ax1.scatter(y_test, y_pred, alpha=0.5, s=30)\n",
    "lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
    "ax1.plot(lims, lims, 'r--', linewidth=2, label='Perfect prediction')\n",
    "ax1.set_xlabel('Observed (noisy)')\n",
    "ax1.set_ylabel('Predicted')\n",
    "ax1.set_title('Predictions vs Noisy Observations')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Predictions vs ground truth\n",
    "ax2.scatter(y_true, y_pred, alpha=0.5, s=30, color='green')\n",
    "lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n",
    "ax2.plot(lims, lims, 'r--', linewidth=2, label='Perfect prediction')\n",
    "ax2.set_xlabel('Ground truth (noiseless)')\n",
    "ax2.set_ylabel('Predicted')\n",
    "ax2.set_title('Predictions vs Ground Truth')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_ml_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49090fae",
   "metadata": {},
   "source": [
    "## 5. Correlated Noise\n",
    "\n",
    "Real data often has **correlated noise** (e.g., autocorrelation in time series).\n",
    "\n",
    "Generate realistic noise using Ornstein-Uhlenbeck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean signal\n",
    "t = np.linspace(0, 10, 200)\n",
    "signal = np.sin(2 * np.pi * 0.5 * t)\n",
    "\n",
    "# Add correlated noise\n",
    "noisy_signal = add_correlated_noise(\n",
    "    signal,\n",
    "    noise_scale=0.3,\n",
    "    correlation_time=0.5,  # Correlation length\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Add IID noise for comparison\n",
    "np.random.seed(42)\n",
    "iid_noise = signal + np.random.randn(len(signal)) * 0.3\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "ax1.plot(t, signal, 'k-', linewidth=2, label='True signal')\n",
    "ax1.set_ylabel('Clean')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(t, iid_noise, alpha=0.7, label='IID noise')\n",
    "ax2.plot(t, signal, 'k--', alpha=0.5, linewidth=1)\n",
    "ax2.set_ylabel('IID Noise')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3.plot(t, noisy_signal, alpha=0.7, color='orange', label='Correlated noise')\n",
    "ax3.plot(t, signal, 'k--', alpha=0.5, linewidth=1)\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('Correlated Noise')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '05_correlated_noise.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Correlated noise is smoother (autocorrelated)\")\n",
    "print(\"✓ More realistic for many applications (sensors, finance, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f364ea",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "✓ **Synthetic data = known ground truth** → measure true model error\n",
    "\n",
    "✓ **Stochastic processes:**\n",
    "- Brownian: Constant drift + diffusion\n",
    "- OU: Mean-reverting\n",
    "- Lévy: Heavy-tailed jumps\n",
    "\n",
    "✓ **Physics problems** provide realistic functional forms for testing\n",
    "\n",
    "✓ **Correlated noise** → more realistic than IID\n",
    "\n",
    "✓ **Workflow:**\n",
    "1. Generate synthetic data with ground truth\n",
    "2. Train/test ML model\n",
    "3. Measure true error (vs ground truth)\n",
    "4. Debug model on controlled examples\n",
    "5. Validate on real data\n",
    "\n",
    "⚠ **Limitation:** Synthetic ≠ real (domain shift). Always validate on real data before deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0429c6",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "**Exercise 1:** Generate a dataset mixing two physics problems (e.g., oscillator + projectile). Train a model and check if it can learn both components.\n",
    "\n",
    "**Exercise 2:** Compare model performance (Random Forest, Linear Regression, Neural Network) on the heat diffusion problem. Which works best?\n",
    "\n",
    "**Exercise 3:** Gradually increase noise level from 0 to 0.5. Plot model RMSE vs noise. Where does your model break down?\n",
    "\n",
    "**Exercise 4:** Generate Brownian motion with different $\\mu$ and $\\sigma$. Train a model to **predict** the parameters from a single path.\n",
    "\n",
    "**Exercise 5:** **Challenge:** Create a time-series forecasting problem using OU process. Generate sequences, train an LSTM, and measure true error vs prediction horizon.\n",
    "\n",
    "**Exercise 6:** Add outliers (5% of points with 10x noise) to a physics dataset. Test robust regression methods (Huber, RANSAC) vs OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d020f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f6782",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Module 05 Complete!\n",
    "\n",
    "You've learned:\n",
    "1. ✅ MC integration with error bars and convergence diagnostics\n",
    "2. ✅ Variance reduction: IS (10-1000x speedup)\n",
    "3. ✅ Control variates and antithetic sampling\n",
    "4. ✅ Rare event estimation (P < 10^-6)\n",
    "5. ✅ Synthetic data generation for ML\n",
    "\n",
    "**Next steps:**\n",
    "- Apply these methods to your research problems\n",
    "- Read: *Monte Carlo Statistical Methods* by Robert & Casella\n",
    "- Explore: Quasi-Monte Carlo, MCMC (Module 06), multilevel MC\n",
    "\n",
    "**Key principle:** Always report uncertainty. MC estimates are random!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
