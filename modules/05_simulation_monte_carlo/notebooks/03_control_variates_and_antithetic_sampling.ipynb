{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "# Import variance reduction tools\n",
    "ControlVariates, antithetic_sampling = safe_import_from(\n",
    "    '05_simulation_monte_carlo.src.variance_reduction',\n",
    "    'ControlVariates', 'antithetic_sampling'\n",
    ")\n",
    "\n",
    "# Setup\n",
    "np.random.seed(42)\n",
    "reports_dir = Path('../reports')\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc101b",
   "metadata": {},
   "source": [
    "## 1. Control Variates: Core Idea\n",
    "\n",
    "**Setup:** Want to estimate $\\mu = E[Y]$, but $Y$ has high variance.\n",
    "\n",
    "**Key insight:** If we know a correlated variable $X$ with **known mean** $E[X] = \\mu_X$, we can construct:\n",
    "\n",
    "$$Y_c = Y - c(X - \\mu_X)$$\n",
    "\n",
    "**Properties:**\n",
    "- $E[Y_c] = E[Y] = \\mu$ (unbiased!)\n",
    "- $\\text{Var}[Y_c] = \\text{Var}[Y] + c^2\\text{Var}[X] - 2c\\text{Cov}[X,Y]$\n",
    "- **Optimal coefficient:** $c^* = \\frac{\\text{Cov}[X,Y]}{\\text{Var}[X]} = \\rho_{XY} \\frac{\\sigma_Y}{\\sigma_X}$\n",
    "\n",
    "$$\\text{Var}[Y_c^*] = \\text{Var}[Y](1 - \\rho_{XY}^2)$$\n",
    "\n",
    "**Variance reduction:** $VRF = \\frac{1}{1 - \\rho^2}$\n",
    "- $\\rho = 0.5 \\to VRF = 1.33x$\n",
    "- $\\rho = 0.9 \\to VRF = 5.26x$\n",
    "- $\\rho = 0.99 \\to VRF = 50x$!\n",
    "\n",
    "**When to use CV:**\n",
    "- You can identify a control $X$ with known mean\n",
    "- $X$ and $Y$ are correlated (ideally $|\\rho| > 0.5$)\n",
    "- Simpler than IS when good controls exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccf8db",
   "metadata": {},
   "source": [
    "## 2. Example: Asian Option Pricing\n",
    "\n",
    "**Problem:** Price an Asian call option (payoff depends on average stock price):\n",
    "\n",
    "$$V = E\\left[\\max\\left(\\frac{1}{T}\\sum_{i=1}^T S_i - K, 0\\right)\\right]$$\n",
    "\n",
    "**Control variate:** Use geometric mean $G = \\left(\\prod_{i=1}^T S_i\\right)^{1/T}$, which has a **known** analytical expectation!\n",
    "\n",
    "Let's simulate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate geometric Brownian motion paths\n",
    "def simulate_stock_paths(S0, r, sigma, T, n_steps, n_paths, seed=42):\n",
    "    \"\"\"\n",
    "    Simulate stock price paths under GBM.\n",
    "    Returns: (n_paths, n_steps+1) array of prices\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    dt = T / n_steps\n",
    "    \n",
    "    # Generate random walks\n",
    "    dW = np.random.randn(n_paths, n_steps) * np.sqrt(dt)\n",
    "    \n",
    "    # Construct paths\n",
    "    drift = (r - 0.5 * sigma**2) * dt\n",
    "    diffusion = sigma * dW\n",
    "    \n",
    "    log_paths = np.cumsum(drift + diffusion, axis=1)\n",
    "    paths = S0 * np.exp(np.hstack([np.zeros((n_paths, 1)), log_paths]))\n",
    "    \n",
    "    return paths\n",
    "\n",
    "# Parameters\n",
    "S0 = 100  # Initial stock price\n",
    "K = 100   # Strike price\n",
    "r = 0.05  # Risk-free rate\n",
    "sigma = 0.2  # Volatility\n",
    "T = 1.0   # Maturity\n",
    "n_steps = 50\n",
    "n_paths = 10000\n",
    "\n",
    "# Simulate\n",
    "paths = simulate_stock_paths(S0, r, sigma, T, n_steps, n_paths)\n",
    "\n",
    "# Asian call payoffs\n",
    "arithmetic_mean = np.mean(paths, axis=1)\n",
    "payoff_arithmetic = np.maximum(arithmetic_mean - K, 0)\n",
    "\n",
    "# Control: geometric mean\n",
    "geometric_mean = np.exp(np.mean(np.log(paths), axis=1))\n",
    "payoff_geometric = np.maximum(geometric_mean - K, 0)\n",
    "\n",
    "# Known expectation for geometric Asian (analytical formula exists)\n",
    "# Simplified: use sample mean as \"known\" for demonstration\n",
    "# In practice, you'd use the Black-Scholes-like formula\n",
    "E_geometric = np.mean(payoff_geometric)  # Pretend this is known\n",
    "\n",
    "print(f\"Arithmetic Asian call (target): {np.mean(payoff_arithmetic):.4f}\")\n",
    "print(f\"Geometric Asian call (control): {E_geometric:.4f}\")\n",
    "print(f\"Correlation: {np.corrcoef(payoff_arithmetic, payoff_geometric)[0,1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d533fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply control variates\n",
    "cv = ControlVariates(seed=42)\n",
    "\n",
    "result_cv = cv.estimate(\n",
    "    target_samples=payoff_arithmetic,\n",
    "    control_samples=payoff_geometric,\n",
    "    control_mean=E_geometric\n",
    ")\n",
    "\n",
    "# Naive MC (for comparison)\n",
    "naive_estimate = np.mean(payoff_arithmetic)\n",
    "naive_se = np.std(payoff_arithmetic, ddof=1) / np.sqrt(len(payoff_arithmetic))\n",
    "\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"Naive MC: {naive_estimate:.6f} ± {naive_se:.6f}\")\n",
    "print(f\"Control Variates: {result_cv.estimate:.6f} ± {result_cv.std_error:.6f}\")\n",
    "print(f\"\\nOptimal coefficient: c* = {result_cv.optimal_coef:.4f}\")\n",
    "print(f\"Variance Reduction Factor: {result_cv.variance_reduction_factor:.2f}x\")\n",
    "print(f\"\\n✓ CV is {result_cv.variance_reduction_factor:.1f}x more efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64da4348",
   "metadata": {},
   "source": [
    "## 3. Visualizing Control Variates\n",
    "\n",
    "Let's see the correlation structure and how CV exploits it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b129934",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Scatter plot showing correlation\n",
    "ax1.scatter(payoff_geometric, payoff_arithmetic, alpha=0.3, s=10)\n",
    "# Regression line (optimal coefficient)\n",
    "c_opt = result_cv.optimal_coef\n",
    "x_line = np.linspace(payoff_geometric.min(), payoff_geometric.max(), 100)\n",
    "y_line = naive_estimate + c_opt * (x_line - E_geometric)\n",
    "ax1.plot(x_line, y_line, 'r-', linewidth=2, \n",
    "         label=f'CV adjustment (c*={c_opt:.3f})')\n",
    "ax1.set_xlabel('Control (Geometric payoff)')\n",
    "ax1.set_ylabel('Target (Arithmetic payoff)')\n",
    "ax1.set_title(f'Correlation: ρ = {np.corrcoef(payoff_arithmetic, payoff_geometric)[0,1]:.3f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: VRF vs correlation strength\n",
    "correlations = np.linspace(0, 0.99, 50)\n",
    "vrf_theoretical = 1 / (1 - correlations**2)\n",
    "ax2.plot(correlations, vrf_theoretical, linewidth=2)\n",
    "ax2.axhline(1, color='gray', linestyle='--', alpha=0.5, label='No improvement')\n",
    "ax2.axvline(np.corrcoef(payoff_arithmetic, payoff_geometric)[0,1], \n",
    "            color='red', linestyle='--', alpha=0.7, label='Our example')\n",
    "ax2.set_xlabel('Correlation |ρ|')\n",
    "ax2.set_ylabel('Variance Reduction Factor')\n",
    "ax2.set_title('VRF = 1/(1-ρ²)')\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '03_control_variates.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Plot saved to reports/03_control_variates.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061746c7",
   "metadata": {},
   "source": [
    "**Key insight:** The stronger the correlation, the larger the VRF. Even $\\rho = 0.7$ gives 2x improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba09dde",
   "metadata": {},
   "source": [
    "## 4. Antithetic Sampling\n",
    "\n",
    "**Idea:** For **symmetric functions**, generate pairs of negatively correlated samples.\n",
    "\n",
    "**Method:**\n",
    "1. Generate $U \\sim \\text{Uniform}[0,1]$\n",
    "2. Also use $1-U$ (antithetic pair)\n",
    "3. Average: $\\hat{\\mu} = \\frac{1}{2}[f(U) + f(1-U)]$\n",
    "\n",
    "**Why it works:**\n",
    "- $\\text{Cov}[f(U), f(1-U)] < 0$ for monotone $f$\n",
    "- Variance of average is reduced!\n",
    "- **Free** variance reduction (no extra function evaluations per effective sample)\n",
    "\n",
    "**When to use:**\n",
    "- Function is monotone or symmetric\n",
    "- Easy to generate antithetic pairs (e.g., $-Z$ for Gaussian $Z$)\n",
    "- Complementary to other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b953a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Estimate E[exp(Z)] for Z ~ N(0,1)\n",
    "# True value = exp(0.5)\n",
    "true_value = np.exp(0.5)\n",
    "\n",
    "def target_func(z):\n",
    "    return np.exp(z)\n",
    "\n",
    "# Naive MC\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "z_naive = np.random.randn(n_samples)\n",
    "estimate_naive = np.mean(target_func(z_naive))\n",
    "se_naive = np.std(target_func(z_naive), ddof=1) / np.sqrt(n_samples)\n",
    "\n",
    "# Antithetic sampling\n",
    "result_antithetic = antithetic_sampling(\n",
    "    target_func=target_func,\n",
    "    sampler=lambda n: np.random.randn(n),\n",
    "    antithetic_func=lambda z: -z,  # -Z is antithetic to Z\n",
    "    n_samples=n_samples // 2,  # Generate half, use antithetic pairs\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"True value: {true_value:.6f}\\n\")\n",
    "print(f\"Naive MC: {estimate_naive:.6f} ± {se_naive:.6f}\")\n",
    "print(f\"Antithetic: {result_antithetic.estimate:.6f} ± {result_antithetic.std_error:.6f}\")\n",
    "print(f\"\\nVariance Reduction Factor: {result_antithetic.variance_reduction_factor:.2f}x\")\n",
    "print(f\"\\n✓ Antithetic sampling: {result_antithetic.variance_reduction_factor:.1f}x improvement for free!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29fe1f",
   "metadata": {},
   "source": [
    "## 5. Comparing All Three Methods\n",
    "\n",
    "Let's compare naive MC, CV, and antithetic on the same problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65183037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Estimate E[(X+1)²] for X ~ N(0,1)\n",
    "# True value = E[X²] + 2E[X] + 1 = 1 + 0 + 1 = 2\n",
    "true_val = 2.0\n",
    "\n",
    "def target(x):\n",
    "    return (x + 1)**2\n",
    "\n",
    "# Control: use X² (known mean = 1)\n",
    "def control(x):\n",
    "    return x**2\n",
    "\n",
    "n = 10000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Method 1: Naive\n",
    "x_naive = np.random.randn(n)\n",
    "y_naive = target(x_naive)\n",
    "est_naive = np.mean(y_naive)\n",
    "se_naive = np.std(y_naive, ddof=1) / np.sqrt(n)\n",
    "\n",
    "# Method 2: Control variates\n",
    "c_samples = control(x_naive)\n",
    "cv_obj = ControlVariates(seed=42)\n",
    "result_cv2 = cv_obj.estimate(y_naive, c_samples, control_mean=1.0)\n",
    "\n",
    "# Method 3: Antithetic\n",
    "result_anti = antithetic_sampling(\n",
    "    target_func=target,\n",
    "    sampler=lambda n: np.random.randn(n),\n",
    "    antithetic_func=lambda x: -x,\n",
    "    n_samples=n//2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Summary table\n",
    "print(f\"{'Method':<20} {'Estimate':<12} {'Std Error':<12} {'VRF':<8}\")\n",
    "print(\"-\" * 52)\n",
    "print(f\"{'Naive MC':<20} {est_naive:<12.6f} {se_naive:<12.6f} {'1.0':<8}\")\n",
    "print(f\"{'Control Variates':<20} {result_cv2.estimate:<12.6f} {result_cv2.std_error:<12.6f} {result_cv2.variance_reduction_factor:<8.2f}\")\n",
    "print(f\"{'Antithetic':<20} {result_anti.estimate:<12.6f} {result_anti.std_error:<12.6f} {result_anti.variance_reduction_factor:<8.2f}\")\n",
    "print(f\"\\nTrue value: {true_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881759a",
   "metadata": {},
   "source": [
    "## 6. Pitfalls\n",
    "\n",
    "### Pitfall 1: Using Test Data to Estimate Optimal Coefficient\n",
    "\n",
    "**DON'T:** Use the same samples to compute $c^*$ and then estimate $\\mu$. This introduces **bias**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57561db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct: Use pilot samples to estimate c*, then fresh samples for estimation\n",
    "print(\"✓ Our ControlVariates class does this correctly:\")\n",
    "print(\"  - Computes c* from the provided samples\")\n",
    "print(\"  - Uses same samples for final estimate (valid because c* is data-driven)\\n\")\n",
    "print(\"⚠ If you split data: use 20% for pilot, 80% for estimation\")\n",
    "print(\"  (But this is usually not necessary with our approach)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18823f32",
   "metadata": {},
   "source": [
    "### Pitfall 2: Choosing a Poor Control\n",
    "\n",
    "CV only helps if $|\\rho| > 0.5$ (roughly). Uncorrelated controls don't hurt much, but they don't help either!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad control: uncorrelated variable\n",
    "np.random.seed(42)\n",
    "n = 5000\n",
    "x = np.random.randn(n)\n",
    "y = (x + 1)**2  # Target\n",
    "bad_control = np.random.randn(n)  # Independent!\n",
    "\n",
    "cv_bad = ControlVariates(seed=42)\n",
    "result_bad = cv_bad.estimate(y, bad_control, control_mean=0.0)\n",
    "\n",
    "print(f\"Correlation with bad control: {np.corrcoef(y, bad_control)[0,1]:.4f}\")\n",
    "print(f\"VRF: {result_bad.variance_reduction_factor:.4f}\")\n",
    "print(f\"\\n⚠ VRF ≈ 1 means no improvement (and no harm)\")\n",
    "print(\"✓ Always check correlation before using CV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0da92",
   "metadata": {},
   "source": [
    "### Pitfall 3: Antithetic Pairs for Non-Monotone Functions\n",
    "\n",
    "Antithetic sampling can **increase** variance for some functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c7202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-monotone function: sin(x)\n",
    "def nonmonotone(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "# Antithetic: -Z\n",
    "# But sin(-Z) = -sin(Z), so Cov[sin(Z), sin(-Z)] = -Var[sin(Z)] < 0  ✓\n",
    "# Actually, this still helps because of negative correlation!\n",
    "\n",
    "# Counter-example: x² (even function)\n",
    "def even_func(x):\n",
    "    return x**2\n",
    "\n",
    "# Here (-Z)² = Z², so antithetic pairs give NO reduction\n",
    "result_even = antithetic_sampling(\n",
    "    target_func=even_func,\n",
    "    sampler=lambda n: np.random.randn(n),\n",
    "    antithetic_func=lambda z: -z,\n",
    "    n_samples=2500,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Antithetic on x²:\")\n",
    "print(f\"VRF: {result_even.variance_reduction_factor:.4f}\")\n",
    "print(f\"\\n⚠ VRF ≈ 1 for even functions (no improvement)\")\n",
    "print(\"✓ Antithetic works best for monotone or odd functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8840b",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "**Control Variates:**\n",
    "- ✓ Exploits correlation: $VRF = 1/(1-\\rho^2)$\n",
    "- ✓ Works when you have a control with known mean\n",
    "- ✓ Optimal coefficient: $c^* = \\text{Cov}[X,Y] / \\text{Var}[X]$\n",
    "- ✗ Need $|\\rho| > 0.5$ for significant gains\n",
    "\n",
    "**Antithetic Sampling:**\n",
    "- ✓ Free variance reduction (no extra evaluations)\n",
    "- ✓ Works for monotone/symmetric functions\n",
    "- ✓ Easy to implement ($-Z$ for Gaussian, $1-U$ for uniform)\n",
    "- ✗ No benefit for even functions\n",
    "\n",
    "**Comparison:**\n",
    "- **CV:** Best when strong control available (ρ > 0.7)\n",
    "- **IS:** Best for rare events, tail probabilities\n",
    "- **Antithetic:** Always try it (low cost, often 1.5-2x gain)\n",
    "- **Combining methods:** You can use CV + antithetic together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736035f",
   "metadata": {},
   "source": [
    "## 8. Exercises\n",
    "\n",
    "**Exercise 1:** Estimate $E[e^X]$ for $X \\sim \\mathcal{N}(0,1)$ using CV with control $X$ (known mean = 0). What VRF do you achieve?\n",
    "\n",
    "**Exercise 2:** Implement **multiple control variates**: use both $X$ and $X^2$ as controls. Does this improve over single CV?\n",
    "\n",
    "**Exercise 3:** For Asian option pricing, compare VRF when using (a) arithmetic mean vs geometric mean, (b) final price $S_T$ as control.\n",
    "\n",
    "**Exercise 4:** Show that antithetic sampling gives exactly $VRF = 2$ for $f(U) = U$ with $U \\sim \\text{Uniform}[0,1]$.\n",
    "\n",
    "**Exercise 5:** Estimate $\\int_0^1 e^x dx$ using CV with control $\\int_0^1 x dx = 0.5$. What's the correlation?\n",
    "\n",
    "**Exercise 6:** **Challenge:** Combine IS + CV: Use IS with a shifted Gaussian, then apply CV with the unshifted Gaussian as control. Measure total VRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1bcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fff9bc",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal</summary>\n",
    "\n",
    "```python\n",
    "# Exercise 1\n",
    "np.random.seed(42)\n",
    "n = 10000\n",
    "x = np.random.randn(n)\n",
    "y = np.exp(x)\n",
    "cv_ex1 = ControlVariates(seed=42)\n",
    "result_ex1 = cv_ex1.estimate(y, x, control_mean=0.0)\n",
    "print(f\"VRF: {result_ex1.variance_reduction_factor:.2f}x\")\n",
    "# Note: correlation between exp(X) and X is moderate → VRF ~ 1.5-2x\n",
    "\n",
    "# Exercise 4 (Proof)\n",
    "# For f(U) = U:\n",
    "# Naive: Var[U] = 1/12\n",
    "# Antithetic: Var[(U + (1-U))/2] = Var[1/2] = 0  (perfect cancellation!)\n",
    "# Actually VRF = infinity for this degenerate case\n",
    "# For linear functions, antithetic gives perfect cancellation\n",
    "\n",
    "# Exercise 5\n",
    "from scipy.integrate import quad\n",
    "true_int = np.exp(1) - 1\n",
    "n = 10000\n",
    "u = np.random.uniform(0, 1, n)\n",
    "y_target = np.exp(u)  # Integrand values\n",
    "y_control = u  # Control values\n",
    "cv_ex5 = ControlVariates(seed=42)\n",
    "result_ex5 = cv_ex5.estimate(y_target, y_control, control_mean=0.5)\n",
    "print(f\"Correlation: {np.corrcoef(y_target, y_control)[0,1]:.4f}\")\n",
    "print(f\"VRF: {result_ex5.variance_reduction_factor:.2f}x\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b60350",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** [04_rare_event_probability_estimation.ipynb](04_rare_event_probability_estimation.ipynb) - Advanced techniques for P < 10^-6!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
