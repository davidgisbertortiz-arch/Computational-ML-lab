{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf17290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "# Import variance reduction tools\n",
    "ImportanceSampler = safe_import_from(\n",
    "    '05_simulation_monte_carlo.src.variance_reduction',\n",
    "    'ImportanceSampler'\n",
    ")\n",
    "\n",
    "# Setup\n",
    "np.random.seed(42)\n",
    "reports_dir = Path('../reports')\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04476f94",
   "metadata": {},
   "source": [
    "## 1. Motivation: When Naive MC Struggles\n",
    "\n",
    "**Problem:** Estimate $I = E_p[h(X)]$ where $h(X)$ is very small most of the time.\n",
    "\n",
    "**Example:** Tail probability $P(X > 3)$ for $X \\sim \\mathcal{N}(0,1)$:\n",
    "- True value: $P(X > 3) \\approx 0.00135$\n",
    "- Naive MC: Sample $X_1, ..., X_N \\sim \\mathcal{N}(0,1)$, estimate $\\hat{p} = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}_{X_i > 3}$\n",
    "\n",
    "**Problem:** With $N=10000$, we expect only ~13 hits. High variance!\n",
    "\n",
    "$$\\text{Var}[\\hat{p}] = \\frac{p(1-p)}{N} \\approx \\frac{0.00135}{N}$$\n",
    "\n",
    "Standard error: $SE \\approx 0.000367$ → **27% relative error**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Naive MC for tail probability\n",
    "def naive_tail_estimate(threshold, n_samples, seed=42):\n",
    "    \"\"\"Estimate P(X > threshold) for X ~ N(0,1) using naive MC.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    samples = np.random.randn(n_samples)\n",
    "    hits = samples > threshold\n",
    "    p_hat = np.mean(hits)\n",
    "    \n",
    "    # Standard error for Bernoulli\n",
    "    se = np.sqrt(p_hat * (1 - p_hat) / n_samples) if p_hat > 0 else 0\n",
    "    \n",
    "    return p_hat, se, np.sum(hits)\n",
    "\n",
    "threshold = 3.0\n",
    "true_prob = 1 - stats.norm.cdf(threshold)\n",
    "n = 10000\n",
    "\n",
    "p_hat, se, n_hits = naive_tail_estimate(threshold, n)\n",
    "\n",
    "print(f\"True probability: {true_prob:.6f}\")\n",
    "print(f\"Naive MC estimate: {p_hat:.6f} ± {se:.6f}\")\n",
    "print(f\"Relative SE: {(se/true_prob)*100:.1f}%\")\n",
    "print(f\"Number of hits: {n_hits}/{n}\")\n",
    "print(f\"\\n⚠ Problem: Very few samples in tail → high variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ad7c1",
   "metadata": {},
   "source": [
    "## 2. Importance Sampling: The Core Idea\n",
    "\n",
    "**Key insight:** Sample from a **proposal distribution** $q(x)$ that concentrates mass where $h(x)$ is large!\n",
    "\n",
    "**Derivation:**\n",
    "$$I = \\int h(x) p(x) dx = \\int h(x) \\frac{p(x)}{q(x)} q(x) dx = E_q\\left[h(X) \\cdot \\frac{p(X)}{q(X)}\\right]$$\n",
    "\n",
    "**IS estimator:**\n",
    "$$\\hat{I}_{IS} = \\frac{1}{N}\\sum_{i=1}^N h(X_i) \\cdot w(X_i), \\quad X_i \\sim q, \\quad w(x) = \\frac{p(x)}{q(x)}$$\n",
    "\n",
    "**Why it works:**\n",
    "- $\\hat{I}_{IS}$ is **unbiased**: $E_q[\\hat{I}_{IS}] = I$\n",
    "- Variance can be **much smaller** if $q$ concentrates on important regions\n",
    "- Optimal $q^*(x) \\propto |h(x)|p(x)$ (zero variance if $h \\geq 0$!)\n",
    "\n",
    "**For tail probability:** Use $q = \\mathcal{N}(\\mu, 1)$ with $\\mu$ near the threshold!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df5e70",
   "metadata": {},
   "source": [
    "## 3. Implementing Importance Sampling\n",
    "\n",
    "Let's estimate the same tail probability using IS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1957d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: P(X > 3) for X ~ N(0, 1)\n",
    "threshold = 3.0\n",
    "n_samples = 10000\n",
    "\n",
    "# Target function (indicator for tail event)\n",
    "def target_func(x):\n",
    "    return (x > threshold).astype(float)\n",
    "\n",
    "# Proposal: N(threshold, 1) - shifted to tail\n",
    "def proposal_sampler(n):\n",
    "    return np.random.randn(n) + threshold\n",
    "\n",
    "# Weight function: w(x) = p(x) / q(x) in log space for stability\n",
    "def log_weight_func(x):\n",
    "    log_p = stats.norm.logpdf(x, 0, 1)  # Target: N(0,1)\n",
    "    log_q = stats.norm.logpdf(x, threshold, 1)  # Proposal: N(3,1)\n",
    "    return log_p - log_q\n",
    "\n",
    "# Run importance sampling\n",
    "sampler = ImportanceSampler(\n",
    "    proposal_sampler=proposal_sampler,\n",
    "    weight_func=log_weight_func,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "result = sampler.estimate(\n",
    "    target_func=target_func,\n",
    "    n_samples=n_samples\n",
    ")\n",
    "\n",
    "print(f\"True probability: {true_prob:.6f}\")\n",
    "print(f\"\\nNaive MC: {p_hat:.6f} ± {se:.6f} (relative SE: {(se/true_prob)*100:.1f}%)\")\n",
    "print(f\"Importance Sampling: {result.estimate:.6f} ± {result.std_error:.6f} (relative SE: {(result.std_error/true_prob)*100:.1f}%)\")\n",
    "print(f\"\\nVariance Reduction Factor: {result.variance_reduction_factor:.1f}x\")\n",
    "print(f\"✓ IS is {result.variance_reduction_factor:.0f}x more efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0576e62",
   "metadata": {},
   "source": [
    "**Observation:** IS gives ~20-50x variance reduction → equivalent to using 200,000-500,000 naive samples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aeb04f",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Difference\n",
    "\n",
    "Let's see where naive MC vs IS concentrate their samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from both methods\n",
    "np.random.seed(42)\n",
    "n_vis = 5000\n",
    "\n",
    "# Naive: sample from N(0,1)\n",
    "samples_naive = np.random.randn(n_vis)\n",
    "\n",
    "# IS: sample from N(3,1)\n",
    "samples_is = np.random.randn(n_vis) + threshold\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Histograms\n",
    "x = np.linspace(-3, 7, 200)\n",
    "ax1.hist(samples_naive, bins=50, alpha=0.5, density=True, label='Naive MC samples', color='blue')\n",
    "ax1.hist(samples_is, bins=50, alpha=0.5, density=True, label='IS samples', color='orange')\n",
    "ax1.plot(x, stats.norm.pdf(x, 0, 1), 'b-', linewidth=2, label='Target p(x) = N(0,1)')\n",
    "ax1.plot(x, stats.norm.pdf(x, threshold, 1), 'orange', linewidth=2, label='Proposal q(x) = N(3,1)')\n",
    "ax1.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold = {threshold}')\n",
    "ax1.fill_between(x[x>threshold], 0, stats.norm.pdf(x[x>threshold], 0, 1), \n",
    "                  alpha=0.3, color='red', label='Target region')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Naive MC vs Importance Sampling')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Convergence comparison\n",
    "sample_sizes = np.logspace(2, 4, 20).astype(int)\n",
    "errors_naive = []\n",
    "errors_is = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    # Naive\n",
    "    p_naive, _, _ = naive_tail_estimate(threshold, n, seed=42)\n",
    "    errors_naive.append(abs(p_naive - true_prob))\n",
    "    \n",
    "    # IS\n",
    "    sampler_temp = ImportanceSampler(proposal_sampler, log_weight_func, seed=42)\n",
    "    r_is = sampler_temp.estimate(target_func, n)\n",
    "    errors_is.append(abs(r_is.estimate - true_prob))\n",
    "\n",
    "ax2.loglog(sample_sizes, errors_naive, 'o-', label='Naive MC', linewidth=2)\n",
    "ax2.loglog(sample_sizes, errors_is, 's-', label='Importance Sampling', linewidth=2)\n",
    "ax2.set_xlabel('Number of samples (N)')\n",
    "ax2.set_ylabel('Absolute error')\n",
    "ax2.set_title('Convergence: Naive vs IS')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '02_importance_sampling_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Plot saved to reports/02_importance_sampling_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46468109",
   "metadata": {},
   "source": [
    "**Key insight:** IS samples concentrate in the tail (orange) where the event happens, while naive MC wastes most samples in irrelevant regions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53732be",
   "metadata": {},
   "source": [
    "## 5. Sensitivity to Proposal Choice\n",
    "\n",
    "**Critical question:** What if we choose a *bad* proposal?\n",
    "\n",
    "Let's compare three proposals:\n",
    "1. **Good:** $q_1 = \\mathcal{N}(3, 1)$ - centered on threshold\n",
    "2. **Mediocre:** $q_2 = \\mathcal{N}(2, 1)$ - partially shifted\n",
    "3. **Bad:** $q_3 = \\mathcal{N}(0, 2)$ - too diffuse, wrong center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposal 1: Good (already implemented above)\n",
    "def proposal_good(n):\n",
    "    return np.random.randn(n) + 3\n",
    "\n",
    "def log_weight_good(x):\n",
    "    return stats.norm.logpdf(x, 0, 1) - stats.norm.logpdf(x, 3, 1)\n",
    "\n",
    "# Proposal 2: Mediocre\n",
    "def proposal_mediocre(n):\n",
    "    return np.random.randn(n) + 2\n",
    "\n",
    "def log_weight_mediocre(x):\n",
    "    return stats.norm.logpdf(x, 0, 1) - stats.norm.logpdf(x, 2, 1)\n",
    "\n",
    "# Proposal 3: Bad\n",
    "def proposal_bad(n):\n",
    "    return np.random.randn(n) * 2  # N(0, 4) - wrong center\n",
    "\n",
    "def log_weight_bad(x):\n",
    "    return stats.norm.logpdf(x, 0, 1) - stats.norm.logpdf(x, 0, 2)\n",
    "\n",
    "# Test all three\n",
    "proposals = [\n",
    "    (\"Good (μ=3, σ=1)\", proposal_good, log_weight_good),\n",
    "    (\"Mediocre (μ=2, σ=1)\", proposal_mediocre, log_weight_mediocre),\n",
    "    (\"Bad (μ=0, σ=2)\", proposal_bad, log_weight_bad),\n",
    "]\n",
    "\n",
    "print(f\"Target: P(X > 3) = {true_prob:.6f}\\n\")\n",
    "results_comparison = []\n",
    "\n",
    "for name, prop_sampler, log_weight in proposals:\n",
    "    sampler = ImportanceSampler(prop_sampler, log_weight, seed=42)\n",
    "    result = sampler.estimate(target_func, 10000)\n",
    "    results_comparison.append((name, result))\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Estimate: {result.estimate:.6f} ± {result.std_error:.6f}\")\n",
    "    print(f\"  VRF: {result.variance_reduction_factor:.1f}x\")\n",
    "    print(f\"  Rel. error: {abs(result.estimate - true_prob)/true_prob * 100:.1f}%\\n\")\n",
    "\n",
    "print(\"✓ Good proposal → large VRF\")\n",
    "print(\"⚠ Bad proposal → little or no improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab05b2",
   "metadata": {},
   "source": [
    "**Lesson:** Proposal choice is **critical**. Rules of thumb:\n",
    "- Overlap with target where $|h(x)p(x)|$ is large\n",
    "- Heavier tails than target (if unsure)\n",
    "- For tail events: shift mean towards the tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38e7c7",
   "metadata": {},
   "source": [
    "## 6. Pitfalls and Failure Modes\n",
    "\n",
    "### Pitfall 1: Weight Degeneracy\n",
    "\n",
    "If $q$ has lighter tails than $p$, a few samples get **huge weights** → variance explodes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87faeffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dangerous: proposal with lighter tails\n",
    "def proposal_light_tails(n):\n",
    "    return np.random.uniform(-1, 1, n)  # Bounded support!\n",
    "\n",
    "def log_weight_light(x):\n",
    "    # Target: N(0,1), Proposal: Uniform[-1,1]\n",
    "    log_p = stats.norm.logpdf(x, 0, 1)\n",
    "    log_q = stats.uniform.logpdf(x, -1, 2)  # Uniform[-1,1]\n",
    "    return log_p - log_q\n",
    "\n",
    "# Function with significant tail mass\n",
    "def tail_sensitive_func(x):\n",
    "    return x**2  # Grows in tails\n",
    "\n",
    "sampler_bad = ImportanceSampler(proposal_light_tails, log_weight_light, seed=42)\n",
    "try:\n",
    "    result_bad = sampler_bad.estimate(tail_sensitive_func, 10000)\n",
    "    print(f\"Estimate: {result_bad.estimate:.6f}\")\n",
    "    print(f\"Std error: {result_bad.std_error:.6f}\")\n",
    "    print(f\"VRF: {result_bad.variance_reduction_factor:.6f}\")\n",
    "    print(f\"\\n⚠ VRF < 1 means IS is WORSE than naive MC!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"\\n⚠ Numerical issues from extreme weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f3892",
   "metadata": {},
   "source": [
    "### Pitfall 2: Infinite Variance\n",
    "\n",
    "If $\\sup_x |h(x)w(x)| = \\infty$, the IS estimator has **infinite variance** even though it's unbiased!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Cauchy target, Gaussian proposal\n",
    "# w(x) = Cauchy(x) / N(x) ~ 1/(1+x²) / exp(-x²/2) → unbounded as x→∞\n",
    "print(\"Theoretical example (not running to avoid instability):\")\n",
    "print(\"  Target: Cauchy distribution (heavy tails)\")\n",
    "print(\"  Proposal: N(0,1) (light tails)\")\n",
    "print(\"  Result: w(x) → ∞ as |x| → ∞\")\n",
    "print(\"\\n⚠ IS estimator is unbiased but has infinite variance!\")\n",
    "print(\"✓ Always check: proposal must have heavier tails than target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced123ea",
   "metadata": {},
   "source": [
    "### Pitfall 3: Numerical Underflow in Weights\n",
    "\n",
    "**Always work in log space** when computing weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67856aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD: Direct computation\n",
    "x_test = np.array([10.0])  # Far in tail\n",
    "p_x = stats.norm.pdf(x_test, 0, 1)\n",
    "q_x = stats.norm.pdf(x_test, 3, 1)\n",
    "w_direct = p_x / q_x\n",
    "print(f\"Direct weight computation:\")\n",
    "print(f\"  p(x) = {p_x[0]:.2e}\")\n",
    "print(f\"  q(x) = {q_x[0]:.2e}\")\n",
    "print(f\"  w(x) = {w_direct[0]:.2e}  ← May underflow!\\n\")\n",
    "\n",
    "# GOOD: Log-space computation\n",
    "log_p = stats.norm.logpdf(x_test, 0, 1)\n",
    "log_q = stats.norm.logpdf(x_test, 3, 1)\n",
    "log_w = log_p - log_q\n",
    "w_stable = np.exp(log_w)\n",
    "print(f\"Log-space weight computation:\")\n",
    "print(f\"  log p(x) = {log_p[0]:.2f}\")\n",
    "print(f\"  log q(x) = {log_q[0]:.2f}\")\n",
    "print(f\"  log w(x) = {log_w[0]:.2f}\")\n",
    "print(f\"  w(x) = {w_stable[0]:.2e}  ✓ Stable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916bd9a",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "✓ **IS reduces variance** by sampling from $q$ concentrated where $|h(x)p(x)|$ is large\n",
    "\n",
    "✓ **Unbiased estimator:** $E_q[h(X)w(X)] = E_p[h(X)]$\n",
    "\n",
    "✓ **Speedup:** 10-1000x for tail probabilities, rare events\n",
    "\n",
    "✓ **Optimal proposal:** $q^*(x) \\propto |h(x)|p(x)$ (zero variance if $h \\geq 0$)\n",
    "\n",
    "✗ **Pitfall 1:** Light-tailed proposals → weight degeneracy\n",
    "\n",
    "✗ **Pitfall 2:** Infinite variance if tails mismatch\n",
    "\n",
    "✗ **Pitfall 3:** Numerical underflow → always use log-space weights\n",
    "\n",
    "**Rule of thumb:** Proposal should have **heavier or equal tails** as target, and **overlap with high-mass regions of $|h(x)|p(x)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c43856",
   "metadata": {},
   "source": [
    "## 8. Exercises\n",
    "\n",
    "**Exercise 1:** Estimate $P(X > 4)$ for $X \\sim \\mathcal{N}(0,1)$ using both naive MC and IS. What VRF do you achieve?\n",
    "\n",
    "**Exercise 2:** What happens if you use $q = \\mathcal{N}(5, 1)$ (shifted too far) for the $P(X > 3)$ problem? Measure the VRF.\n",
    "\n",
    "**Exercise 3:** Estimate $E[X^4]$ for $X \\sim \\mathcal{N}(0,1)$ (true value = 3) using IS with $q = \\mathcal{N}(0, 2)$. Does IS help or hurt?\n",
    "\n",
    "**Exercise 4:** Plot the **effective sample size** $ESS = (\\sum w_i)^2 / \\sum w_i^2$ for different proposal shifts $\\mu \\in [0, 1, 2, 3, 4, 5]$. Where is ESS maximized?\n",
    "\n",
    "**Exercise 5:** Implement **self-normalized IS** (divide by $\\sum w_i$ instead of $N$). When is this preferred?\n",
    "\n",
    "**Exercise 6:** Design a proposal for estimating $\\int_0^\\infty e^{-x} \\mathbb{1}_{x > 10} dx$ (tail of exponential). Compare VRF with exponential proposals of different rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a6684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2614ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions (Spoilers!)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal</summary>\n",
    "\n",
    "```python\n",
    "# Exercise 1\n",
    "threshold_ex1 = 4.0\n",
    "true_prob_ex1 = 1 - stats.norm.cdf(threshold_ex1)\n",
    "target_ex1 = lambda x: (x > threshold_ex1).astype(float)\n",
    "proposal_ex1 = lambda n: np.random.randn(n) + threshold_ex1\n",
    "log_weight_ex1 = lambda x: stats.norm.logpdf(x, 0, 1) - stats.norm.logpdf(x, threshold_ex1, 1)\n",
    "\n",
    "sampler_ex1 = ImportanceSampler(proposal_ex1, log_weight_ex1, seed=42)\n",
    "result_ex1 = sampler_ex1.estimate(target_ex1, 10000)\n",
    "print(f\"True: {true_prob_ex1:.6f}\")\n",
    "print(f\"IS: {result_ex1.estimate:.6f}\")\n",
    "print(f\"VRF: {result_ex1.variance_reduction_factor:.1f}x\")\n",
    "\n",
    "# Exercise 4\n",
    "def compute_ess(weights):\n",
    "    return (np.sum(weights)**2) / np.sum(weights**2)\n",
    "\n",
    "shifts = np.linspace(0, 5, 20)\n",
    "ess_values = []\n",
    "for shift in shifts:\n",
    "    prop = lambda n: np.random.randn(n) + shift\n",
    "    log_w = lambda x: stats.norm.logpdf(x, 0, 1) - stats.norm.logpdf(x, shift, 1)\n",
    "    s = ImportanceSampler(prop, log_w, seed=42)\n",
    "    # Sample and compute ESS\n",
    "    samples = prop(1000)\n",
    "    weights = np.exp(log_w(samples))\n",
    "    ess_values.append(compute_ess(weights))\n",
    "\n",
    "plt.plot(shifts, ess_values)\n",
    "plt.xlabel('Proposal shift μ')\n",
    "plt.ylabel('Effective Sample Size')\n",
    "plt.title('ESS vs Proposal Location')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894de8e",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** [03_control_variates_and_antithetic_sampling.ipynb](03_control_variates_and_antithetic_sampling.ipynb) - Learn complementary variance reduction tricks!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
