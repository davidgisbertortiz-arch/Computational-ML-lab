{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb19aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "# Import from our MC toolbox\n",
    "MCIntegrator, convergence_analysis = safe_import_from(\n",
    "    '05_simulation_monte_carlo.src.mc_integration',\n",
    "    'MCIntegrator', 'convergence_analysis'\n",
    ")\n",
    "\n",
    "# Setup\n",
    "np.random.seed(42)\n",
    "reports_dir = Path('../reports')\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Plot styling\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abce5c",
   "metadata": {},
   "source": [
    "## 1. Intuition: Why Monte Carlo Works\n",
    "\n",
    "**The core idea:** To estimate an integral $I = \\int_a^b f(x)dx$, we:\n",
    "1. Sample $N$ random points $x_1, ..., x_N \\sim \\text{Uniform}[a,b]$\n",
    "2. Compute the sample mean: $\\hat{I} = (b-a) \\cdot \\frac{1}{N}\\sum_{i=1}^N f(x_i)$\n",
    "\n",
    "**Why it works:**\n",
    "- **Law of Large Numbers (LLN):** $\\hat{I} \\to I$ as $N \\to \\infty$ (almost surely)\n",
    "- **Central Limit Theorem (CLT):** $\\sqrt{N}(\\hat{I} - I) \\sim \\mathcal{N}(0, \\sigma^2)$ for large $N$\n",
    "  - Standard error: $SE = \\sigma / \\sqrt{N}$ where $\\sigma^2 = \\text{Var}[f(X)]$\n",
    "  - Error decreases as $O(N^{-1/2})$ **regardless of dimension**\n",
    "\n",
    "**When MC wins:**\n",
    "- High-dimensional integrals (curse of dimensionality affects grid methods)\n",
    "- Complex domains (MC doesn't care about geometry)\n",
    "- When you only need moderate accuracy (1-3 significant figures)\n",
    "\n",
    "**Key insight:** MC error is **probabilistic**. We must always report confidence intervals!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3aa1ec",
   "metadata": {},
   "source": [
    "## 2. Experiment 1: Simple 1D Integral\n",
    "\n",
    "Let's estimate $I = \\int_0^1 x^2 dx = \\frac{1}{3}$ and verify:\n",
    "1. The estimate converges to the true value\n",
    "2. The 95% confidence interval has correct coverage\n",
    "3. Error decreases as $O(N^{-1/2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define integrand and ground truth\n",
    "def f_simple(x):\n",
    "    return x**2\n",
    "\n",
    "true_value = 1/3\n",
    "a, b = 0.0, 1.0\n",
    "\n",
    "# Run MC with increasing sample sizes\n",
    "sample_sizes = [100, 500, 1000, 5000, 10000, 50000]\n",
    "integrator = MCIntegrator(seed=42)\n",
    "\n",
    "results = []\n",
    "for n in sample_sizes:\n",
    "    result = integrator.integrate(func=f_simple, a=a, b=b, n_samples=n)\n",
    "    results.append(result)\n",
    "    print(f\"N={n:6d}: estimate={result.estimate:.6f}, \"\n",
    "          f\"SE={result.std_error:.6f}, \"\n",
    "          f\"95% CI=[{result.ci_lower:.6f}, {result.ci_upper:.6f}]\")\n",
    "\n",
    "print(f\"\\nTrue value: {true_value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5010c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Convergence with confidence bands\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Estimate vs N with CI\n",
    "estimates = [r.estimate for r in results]\n",
    "ci_lower = [r.ci_lower for r in results]\n",
    "ci_upper = [r.ci_upper for r in results]\n",
    "\n",
    "ax1.plot(sample_sizes, estimates, 'o-', label='MC estimate', linewidth=2)\n",
    "ax1.fill_between(sample_sizes, ci_lower, ci_upper, alpha=0.3, label='95% CI')\n",
    "ax1.axhline(true_value, color='red', linestyle='--', linewidth=2, label='True value')\n",
    "ax1.set_xlabel('Number of samples (N)')\n",
    "ax1.set_ylabel('Estimate')\n",
    "ax1.set_title('Convergence with Confidence Intervals')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Error vs N in log-log (should be -1/2 slope)\n",
    "errors = [abs(r.estimate - true_value) for r in results]\n",
    "ax2.loglog(sample_sizes, errors, 'o-', label='Absolute error', linewidth=2)\n",
    "# Reference line: O(N^{-1/2})\n",
    "ref_line = errors[0] * (np.array(sample_sizes) / sample_sizes[0])**(-0.5)\n",
    "ax2.loglog(sample_sizes, ref_line, 'k--', alpha=0.5, label='$O(N^{-1/2})$ reference')\n",
    "ax2.set_xlabel('Number of samples (N)')\n",
    "ax2.set_ylabel('Absolute error')\n",
    "ax2.set_title('Convergence Rate (Log-Log)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / '01_simple_convergence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Plot saved to reports/01_simple_convergence.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783be9c8",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "1. The estimate converges to 0.333... as N increases\n",
    "2. The confidence interval shrinks with more samples\n",
    "3. Error follows $O(N^{-1/2})$ in log-log plot (parallel to reference line)\n",
    "\n",
    "**Critical point:** The error is **stochastic**. If we repeat with a different seed, we get a different trajectory, but the same average behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349f411",
   "metadata": {},
   "source": [
    "## 3. Checking CI Coverage\n",
    "\n",
    "**Question:** If we claim \"95% confidence interval\", does the true value fall inside the CI 95% of the time?\n",
    "\n",
    "Let's verify by running many independent trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2cfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 100 independent MC estimates\n",
    "n_trials = 100\n",
    "n_samples = 5000\n",
    "coverage_count = 0\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    integrator = MCIntegrator(seed=42 + trial)  # Different seed each trial\n",
    "    result = integrator.integrate(func=f_simple, a=a, b=b, n_samples=n_samples)\n",
    "    \n",
    "    # Check if true value is in CI\n",
    "    if result.ci_lower <= true_value <= result.ci_upper:\n",
    "        coverage_count += 1\n",
    "\n",
    "coverage = 100 * coverage_count / n_trials\n",
    "print(f\"CI coverage over {n_trials} trials: {coverage:.1f}%\")\n",
    "print(f\"Expected: ~95%\")\n",
    "print(f\"\\n✓ Confidence intervals are well-calibrated\" if 93 <= coverage <= 97 else \"⚠ CI may be miscalibrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1544e",
   "metadata": {},
   "source": [
    "## 4. Experiment 2: Higher-Dimensional Integral\n",
    "\n",
    "**MC advantage:** Error rate $O(N^{-1/2})$ is **dimension-independent**.\n",
    "\n",
    "Grid methods scale as $O(N^{-2/d})$ in $d$ dimensions → exponential slowdown!\n",
    "\n",
    "**Example:** Estimate volume of unit sphere in 5D:\n",
    "$$I = \\int_{[-1,1]^5} \\mathbb{1}_{x_1^2 + ... + x_5^2 \\leq 1} dx = \\frac{8\\pi^{5/2}}{15\\Gamma(5/2)} \\approx 5.264$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5D unit sphere indicator\n",
    "def f_sphere(x):\n",
    "    \"\"\"x is (n_samples, 5) array\"\"\"\n",
    "    return (np.sum(x**2, axis=1) <= 1.0).astype(float)\n",
    "\n",
    "# True volume\n",
    "from scipy.special import gamma\n",
    "dim = 5\n",
    "true_volume = 2**dim * np.pi**(dim/2) / gamma(dim/2 + 1)\n",
    "\n",
    "# MC estimation\n",
    "integrator = MCIntegrator(seed=42)\n",
    "bounds = [(-1, 1)] * dim  # [-1,1]^5\n",
    "result = integrator.integrate_multidim(\n",
    "    func=f_sphere,\n",
    "    bounds=bounds,\n",
    "    n_samples=100000\n",
    ")\n",
    "\n",
    "print(f\"True volume: {true_volume:.6f}\")\n",
    "print(f\"MC estimate: {result.estimate:.6f} ± {result.std_error:.6f}\")\n",
    "print(f\"95% CI: [{result.ci_lower:.6f}, {result.ci_upper:.6f}]\")\n",
    "print(f\"Relative error: {abs(result.estimate - true_volume)/true_volume * 100:.2f}%\")\n",
    "print(f\"\\n✓ True value within CI: {result.ci_lower <= true_volume <= result.ci_upper}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f75eed",
   "metadata": {},
   "source": [
    "**Key takeaway:** With 100k samples, we get ~1% accuracy in 5D. \n",
    "A grid method would need $(100000)^{5/2} \\approx 10^{12}$ points to match this accuracy!\n",
    "\n",
    "**Rule of thumb:** MC is preferred when $d \\gtrsim 4$ and moderate accuracy suffices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f8d10",
   "metadata": {},
   "source": [
    "## 5. Convergence Analysis Across Multiple Runs\n",
    "\n",
    "Instead of a single trajectory, let's see the **average error behavior** across many runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a150c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence analysis with multiple runs\n",
    "results_conv = convergence_analysis(\n",
    "    func=f_simple,\n",
    "    a=0.0,\n",
    "    b=1.0,\n",
    "    true_value=true_value,\n",
    "    n_samples_list=[100, 500, 1000, 5000, 10000, 50000],\n",
    "    n_runs=20,  # Average over 20 independent runs\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.loglog(results_conv['n_samples'], results_conv['mean_error'], 'o-', \n",
    "          linewidth=2, markersize=8, label='Mean absolute error')\n",
    "ax.fill_between(\n",
    "    results_conv['n_samples'],\n",
    "    np.array(results_conv['mean_error']) - np.array(results_conv['std_error']),\n",
    "    np.array(results_conv['mean_error']) + np.array(results_conv['std_error']),\n",
    "    alpha=0.3,\n",
    "    label='±1 std dev'\n",
    ")\n",
    "\n",
    "# Reference line\n",
    "N = np.array(results_conv['n_samples'])\n",
    "ref = results_conv['mean_error'][0] * (N / N[0])**(-0.5)\n",
    "ax.loglog(N, ref, 'k--', alpha=0.5, label='$O(N^{-1/2})$')\n",
    "\n",
    "ax.set_xlabel('Number of samples (N)')\n",
    "ax.set_ylabel('Mean absolute error')\n",
    "ax.set_title('Convergence: Average Over 20 Independent Runs')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.savefig(reports_dir / '01_convergence_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Plot saved to reports/01_convergence_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b287f",
   "metadata": {},
   "source": [
    "## 6. Common Pitfalls\n",
    "\n",
    "### Pitfall 1: Confusing Variance with Standard Error\n",
    "- **Variance** $\\sigma^2 = \\text{Var}[f(X)]$ measures spread of $f$ values\n",
    "- **Standard error** $SE = \\sigma/\\sqrt{N}$ measures uncertainty in the **mean estimate**\n",
    "- SE decreases with N; variance does NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8aa2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration\n",
    "def high_variance_func(x):\n",
    "    return 100 * x**2  # Same shape as f_simple, but 100x larger variance\n",
    "\n",
    "n = 10000\n",
    "integrator = MCIntegrator(seed=42)\n",
    "r1 = integrator.integrate(f_simple, 0, 1, n)\n",
    "r2 = integrator.integrate(high_variance_func, 0, 1, n)\n",
    "\n",
    "print(\"Function with normal variance:\")\n",
    "print(f\"  Estimate: {r1.estimate:.4f} ± {r1.std_error:.4f}\")\n",
    "print(\"\\nFunction with 100x variance:\")\n",
    "print(f\"  Estimate: {r2.estimate:.4f} ± {r2.std_error:.4f}\")\n",
    "print(f\"\\n⚠ Higher variance → larger SE → wider CI\")\n",
    "print(f\"But estimate is still unbiased!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4f39d",
   "metadata": {},
   "source": [
    "### Pitfall 2: Reusing the Same Random Seed\n",
    "\n",
    "**Never** reuse the same seed for independent experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD: Same seed → identical samples\n",
    "bad_results = []\n",
    "for _ in range(5):\n",
    "    integrator = MCIntegrator(seed=42)  # ⚠ Same seed!\n",
    "    r = integrator.integrate(f_simple, 0, 1, 1000)\n",
    "    bad_results.append(r.estimate)\n",
    "\n",
    "print(\"With same seed (WRONG):\")\n",
    "print(f\"  Estimates: {bad_results}\")\n",
    "print(f\"  Std dev: {np.std(bad_results):.6f}  ← ZERO variance!\")\n",
    "\n",
    "# GOOD: Different seeds → independent samples\n",
    "good_results = []\n",
    "for i in range(5):\n",
    "    integrator = MCIntegrator(seed=42 + i)  # ✓ Different seeds\n",
    "    r = integrator.integrate(f_simple, 0, 1, 1000)\n",
    "    good_results.append(r.estimate)\n",
    "\n",
    "print(\"\\nWith different seeds (CORRECT):\")\n",
    "print(f\"  Estimates: {[f'{x:.6f}' for x in good_results]}\")\n",
    "print(f\"  Std dev: {np.std(good_results):.6f}  ← Shows natural variation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac9aa2",
   "metadata": {},
   "source": [
    "### Pitfall 3: Ignoring Bias from Poor Sampling\n",
    "\n",
    "MC is **unbiased** only if samples are truly uniform (or match the required distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: biased sampling\n",
    "n = 10000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Correct: uniform samples\n",
    "x_uniform = np.random.uniform(0, 1, n)\n",
    "estimate_correct = np.mean(f_simple(x_uniform))\n",
    "\n",
    "# WRONG: non-uniform samples (e.g., beta distribution)\n",
    "x_biased = np.random.beta(0.5, 0.5, n)  # Concentrates at 0 and 1\n",
    "estimate_biased = np.mean(f_simple(x_biased))\n",
    "\n",
    "print(f\"True value: {true_value:.6f}\")\n",
    "print(f\"Uniform samples: {estimate_correct:.6f}  ✓\")\n",
    "print(f\"Biased samples:  {estimate_biased:.6f}  ✗ (systematically wrong)\")\n",
    "print(f\"\\n⚠ Bias: {abs(estimate_biased - true_value):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70de609",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "✓ **MC error is $O(N^{-1/2})$, dimension-independent** → ideal for high-D integrals\n",
    "\n",
    "✓ **Always report uncertainty** (SE or CI) → MC estimates are random!\n",
    "\n",
    "✓ **Convergence diagnostics:** Plot error vs N on log-log to verify -1/2 slope\n",
    "\n",
    "✓ **CI coverage checks:** Run multiple trials to verify calibration\n",
    "\n",
    "✗ **Pitfalls:** Confusing variance with SE, reusing seeds, biased sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067177fa",
   "metadata": {},
   "source": [
    "## 8. Exercises\n",
    "\n",
    "**Exercise 1:** Estimate $\\int_0^\\pi \\sin(x) dx = 2$ using MC. Report estimate with 95% CI.\n",
    "\n",
    "**Exercise 2:** Estimate the area under $e^{-x^2}$ from 0 to 1. Compare with numerical integration (scipy.integrate.quad).\n",
    "\n",
    "**Exercise 3:** Verify the $O(N^{-1/2})$ scaling for a 3D integral of your choice.\n",
    "\n",
    "**Exercise 4:** What happens if you use $N=100$ samples and repeat the estimate 1000 times? Plot the histogram of estimates and verify it's approximately Gaussian (CLT).\n",
    "\n",
    "**Exercise 5:** For the 5D sphere problem, plot how many samples you need to achieve 1%, 0.1%, and 0.01% relative error (approximately).\n",
    "\n",
    "**Exercise 6:** Create an integral where naive MC is inefficient (hint: integrand is nearly zero almost everywhere). We'll fix this with variance reduction in the next notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0974a4e",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions (Spoilers Below!)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solutions</summary>\n",
    "\n",
    "```python\n",
    "# Exercise 1\n",
    "integrator = MCIntegrator(seed=42)\n",
    "result = integrator.integrate(func=np.sin, a=0, b=np.pi, n_samples=10000)\n",
    "print(f\"Estimate: {result.estimate:.6f} ± {result.std_error:.6f}\")\n",
    "print(f\"95% CI: [{result.ci_lower:.6f}, {result.ci_upper:.6f}]\")\n",
    "print(f\"True value: 2.0\")\n",
    "\n",
    "# Exercise 2\n",
    "from scipy.integrate import quad\n",
    "f = lambda x: np.exp(-x**2)\n",
    "mc_result = integrator.integrate(func=f, a=0, b=1, n_samples=50000)\n",
    "quad_result, _ = quad(f, 0, 1)\n",
    "print(f\"MC: {mc_result.estimate:.6f}\")\n",
    "print(f\"Quad: {quad_result:.6f}\")\n",
    "\n",
    "# Exercise 4\n",
    "estimates = []\n",
    "for i in range(1000):\n",
    "    integrator = MCIntegrator(seed=42 + i)\n",
    "    r = integrator.integrate(f_simple, 0, 1, 100)\n",
    "    estimates.append(r.estimate)\n",
    "plt.hist(estimates, bins=30, density=True, alpha=0.7)\n",
    "plt.axvline(1/3, color='red', linestyle='--', label='True value')\n",
    "plt.xlabel('Estimate')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Estimates (CLT verification)')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2dd381",
   "metadata": {},
   "source": [
    "---\n",
    "**Next notebook:** [02_variance_reduction_importance_sampling.ipynb](02_variance_reduction_importance_sampling.ipynb) - Learn how to make MC 10-1000x more efficient!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
