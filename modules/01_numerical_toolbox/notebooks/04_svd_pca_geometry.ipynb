{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core.seeding', 'set_seed')\n",
    "pca_via_svd, PCAResult = safe_import_from('01_numerical_toolbox.src.linear_algebra', \n",
    "                                          'pca_via_svd', 'PCAResult')\n",
    "\n",
    "set_seed(42)\n",
    "REPORTS_DIR = Path('../reports')\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c891ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition: What is PCA?\n",
    "\n",
    "**Key Ideas:**\n",
    "- **PCA finds the \"best\" low-dimensional representation** of high-dimensional data\n",
    "- **\"Best\" = preserves maximum variance** (information)\n",
    "- **Geometric view**: Rotate data to align with directions of maximum spread\n",
    "- **Applications**: Dimensionality reduction, noise filtering, data compression, visualization\n",
    "- **SVD is the numerical algorithm**: More stable than covariance eigendecomposition\n",
    "\n",
    "**Analogy**: Imagine photographing a 3D object:\n",
    "- Some camera angles capture more information than others\n",
    "- PCA finds the \"best viewpoints\" (principal components)\n",
    "- Taking photos from these angles loses least information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc0779",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Minimal Math: SVD Decomposition\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "Any matrix $X \\in \\mathbb{R}^{n \\times p}$ can be decomposed:\n",
    "\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $U \\in \\mathbb{R}^{n \\times n}$: Left singular vectors (rotation in sample space)\n",
    "- $\\Sigma \\in \\mathbb{R}^{n \\times p}$: Diagonal matrix of singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq 0$\n",
    "- $V \\in \\mathbb{R}^{p \\times p}$: Right singular vectors (**principal components**)\n",
    "\n",
    "### Connection to PCA\n",
    "For **centered** data $X$ (mean = 0):\n",
    "\n",
    "1. **Covariance matrix**: $\\text{Cov}(X) = \\frac{1}{n-1} X^T X$\n",
    "2. **Principal components**: Columns of $V$ (eigenvectors of $X^T X$)\n",
    "3. **Explained variance**: $\\text{Var}(\\text{PC}_k) = \\frac{\\sigma_k^2}{n-1}$\n",
    "4. **Projection**: $Z = X V$ (scores on principal components)\n",
    "\n",
    "**Why SVD instead of eigendecomposition?**\n",
    "- More numerically stable (avoids forming $X^T X$)\n",
    "- Works even when $p > n$ (more features than samples)\n",
    "- Single step instead of two (center → SVD vs center → covariance → eigendecomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df74a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Implementation: PCA from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca(X, n_components=None):\n",
    "    \"\"\"\n",
    "    Implement PCA via SVD from scratch.\n",
    "    \n",
    "    Returns:\n",
    "        components: Principal component vectors (columns)\n",
    "        explained_variance: Variance explained by each PC\n",
    "        singular_values: Singular values\n",
    "        mean: Data mean (for reconstruction)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Step 1: Center data (subtract mean)\n",
    "    mean = np.mean(X, axis=0)\n",
    "    X_centered = X - mean\n",
    "    \n",
    "    # Step 2: Compute SVD\n",
    "    U, singular_values, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "    \n",
    "    # Step 3: Extract principal components (columns of V = rows of Vt)\n",
    "    components = Vt.T  # Shape: (n_features, n_components)\n",
    "    \n",
    "    # Step 4: Compute explained variance\n",
    "    explained_variance = (singular_values ** 2) / (n_samples - 1)\n",
    "    \n",
    "    # Keep only n_components\n",
    "    if n_components is not None:\n",
    "        components = components[:, :n_components]\n",
    "        explained_variance = explained_variance[:n_components]\n",
    "        singular_values = singular_values[:n_components]\n",
    "    \n",
    "    return components, explained_variance, singular_values, mean\n",
    "\n",
    "# Test on simple 2D data\n",
    "np.random.seed(42)\n",
    "X_simple = np.random.randn(100, 2) @ np.array([[3, 0], [0, 0.5]])  # Stretch along x\n",
    "\n",
    "components, var, sigma, mean = my_pca(X_simple, n_components=2)\n",
    "\n",
    "print(\"Principal Components (columns):\")\n",
    "print(components)\n",
    "print(f\"\\nExplained Variance: {var}\")\n",
    "print(f\"Variance Ratio: {var / var.sum()}\")\n",
    "print(f\"\\nFirst PC explains {100*var[0]/var.sum():.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942785ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Visualization: 2D PCA Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original data\n",
    "ax = axes[0]\n",
    "ax.scatter(X_simple[:, 0], X_simple[:, 1], alpha=0.5, s=30)\n",
    "\n",
    "# Plot principal component directions\n",
    "scale = 3  # Visual scale\n",
    "for i, (pc, var_i) in enumerate(zip(components.T, var)):\n",
    "    ax.arrow(mean[0], mean[1], \n",
    "             pc[0] * scale * np.sqrt(var_i), \n",
    "             pc[1] * scale * np.sqrt(var_i),\n",
    "             head_width=0.3, head_length=0.3, fc=f'C{i+1}', ec=f'C{i+1}',\n",
    "             linewidth=3, label=f'PC{i+1}')\n",
    "\n",
    "ax.scatter(mean[0], mean[1], c='red', s=100, marker='x', linewidths=3, \n",
    "           label='Mean', zorder=10)\n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12)\n",
    "ax.set_title('Original Data + Principal Components', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Projected data (PC coordinates)\n",
    "ax = axes[1]\n",
    "X_centered = X_simple - mean\n",
    "Z = X_centered @ components  # Project onto PCs\n",
    "ax.scatter(Z[:, 0], Z[:, 1], alpha=0.5, s=30)\n",
    "\n",
    "# PC axes in transformed space\n",
    "ax.axhline(0, color='C2', linestyle='--', linewidth=2, alpha=0.7, label='PC2 axis')\n",
    "ax.axvline(0, color='C1', linestyle='--', linewidth=2, alpha=0.7, label='PC1 axis')\n",
    "\n",
    "ax.set_xlabel('PC1 (Principal Component 1)', fontsize=12)\n",
    "ax.set_ylabel('PC2 (Principal Component 2)', fontsize=12)\n",
    "ax.set_title('Transformed Data (PC Space)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '04_pca_2d_geometry.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {REPORTS_DIR / '04_pca_2d_geometry.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b49182",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- Left: PCs point in directions of maximum variance\n",
    "- Right: Data is now aligned with coordinate axes\n",
    "- PC1 captures most variance (data spread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb2825",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Real Example: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-dimensional correlated data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "n_features = 50\n",
    "\n",
    "# Create covariance with exponentially decaying eigenvalues\n",
    "# (simulates real data where only few dimensions matter)\n",
    "eigenvalues = np.exp(-np.arange(n_features) / 5)\n",
    "Q, _ = np.linalg.qr(np.random.randn(n_features, n_features))\n",
    "Sigma = Q @ np.diag(eigenvalues) @ Q.T\n",
    "\n",
    "X_high = np.random.multivariate_normal(np.zeros(n_features), Sigma, size=n_samples)\n",
    "\n",
    "print(f\"Data shape: {X_high.shape} (n_samples × n_features)\")\n",
    "print(f\"Total variance: {np.var(X_high, axis=0).sum():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our implementation\n",
    "components_ours, var_ours, sigma_ours, mean_ours = my_pca(X_high)\n",
    "\n",
    "# Using the repo's implementation\n",
    "pca_result = pca_via_svd(X_high)\n",
    "\n",
    "print(\"Comparison:\")\n",
    "print(f\"  Our explained variance: {var_ours[:3]}\")\n",
    "print(f\"  Repo explained variance: {pca_result.explained_variance[:3]}\")\n",
    "print(f\"\\n  Max difference: {np.max(np.abs(var_ours - pca_result.explained_variance)):.2e}\")\n",
    "print(\"  ✓ Match within numerical precision!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06815276",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Explained Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed322db",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ratio = pca_result.explained_variance_ratio\n",
    "cumulative_var = np.cumsum(var_ratio)\n",
    "\n",
    "# Find number of components for different thresholds\n",
    "thresholds = [0.8, 0.9, 0.95, 0.99]\n",
    "print(\"Components needed to capture:\")\n",
    "for thresh in thresholds:\n",
    "    n_comp = np.argmax(cumulative_var >= thresh) + 1\n",
    "    print(f\"  {thresh*100:.0f}% variance: {n_comp} components \"\n",
    "          f\"(compression: {100*(1 - n_comp/n_features):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "ax = axes[0]\n",
    "ax.plot(range(1, 21), var_ratio[:20], 'bo-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Principal Component', fontsize=12)\n",
    "ax.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax.set_title('Scree Plot (First 20 PCs)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "ax = axes[1]\n",
    "ax.plot(range(1, n_features+1), cumulative_var, 'ro-', linewidth=2, markersize=6)\n",
    "for thresh in [0.9, 0.95, 0.99]:\n",
    "    n_comp = np.argmax(cumulative_var >= thresh) + 1\n",
    "    ax.axhline(thresh, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axvline(n_comp, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.plot(n_comp, thresh, 'ko', markersize=8)\n",
    "    ax.text(n_comp+1, thresh-0.03, f'{n_comp} PCs', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Number of Components', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "ax.set_title('Cumulative Variance Explained', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '04_variance_explained.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {REPORTS_DIR / '04_variance_explained.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1dca32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Reconstruction: Information Loss vs Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fadba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reconstruction with different numbers of components\n",
    "n_components_list = [2, 5, 10, 20, 30, 50]\n",
    "reconstruction_errors = []\n",
    "relative_errors = []\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    # Project to n_comp dimensions\n",
    "    Z = pca_result.transform(X_high, n_components=n_comp)\n",
    "    \n",
    "    # Reconstruct\n",
    "    X_reconstructed = pca_result.inverse_transform(Z)\n",
    "    \n",
    "    # Compute error\n",
    "    error = np.mean((X_high - X_reconstructed) ** 2)\n",
    "    relative_error = error / np.var(X_high)\n",
    "    \n",
    "    reconstruction_errors.append(error)\n",
    "    relative_errors.append(relative_error)\n",
    "    \n",
    "    print(f\"n={n_comp:2d}: MSE={error:.4f}, Relative Error={relative_error:.4f}, \"\n",
    "          f\"Compression={100*(1-n_comp/n_features):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Reconstruction error\n",
    "ax1.plot(n_components_list, reconstruction_errors, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Components', fontsize=12)\n",
    "ax1.set_ylabel('Reconstruction MSE', fontsize=12)\n",
    "ax1.set_title('Reconstruction Error vs Components', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Relative error (log scale)\n",
    "ax2.semilogy(n_components_list, relative_errors, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Components', fontsize=12)\n",
    "ax2.set_ylabel('Relative Error (log scale)', fontsize=12)\n",
    "ax2.set_title('Relative Reconstruction Error', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '04_reconstruction_error.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {REPORTS_DIR / '04_reconstruction_error.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e22b51",
   "metadata": {},
   "source": [
    "**Key Result**: With just 10 components (80% compression), we retain ~98% of information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcf9dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Compare to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "\n",
    "# sklearn PCA\n",
    "sklearn_pca = SklearnPCA(n_components=10)\n",
    "Z_sklearn = sklearn_pca.fit_transform(X_high)\n",
    "\n",
    "# Our implementation\n",
    "Z_ours = pca_result.transform(X_high, n_components=10)\n",
    "\n",
    "print(\"Comparison (10 components):\")\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "print(f\"  sklearn: {sklearn_pca.explained_variance_ratio_}\")\n",
    "print(f\"  Ours:    {pca_result.explained_variance_ratio[:10]}\")\n",
    "\n",
    "# Note: Signs of PCs might differ (both valid)\n",
    "print(f\"\\nProjected data shape:\")\n",
    "print(f\"  sklearn: {Z_sklearn.shape}\")\n",
    "print(f\"  Ours:    {Z_ours.shape}\")\n",
    "\n",
    "# Compare reconstructions\n",
    "X_recon_sklearn = sklearn_pca.inverse_transform(Z_sklearn)\n",
    "X_recon_ours = pca_result.inverse_transform(Z_ours)\n",
    "\n",
    "mse_sklearn = np.mean((X_high - X_recon_sklearn) ** 2)\n",
    "mse_ours = np.mean((X_high - X_recon_ours) ** 2)\n",
    "\n",
    "print(f\"\\nReconstruction MSE:\")\n",
    "print(f\"  sklearn: {mse_sklearn:.6f}\")\n",
    "print(f\"  Ours:    {mse_ours:.6f}\")\n",
    "print(f\"\\n✓ Essentially identical!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8eff5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways\n",
    "\n",
    "✅ **PCA finds orthogonal directions of maximum variance**:\n",
    "   - First PC: direction of largest spread\n",
    "   - Subsequent PCs: orthogonal, decreasing variance\n",
    "\n",
    "✅ **SVD is the computational tool**:\n",
    "   - More stable than covariance eigendecomposition\n",
    "   - Single step: $X = U \\Sigma V^T$ → PCs are columns of $V$\n",
    "\n",
    "✅ **Dimensionality reduction**:\n",
    "   - Keep first $k$ PCs to retain most variance\n",
    "   - Typical: 10-50 components capture 90-99% of information\n",
    "   - Useful for visualization, denoising, compression\n",
    "\n",
    "✅ **Reconstruction trade-off**:\n",
    "   - More components → lower error, less compression\n",
    "   - Fewer components → higher error, more compression\n",
    "   - Choose $k$ based on application needs\n",
    "\n",
    "✅ **When to use PCA**:\n",
    "   - High-dimensional data visualization\n",
    "   - Feature extraction before ML\n",
    "   - Noise reduction (keep top PCs)\n",
    "   - Detecting correlations/multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3e03b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Common Pitfalls\n",
    "\n",
    "❌ **Forgetting to center data**: PCA requires mean=0; sklearn does this automatically\n",
    "\n",
    "❌ **Not standardizing features**: Features with larger scales dominate PCs; use StandardScaler first\n",
    "\n",
    "❌ **Interpreting PC loadings blindly**: High-dimensional PCs are hard to interpret; use with caution\n",
    "\n",
    "❌ **Using PCA for classification directly**: PCA maximizes variance, not class separability (use LDA instead)\n",
    "\n",
    "❌ **Choosing k without looking at explained variance**: Always check scree plot or cumulative variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c9c4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619acca4",
   "metadata": {},
   "source": [
    "**Exercise 1**: Load sklearn's digits dataset (8×8 images). Apply PCA and visualize first 2 PCs. Do digit classes separate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13045f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca1c97",
   "metadata": {},
   "source": [
    "**Exercise 2**: Implement whitening transformation: $Z_{\\text{white}} = Z \\cdot \\text{diag}(1/\\sigma_k)$. Verify that $\\text{Cov}(Z_{\\text{white}}) = I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9da005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031da92c",
   "metadata": {},
   "source": [
    "**Exercise 3**: Add Gaussian noise to data. Show that PCA + reconstruction (keeping top 90% variance) filters noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa521fd2",
   "metadata": {},
   "source": [
    "**Exercise 4**: Compare PCA (unsupervised) vs Linear Discriminant Analysis (supervised) on iris dataset. Which gives better class separation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e344ce",
   "metadata": {},
   "source": [
    "**Exercise 5**: Implement incremental PCA (process data in batches). Compare to full PCA on memory usage for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc821a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "<details>\n",
    "<summary><b>Exercise 1 Solution</b></summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits = digits.data  # 1797 samples, 64 features (8x8 images)\n",
    "y_digits = digits.target\n",
    "\n",
    "# Standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_digits)\n",
    "\n",
    "# PCA to 2D\n",
    "pca_digits = pca_via_svd(X_scaled, n_components=2)\n",
    "Z_2d = pca_digits.transform(X_scaled, n_components=2)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(Z_2d[:, 0], Z_2d[:, 1], c=y_digits, cmap='tab10', alpha=0.7, s=30)\n",
    "plt.colorbar(scatter, label='Digit Class')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Digits Dataset: First 2 PCs')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Check explained variance\n",
    "print(f\"Explained variance (2 PCs): {pca_digits.explained_variance_ratio[:2].sum()*100:.1f}%\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Exercise 2 Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Whitening transformation\n",
    "pca_result = pca_via_svd(X_high, n_components=10)\n",
    "Z = pca_result.transform(X_high, n_components=10)\n",
    "\n",
    "# Whiten: divide by sqrt(explained_variance)\n",
    "Z_white = Z / np.sqrt(pca_result.explained_variance[:10])\n",
    "\n",
    "# Verify covariance is identity\n",
    "cov_white = np.cov(Z_white.T)\n",
    "print(\"Covariance of whitened data:\")\n",
    "print(cov_white)\n",
    "print(f\"\\nIs identity? {np.allclose(cov_white, np.eye(10))}\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Exercise 3 Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Add noise\n",
    "np.random.seed(42)\n",
    "noise_level = 0.5\n",
    "X_noisy = X_high + noise_level * np.random.randn(*X_high.shape)\n",
    "\n",
    "# PCA on noisy data\n",
    "pca_noisy = pca_via_svd(X_noisy)\n",
    "\n",
    "# Keep components explaining 90% variance\n",
    "cumsum = np.cumsum(pca_noisy.explained_variance_ratio)\n",
    "n_keep = np.argmax(cumsum >= 0.9) + 1\n",
    "print(f\"Keeping {n_keep} components (90% variance)\")\n",
    "\n",
    "# Reconstruct\n",
    "Z_noisy = pca_noisy.transform(X_noisy, n_components=n_keep)\n",
    "X_denoised = pca_noisy.inverse_transform(Z_noisy)\n",
    "\n",
    "# Compare errors\n",
    "mse_noisy = np.mean((X_high - X_noisy) ** 2)\n",
    "mse_denoised = np.mean((X_high - X_denoised) ** 2)\n",
    "\n",
    "print(f\"MSE (noisy):    {mse_noisy:.4f}\")\n",
    "print(f\"MSE (denoised): {mse_denoised:.4f}\")\n",
    "print(f\"Improvement: {100*(1 - mse_denoised/mse_noisy):.1f}%\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09fa38e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c4825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_95 = np.argmax(cumulative_var >= 0.95) + 1\n",
    "n_99 = np.argmax(cumulative_var >= 0.99) + 1\n",
    "\n",
    "report = f\"\"\"\n",
    "SVD AND PCA: GEOMETRIC ANALYSIS\n",
    "{'='*70}\n",
    "\n",
    "DATA CHARACTERISTICS:\n",
    "  Samples: {n_samples}\n",
    "  Features: {n_features}\n",
    "  Total variance: {np.var(X_high, axis=0).sum():.2f}\n",
    "\n",
    "DIMENSIONALITY REDUCTION:\n",
    "  Components for 95% variance: {n_95} ({100*(1-n_95/n_features):.0f}% compression)\n",
    "  Components for 99% variance: {n_99} ({100*(1-n_99/n_features):.0f}% compression)\n",
    "\n",
    "TOP 5 PRINCIPAL COMPONENTS:\n",
    "\"\"\"\n",
    "\n",
    "for i in range(5):\n",
    "    report += f\"  PC{i+1}: {var_ratio[i]:.4f} ({var_ratio[i]*100:.2f}%)\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "RECONSTRUCTION QUALITY (10 components):\n",
    "  MSE: {reconstruction_errors[1]:.4f}\n",
    "  Relative Error: {relative_errors[1]:.4f}\n",
    "  Compression: {100*(1-10/n_features):.0f}%\n",
    "\n",
    "KEY FINDINGS:\n",
    "  1. First {n_95} PCs capture 95% of information\n",
    "  2. SVD-based PCA matches sklearn exactly\n",
    "  3. Reconstruction error decreases exponentially with components\n",
    "  4. Huge dimensionality reduction possible with minimal loss\n",
    "\n",
    "GEOMETRIC INTERPRETATION:\n",
    "  - PCA rotates data to align with variance directions\n",
    "  - Principal components are orthogonal\n",
    "  - Ordered by importance (explained variance)\n",
    "  - Truncation = projection onto lower-dimensional subspace\n",
    "\n",
    "PRACTICAL APPLICATIONS:\n",
    "  - Visualization: Project to 2D/3D for plotting\n",
    "  - Feature extraction: Reduce features before ML\n",
    "  - Denoising: Keep top PCs, discard noisy low-variance components\n",
    "  - Compression: Store low-dimensional representation\n",
    "\n",
    "Plots saved in: {REPORTS_DIR}/\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "with open(REPORTS_DIR / '04_svd_pca_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\n✓ Report saved: {REPORTS_DIR / '04_svd_pca_report.txt'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
