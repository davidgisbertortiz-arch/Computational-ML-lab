{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4cb620",
   "metadata": {},
   "source": [
    "# PCA via SVD: Explained Variance and Reconstruction\n",
    "\n",
    "This notebook explores Principal Component Analysis (PCA) through Singular Value Decomposition (SVD).\n",
    "\n",
    "**Learning objectives:**\n",
    "- Understand SVD geometry and PCA connection\n",
    "- Visualize explained variance and scree plots\n",
    "- Demonstrate reconstruction error vs number of components\n",
    "- Apply PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "sys.path.append('/workspaces/Computational-ML-lab')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import seaborn as sns\n",
    "\n",
    "from modules.01_numerical_toolbox.src.linear_algebra import (\n",
    "    pca_via_svd, condition_number, ridge_regularization, demonstrate_ill_conditioning\n",
    ")\n",
    "from modules.00_repo_standards.src.mlphys_core import set_seed\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e6a23",
   "metadata": {},
   "source": [
    "## 1. Generate Correlated Data\n",
    "\n",
    "Create synthetic data with known covariance structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlated 2D data for visualization\n",
    "set_seed(42)\n",
    "\n",
    "# Covariance with strong correlation\n",
    "cov = np.array([[2.0, 1.5],\n",
    "                [1.5, 1.0]])\n",
    "\n",
    "X_2d = np.random.multivariate_normal([0, 0], cov, size=200)\n",
    "\n",
    "# Plot original data\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.6, s=50)\n",
    "plt.axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Original 2D Data (Correlated)\")\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Covariance matrix:\\\\n{np.cov(X_2d.T)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a7b72",
   "metadata": {},
   "source": [
    "## 2. Apply PCA and Visualize Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca_result = pca_via_svd(X_2d, n_components=2)\n",
    "\n",
    "print(\"Principal Components (eigenvectors):\")\n",
    "print(pca_result.components)\n",
    "print(f\"\\\\nExplained variance: {pca_result.explained_variance}\")\n",
    "print(f\"Explained variance ratio: {pca_result.explained_variance_ratio}\")\n",
    "print(f\"Total variance explained: {np.sum(pca_result.explained_variance_ratio):.4f}\")\n",
    "\n",
    "# Visualize with principal components overlaid\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Original space with PCs\n",
    "ax1.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.6, s=50, label='Data')\n",
    "ax1.axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
    "ax1.axvline(0, color='k', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Draw principal components as arrows\n",
    "scale = 3.0\n",
    "for i in range(2):\n",
    "    pc = pca_result.components[:, i]\n",
    "    length = np.sqrt(pca_result.explained_variance[i]) * scale\n",
    "    ax1.arrow(0, 0, pc[0]*length, pc[1]*length, \n",
    "              head_width=0.2, head_length=0.3, fc=f'C{i+1}', ec=f'C{i+1}',\n",
    "              linewidth=3, label=f'PC{i+1}')\n",
    "\n",
    "ax1.set_xlabel(\"$x_1$\")\n",
    "ax1.set_ylabel(\"$x_2$\")\n",
    "ax1.set_title(\"Original Space with Principal Components\")\n",
    "ax1.legend()\n",
    "ax1.axis('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Transformed space (PCA coordinates)\n",
    "Z = pca_result.transform(X_2d)\n",
    "ax2.scatter(Z[:, 0], Z[:, 1], alpha=0.6, s=50, color='C2')\n",
    "ax2.axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
    "ax2.axvline(0, color='k', linestyle='--', linewidth=0.5)\n",
    "ax2.set_xlabel(\"PC1\")\n",
    "ax2.set_ylabel(\"PC2\")\n",
    "ax2.set_title(\"PCA-Transformed Space (Decorrelated)\")\n",
    "ax2.axis('equal')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d9549",
   "metadata": {},
   "source": [
    "## 3. High-Dimensional PCA and Scree Plot\n",
    "\n",
    "Generate higher-dimensional data to demonstrate variance selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74589b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 20D data with decaying eigenvalues\n",
    "n_features = 20\n",
    "eigenvalues = np.exp(-np.arange(n_features) / 3)  # Exponential decay\n",
    "Q, _ = np.linalg.qr(np.random.randn(n_features, n_features))\n",
    "Sigma = Q @ np.diag(eigenvalues) @ Q.T\n",
    "\n",
    "X_high = np.random.multivariate_normal(np.zeros(n_features), Sigma, size=300)\n",
    "\n",
    "# Perform PCA\n",
    "pca_high = pca_via_svd(X_high)\n",
    "\n",
    "# Plot scree plot and cumulative variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Scree plot\n",
    "ax1.bar(range(1, n_features + 1), pca_high.explained_variance_ratio, alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel(\"Principal Component\")\n",
    "ax1.set_ylabel(\"Explained Variance Ratio\")\n",
    "ax1.set_title(\"Scree Plot\")\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative_var = np.cumsum(pca_high.explained_variance_ratio)\n",
    "ax2.plot(range(1, n_features + 1), cumulative_var, 'o-', linewidth=2, markersize=8, color='darkorange')\n",
    "ax2.axhline(0.95, color='r', linestyle='--', label='95% threshold')\n",
    "ax2.axhline(0.99, color='purple', linestyle='--', label='99% threshold')\n",
    "ax2.set_xlabel(\"Number of Components\")\n",
    "ax2.set_ylabel(\"Cumulative Explained Variance\")\n",
    "ax2.set_title(\"Cumulative Variance Explained\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find components for thresholds\n",
    "n_95 = np.argmax(cumulative_var >= 0.95) + 1\n",
    "n_99 = np.argmax(cumulative_var >= 0.99) + 1\n",
    "\n",
    "print(f\"Components needed for 95% variance: {n_95}/{n_features}\")\n",
    "print(f\"Components needed for 99% variance: {n_99}/{n_features}\")\n",
    "print(f\"\\\\nDimensionality reduction: {n_features} → {n_95} ({(1 - n_95/n_features)*100:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8272678b",
   "metadata": {},
   "source": [
    "## 4. Reconstruction Error vs Number of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a21627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstruction error for different numbers of components\n",
    "n_components_range = range(1, n_features + 1)\n",
    "reconstruction_errors = []\n",
    "\n",
    "for n_comp in n_components_range:\n",
    "    Z = pca_high.transform(X_high, n_components=n_comp)\n",
    "    X_reconstructed = pca_high.inverse_transform(Z)\n",
    "    error = np.mean((X_high - X_reconstructed) ** 2)\n",
    "    reconstruction_errors.append(error)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Absolute error\n",
    "ax1.plot(n_components_range, reconstruction_errors, 'o-', linewidth=2, markersize=6, color='crimson')\n",
    "ax1.set_xlabel(\"Number of Components\")\n",
    "ax1.set_ylabel(\"Mean Squared Reconstruction Error\")\n",
    "ax1.set_title(\"Reconstruction Error vs Components\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Relative error (as fraction of total variance)\n",
    "total_variance = np.sum(pca_high.explained_variance)\n",
    "relative_errors = np.array(reconstruction_errors) / total_variance\n",
    "ax2.semilogy(n_components_range, relative_errors, 'o-', linewidth=2, markersize=6, color='darkgreen')\n",
    "ax2.set_xlabel(\"Number of Components\")\n",
    "ax2.set_ylabel(\"Relative Reconstruction Error (log scale)\")\n",
    "ax2.set_title(\"Relative Error Decay\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Reconstruction error for different components:\")\n",
    "for n_comp in [1, 5, 10, 15, 20]:\n",
    "    if n_comp <= n_features:\n",
    "        error = reconstruction_errors[n_comp - 1]\n",
    "        print(f\"  {n_comp:2d} components: MSE = {error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e93bce",
   "metadata": {},
   "source": [
    "## 5. Ill-Conditioning and Ridge Stabilization\n",
    "\n",
    "Demonstrate how ridge regularization improves numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e69221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate ill-conditioning effects\n",
    "result = demonstrate_ill_conditioning(kappa_target=1000.0, n_samples=100, n_features=50, seed=42)\n",
    "\n",
    "print(f\"Matrix condition number: κ = {result['condition_number']:.1f}\")\n",
    "print(f\"\\\\nOLS solution norm: {result['ols_norm']:.2f}\")\n",
    "print(f\"True weights norm: {result['true_norm']:.2f}\")\n",
    "print(\"\\\\nRidge regularization results:\")\n",
    "print(f\"{'λ':<10s} {'Weight norm':<15s} {'κ_before':<15s} {'κ_after':<15s} {'Train MSE':<15s}\")\n",
    "\n",
    "for lambda_, ridge_info in result['ridge_results'].items():\n",
    "    print(f\"{lambda_:<10.2f} \"\n",
    "          f\"{ridge_info['norm']:<15.2f} \"\n",
    "          f\"{ridge_info['kappa_before']:<15.1f} \"\n",
    "          f\"{ridge_info['kappa_after']:<15.1f} \"\n",
    "          f\"{ridge_info['train_mse']:<15.6f}\")\n",
    "\n",
    "# Visualize conditioning improvement\n",
    "lambdas = list(result['ridge_results'].keys())\n",
    "kappas_after = [result['ridge_results'][l]['kappa_after'] for l in lambdas]\n",
    "weight_norms = [result['ridge_results'][l]['norm'] for l in lambdas]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Condition number vs lambda\n",
    "ax1.semilogx(lambdas, kappas_after, 'o-', linewidth=2, markersize=10, color='steelblue')\n",
    "ax1.axhline(result['condition_number'], color='r', linestyle='--', label='Original κ')\n",
    "ax1.set_xlabel(\"Ridge Parameter λ\")\n",
    "ax1.set_ylabel(\"Condition Number (after ridge)\")\n",
    "ax1.set_title(\"Conditioning Improvement with Ridge\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Weight norm vs lambda (shrinkage effect)\n",
    "ax2.semilogx(lambdas, weight_norms, 'o-', linewidth=2, markersize=10, color='darkorange')\n",
    "ax2.axhline(result['true_norm'], color='g', linestyle='--', label='True norm')\n",
    "ax2.axhline(result['ols_norm'], color='r', linestyle='--', label='OLS norm')\n",
    "ax2.set_xlabel(\"Ridge Parameter λ\")\n",
    "ax2.set_ylabel(\"Solution Norm ||w||\")\n",
    "ax2.set_title(\"Weight Shrinkage Effect\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nKey insight: Ridge adds λ to all eigenvalues, reducing κ = σ_max/σ_min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52a7d8",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "**PCA via SVD:**\n",
    "- More stable than eigendecomposition of covariance matrix\n",
    "- $X = U \\Sigma V^T$ gives PCs as columns of $V$\n",
    "- Variance = $\\sigma_i^2 / (n-1)$\n",
    "\n",
    "**Dimensionality reduction:**\n",
    "- Keep components explaining 95-99% of variance\n",
    "- Balance reconstruction error vs compression\n",
    "- Scree plot helps identify \"elbow\"\n",
    "\n",
    "**Numerical stability:**\n",
    "- Ill-conditioning ($\\kappa \\gg 1$) amplifies errors\n",
    "- Ridge regularization: $\\min ||Xw - y||^2 + \\lambda ||w||^2$\n",
    "- Adds $\\lambda$ to eigenvalues, reducing condition number\n",
    "\n",
    "**ML applications:**\n",
    "- PCA for feature extraction and visualization\n",
    "- Ridge regression prevents overfitting on correlated features\n",
    "- Understanding $\\kappa$ guides preprocessing choices"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
