{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core.seeding', 'set_seed')\n",
    "GradientDescent = safe_import_from('01_numerical_toolbox.src.optimizers_from_scratch', \n",
    "                                   'GradientDescent')\n",
    "Momentum = safe_import_from('01_numerical_toolbox.src.optimizers_from_scratch', 'Momentum')\n",
    "Adam = safe_import_from('01_numerical_toolbox.src.optimizers_from_scratch', 'Adam')\n",
    "\n",
    "set_seed(42)\n",
    "REPORTS_DIR = Path('../reports')\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59d6698",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition: Why Momentum and Adaptivity?\n",
    "\n",
    "**Problems with Vanilla GD:**\n",
    "- **Oscillates** in narrow valleys (ill-conditioned problems)\n",
    "- **Single learning rate** doesn't adapt to problem geometry\n",
    "- **Slow** in flat regions\n",
    "\n",
    "**Solutions:**\n",
    "- **Momentum**: Accumulates velocity → smooths oscillations, accelerates in consistent directions\n",
    "- **Adam**: Per-parameter adaptive learning rates → handles different scales automatically\n",
    "\n",
    "**Analogy**: \n",
    "- GD = walking carefully step-by-step\n",
    "- Momentum = rolling a ball down hill (builds speed)\n",
    "- Adam = smart walker who adjusts stride per terrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50698dbd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Minimal Math: Update Rules\n",
    "\n",
    "### Vanilla Gradient Descent\n",
    "$$\n",
    "\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\alpha \\nabla f(\\mathbf{x}_t)\n",
    "$$\n",
    "\n",
    "### Momentum (Polyak)\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{v}_{t+1} &= \\beta \\mathbf{v}_t + \\nabla f(\\mathbf{x}_t) \\\\\n",
    "\\mathbf{x}_{t+1} &= \\mathbf{x}_t - \\alpha \\mathbf{v}_{t+1}\n",
    "\\end{align}\n",
    "$$\n",
    "- $\\beta$ (typically 0.9): momentum coefficient\n",
    "- Velocity $\\mathbf{v}$ accumulates exponentially weighted moving average of gradients\n",
    "\n",
    "### Adam (Adaptive Moment Estimation)\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{m}_t &= \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1) \\nabla f(\\mathbf{x}_{t-1}) \\\\\n",
    "\\mathbf{v}_t &= \\beta_2 \\mathbf{v}_{t-1} + (1-\\beta_2) [\\nabla f(\\mathbf{x}_{t-1})]^2 \\\\\n",
    "\\hat{\\mathbf{m}}_t &= \\mathbf{m}_t / (1-\\beta_1^t) \\quad \\text{(bias correction)} \\\\\n",
    "\\hat{\\mathbf{v}}_t &= \\mathbf{v}_t / (1-\\beta_2^t) \\\\\n",
    "\\mathbf{x}_t &= \\mathbf{x}_{t-1} - \\alpha \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon}\n",
    "\\end{align}\n",
    "$$\n",
    "- $\\mathbf{m}$: first moment (mean of gradients)\n",
    "- $\\mathbf{v}$: second moment (uncentered variance)\n",
    "- Defaults: $\\beta_1=0.9, \\beta_2=0.999, \\epsilon=10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59ce44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Experiment 1: Controlled Quadratic Bowl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ill_conditioned_quadratic(kappa=20):\n",
    "    \"\"\"Create quadratic with specified condition number.\"\"\"\n",
    "    # Eigenvalues from 1 to kappa\n",
    "    eigenvalues = np.linspace(1, kappa, 2)\n",
    "    Q, _ = np.linalg.qr(np.random.randn(2, 2))\n",
    "    A = Q @ np.diag(eigenvalues) @ Q.T\n",
    "    \n",
    "    def f(x):\n",
    "        return 0.5 * x @ A @ x\n",
    "    \n",
    "    def grad_f(x):\n",
    "        return A @ x\n",
    "    \n",
    "    return A, f, grad_f\n",
    "\n",
    "# Setup problem\n",
    "A, f, grad_f = create_ill_conditioned_quadratic(kappa=20)\n",
    "x0 = np.array([1.5, 1.0])\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Run optimizers\n",
    "print(\"Running optimizers on ill-conditioned quadratic (κ=20)...\\n\")\n",
    "\n",
    "gd = GradientDescent(learning_rate=learning_rate, max_iter=200, verbose=0)\n",
    "result_gd = gd.minimize(f, grad_f, x0.copy())\n",
    "print(f\"GD:       {result_gd.n_iterations:3d} iterations, loss={result_gd.f_final:.2e}\")\n",
    "\n",
    "momentum_opt = Momentum(learning_rate=learning_rate, momentum=0.9, max_iter=200, verbose=0)\n",
    "result_mom = momentum_opt.minimize(f, grad_f, x0.copy())\n",
    "print(f\"Momentum: {result_mom.n_iterations:3d} iterations, loss={result_mom.f_final:.2e}\")\n",
    "\n",
    "adam_opt = Adam(learning_rate=learning_rate, max_iter=200, verbose=0)\n",
    "result_adam = adam_opt.minimize(f, grad_f, x0.copy())\n",
    "print(f\"Adam:     {result_adam.n_iterations:3d} iterations, loss={result_adam.f_final:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bdcce",
   "metadata": {},
   "source": [
    "**Observation**: Momentum and Adam converge faster than vanilla GD on this ill-conditioned problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597daa0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Visualization: Optimization Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c40e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Contour plot\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "y_range = np.linspace(-1.5, 1.5, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = np.zeros_like(X)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = f(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "levels = np.logspace(-2, 1, 20)\n",
    "contour = ax.contour(X, Y, Z, levels=levels, cmap='gray', alpha=0.4)\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Plot paths\n",
    "path_gd = np.array(result_gd.history['x'])\n",
    "path_mom = np.array(result_mom.history['x'])\n",
    "path_adam = np.array(result_adam.history['x'])\n",
    "\n",
    "ax.plot(path_gd[:, 0], path_gd[:, 1], 'b.-', linewidth=2, markersize=6, \n",
    "        label=f'GD ({result_gd.n_iterations} iter)', alpha=0.8)\n",
    "ax.plot(path_mom[:, 0], path_mom[:, 1], 'r.-', linewidth=2, markersize=6, \n",
    "        label=f'Momentum ({result_mom.n_iterations} iter)', alpha=0.8)\n",
    "ax.plot(path_adam[:, 0], path_adam[:, 1], 'g.-', linewidth=2, markersize=6, \n",
    "        label=f'Adam ({result_adam.n_iterations} iter)', alpha=0.8)\n",
    "\n",
    "ax.plot(x0[0], x0[1], 'ko', markersize=12, label='Start')\n",
    "ax.plot(0, 0, 'k*', markersize=15, label='Optimum')\n",
    "\n",
    "ax.set_xlabel('x₁', fontsize=13)\n",
    "ax.set_ylabel('x₂', fontsize=13)\n",
    "ax.set_title('Optimizer Comparison: Paths to Minimum (κ=20)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '02_optimizer_paths_quadratic.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {REPORTS_DIR / '02_optimizer_paths_quadratic.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909cbb4",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "1. **GD**: Zigzags due to oscillations in narrow valley\n",
    "2. **Momentum**: Smoother path, less oscillation\n",
    "3. **Adam**: Most direct path, adapts to geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1e79f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Convergence Curves: Loss Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3886fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss (log scale)\n",
    "ax1.semilogy(result_gd.history['f'], 'b-', linewidth=2, label='GD')\n",
    "ax1.semilogy(result_mom.history['f'], 'r-', linewidth=2, label='Momentum')\n",
    "ax1.semilogy(result_adam.history['f'], 'g-', linewidth=2, label='Adam')\n",
    "ax1.set_xlabel('Iteration', fontsize=12)\n",
    "ax1.set_ylabel('Loss f(x)', fontsize=12)\n",
    "ax1.set_title('Convergence Speed Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Gradient norm\n",
    "ax2.semilogy(result_gd.history['grad_norm'], 'b-', linewidth=2, label='GD')\n",
    "ax2.semilogy(result_mom.history['grad_norm'], 'r-', linewidth=2, label='Momentum')\n",
    "ax2.semilogy(result_adam.history['grad_norm'], 'g-', linewidth=2, label='Adam')\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('||∇f(x)||', fontsize=12)\n",
    "ax2.set_title('Gradient Norm Decay', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '02_optimizer_convergence.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {REPORTS_DIR / '02_optimizer_convergence.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9cf777",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Experiment 2: Logistic Regression (Real ML Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic binary classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    n_redundant=1,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
    "    return np.where(z >= 0, \n",
    "                    1 / (1 + np.exp(-z)),\n",
    "                    np.exp(z) / (1 + np.exp(z)))\n",
    "\n",
    "def logistic_loss(w, X, y, reg=0.01):\n",
    "    \"\"\"Binary cross-entropy loss with L2 regularization.\"\"\"\n",
    "    z = X @ w\n",
    "    probs = sigmoid(z)\n",
    "    # Numerical stability\n",
    "    probs = np.clip(probs, 1e-15, 1 - 1e-15)\n",
    "    loss = -np.mean(y * np.log(probs) + (1 - y) * np.log(1 - probs))\n",
    "    loss += 0.5 * reg * np.sum(w**2)  # L2 penalty\n",
    "    return loss\n",
    "\n",
    "def logistic_grad(w, X, y, reg=0.01):\n",
    "    \"\"\"Gradient of logistic loss.\"\"\"\n",
    "    z = X @ w\n",
    "    probs = sigmoid(z)\n",
    "    grad = X.T @ (probs - y) / len(y)\n",
    "    grad += reg * w  # L2 gradient\n",
    "    return grad\n",
    "\n",
    "def accuracy(w, X, y):\n",
    "    \"\"\"Classification accuracy.\"\"\"\n",
    "    probs = sigmoid(X @ w)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    return np.mean(preds == y)\n",
    "\n",
    "print(\"✓ Logistic regression functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f810400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize with each method\n",
    "w0 = np.zeros(X_train.shape[1])\n",
    "lr_logreg = 0.5\n",
    "max_iter_logreg = 300\n",
    "\n",
    "print(\"Training logistic regression with different optimizers...\\n\")\n",
    "\n",
    "results_logreg = {}\n",
    "\n",
    "# GD\n",
    "start = time.time()\n",
    "gd_log = GradientDescent(learning_rate=lr_logreg, max_iter=max_iter_logreg, tol=1e-6)\n",
    "res_gd_log = gd_log.minimize(\n",
    "    lambda w: logistic_loss(w, X_train, y_train),\n",
    "    lambda w: logistic_grad(w, X_train, y_train),\n",
    "    w0.copy()\n",
    ")\n",
    "time_gd = time.time() - start\n",
    "acc_gd_train = accuracy(res_gd_log.x_final, X_train, y_train)\n",
    "acc_gd_test = accuracy(res_gd_log.x_final, X_test, y_test)\n",
    "results_logreg['GD'] = (res_gd_log, time_gd, acc_gd_train, acc_gd_test)\n",
    "print(f\"GD:       {res_gd_log.n_iterations:3d} iter | {time_gd:.3f}s | \"\n",
    "      f\"Train: {acc_gd_train:.3f} | Test: {acc_gd_test:.3f}\")\n",
    "\n",
    "# Momentum\n",
    "start = time.time()\n",
    "mom_log = Momentum(learning_rate=lr_logreg, momentum=0.9, max_iter=max_iter_logreg, tol=1e-6)\n",
    "res_mom_log = mom_log.minimize(\n",
    "    lambda w: logistic_loss(w, X_train, y_train),\n",
    "    lambda w: logistic_grad(w, X_train, y_train),\n",
    "    w0.copy()\n",
    ")\n",
    "time_mom = time.time() - start\n",
    "acc_mom_train = accuracy(res_mom_log.x_final, X_train, y_train)\n",
    "acc_mom_test = accuracy(res_mom_log.x_final, X_test, y_test)\n",
    "results_logreg['Momentum'] = (res_mom_log, time_mom, acc_mom_train, acc_mom_test)\n",
    "print(f\"Momentum: {res_mom_log.n_iterations:3d} iter | {time_mom:.3f}s | \"\n",
    "      f\"Train: {acc_mom_train:.3f} | Test: {acc_mom_test:.3f}\")\n",
    "\n",
    "# Adam\n",
    "start = time.time()\n",
    "adam_log = Adam(learning_rate=lr_logreg*0.2, max_iter=max_iter_logreg, tol=1e-6)  # Adam needs smaller LR\n",
    "res_adam_log = adam_log.minimize(\n",
    "    lambda w: logistic_loss(w, X_train, y_train),\n",
    "    lambda w: logistic_grad(w, X_train, y_train),\n",
    "    w0.copy()\n",
    ")\n",
    "time_adam = time.time() - start\n",
    "acc_adam_train = accuracy(res_adam_log.x_final, X_train, y_train)\n",
    "acc_adam_test = accuracy(res_adam_log.x_final, X_test, y_test)\n",
    "results_logreg['Adam'] = (res_adam_log, time_adam, acc_adam_train, acc_adam_test)\n",
    "print(f\"Adam:     {res_adam_log.n_iterations:3d} iter | {time_adam:.3f}s | \"\n",
    "      f\"Train: {acc_adam_train:.3f} | Test: {acc_adam_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c2a56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Benchmark Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8058e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOGISTIC REGRESSION BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Optimizer':<12} {'Iterations':<12} {'Time (s)':<10} {'Train Acc':<10} {'Test Acc':<10} {'Final Loss':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for name, (result, time_elapsed, acc_train, acc_test) in results_logreg.items():\n",
    "    print(f\"{name:<12} {result.n_iterations:<12} {time_elapsed:<10.3f} \"\n",
    "          f\"{acc_train:<10.3f} {acc_test:<10.3f} {result.f_final:<12.4e}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc146e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot logistic regression convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for name, (result, _, _, _) in results_logreg.items():\n",
    "    ax.semilogy(result.history['f'], linewidth=2.5, label=name)\n",
    "\n",
    "ax.set_xlabel('Iteration', fontsize=13)\n",
    "ax.set_ylabel('Training Loss', fontsize=13)\n",
    "ax.set_title('Logistic Regression: Optimizer Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '02_logreg_convergence.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {REPORTS_DIR / '02_logreg_convergence.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9494c62a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Effect of Learning Rate (GD vs Momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3, 0.5]\n",
    "A, f, grad = create_ill_conditioned_quadratic(kappa=20)\n",
    "x0_lr = np.array([1.5, 1.0])\n",
    "\n",
    "iters_gd = []\n",
    "iters_mom = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # GD\n",
    "    opt_gd = GradientDescent(learning_rate=lr, max_iter=500, tol=1e-8)\n",
    "    res_gd = opt_gd.minimize(f, grad, x0_lr.copy())\n",
    "    iters_gd.append(res_gd.n_iterations if res_gd.converged else 500)\n",
    "    \n",
    "    # Momentum\n",
    "    opt_mom = Momentum(learning_rate=lr, momentum=0.9, max_iter=500, tol=1e-8)\n",
    "    res_mom = opt_mom.minimize(f, grad, x0_lr.copy())\n",
    "    iters_mom.append(res_mom.n_iterations if res_mom.converged else 500)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_pos = np.arange(len(learning_rates))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x_pos - width/2, iters_gd, width, label='GD', alpha=0.8)\n",
    "ax.bar(x_pos + width/2, iters_mom, width, label='Momentum', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Learning Rate', fontsize=13)\n",
    "ax.set_ylabel('Iterations to Converge', fontsize=13)\n",
    "ax.set_title('Learning Rate Sensitivity (κ=20)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([str(lr) for lr in learning_rates])\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '02_learning_rate_sensitivity.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {REPORTS_DIR / '02_learning_rate_sensitivity.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03152fd7",
   "metadata": {},
   "source": [
    "**Key Finding**: Momentum is less sensitive to learning rate choice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9a7f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways\n",
    "\n",
    "✅ **Momentum smooths oscillations**:\n",
    "   - Accumulates velocity in consistent directions\n",
    "   - Dampens zigzag behavior in narrow valleys\n",
    "   - Typical β = 0.9 works well in practice\n",
    "\n",
    "✅ **Adam adapts per-parameter**:\n",
    "   - Automatically scales learning rates by gradient history\n",
    "   - Robust to ill-conditioning\n",
    "   - Default choice for deep learning (unless you have reasons not to)\n",
    "\n",
    "✅ **Convergence speed**:\n",
    "   - Quadratic: Adam ≈ Momentum > GD\n",
    "   - Logistic Regression: Similar ordering\n",
    "   - Wall-clock time: All similar (per-iteration cost is low)\n",
    "\n",
    "✅ **When to use what**:\n",
    "   - **GD**: Simple, interpretable, good for well-conditioned problems\n",
    "   - **Momentum**: Standard for convex optimization, less tuning than GD\n",
    "   - **Adam**: Default for neural networks, handles varying scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870954a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Common Pitfalls\n",
    "\n",
    "❌ **Using Adam learning rate for GD**: Adam needs ~10× smaller LR than GD\n",
    "\n",
    "❌ **High momentum on noisy gradients**: Can overshoot; reduce β or use gradient clipping\n",
    "\n",
    "❌ **Not tuning β parameters in Adam**: Defaults (0.9, 0.999) work for most cases, but sometimes β₂=0.99 is better\n",
    "\n",
    "❌ **Comparing optimizers without proper LR tuning**: Each optimizer has different optimal LR ranges\n",
    "\n",
    "❌ **Forgetting bias correction in Adam**: Early iterations are biased without it (our implementation includes this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c0001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62926c8",
   "metadata": {},
   "source": [
    "**Exercise 1**: Implement Nesterov Accelerated Gradient (NAG) and compare to standard momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f96dd",
   "metadata": {},
   "source": [
    "**Exercise 2**: Test Adam with different β₁ values (0.5, 0.7, 0.9, 0.95) on the quadratic problem. Which works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4558961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae58e4",
   "metadata": {},
   "source": [
    "**Exercise 3**: Create a \"Rosenbrock function\" (Google it) and compare all three optimizers. Which handles this non-quadratic, non-convex function best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f9202",
   "metadata": {},
   "source": [
    "**Exercise 4**: Add \"learning rate warmup\" to Adam: start with small LR and linearly increase for first 100 steps. Does this help on the logistic regression task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc4b6ae",
   "metadata": {},
   "source": [
    "**Exercise 5**: Implement learning rate decay: multiply LR by 0.9 every 50 iterations. Compare final loss for GD with/without decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5969d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "*Solutions provided at end of notebook - try exercises first!*\n",
    "\n",
    "<details>\n",
    "<summary><b>Exercise 1 Solution: Nesterov Momentum</b></summary>\n",
    "\n",
    "```python\n",
    "class NesterovMomentum:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9, max_iter=1000, tol=1e-6):\n",
    "        self.lr = learning_rate\n",
    "        self.beta = momentum\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "    \n",
    "    def minimize(self, f, grad, x0):\n",
    "        x = x0.copy()\n",
    "        v = np.zeros_like(x)\n",
    "        history = {'f': [], 'x': [], 'grad_norm': []}\n",
    "        \n",
    "        for k in range(self.max_iter):\n",
    "            # NAG: evaluate gradient at \"look-ahead\" position\n",
    "            x_lookahead = x - self.beta * v\n",
    "            g = grad(x_lookahead)\n",
    "            \n",
    "            v = self.beta * v + g\n",
    "            x = x - self.lr * v\n",
    "            \n",
    "            history['f'].append(f(x))\n",
    "            history['x'].append(x.copy())\n",
    "            history['grad_norm'].append(np.linalg.norm(g))\n",
    "            \n",
    "            if np.linalg.norm(g) < self.tol:\n",
    "                break\n",
    "        \n",
    "        from modules._import_helper import safe_import_from\n",
    "        OptimizationResult = safe_import_from('01_numerical_toolbox.src.optimizers_from_scratch', \n",
    "                                               'OptimizationResult')\n",
    "        return OptimizationResult(x, f(x), history, True, k+1)\n",
    "\n",
    "# Test\n",
    "A, f, grad = create_ill_conditioned_quadratic(kappa=20)\n",
    "nag = NesterovMomentum(learning_rate=0.1, momentum=0.9)\n",
    "res_nag = nag.minimize(f, grad, np.array([1.5, 1.0]))\n",
    "print(f\"NAG: {res_nag.n_iterations} iterations\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Exercise 2 Solution: Adam β₁ sweep</b></summary>\n",
    "\n",
    "```python\n",
    "beta1_values = [0.5, 0.7, 0.9, 0.95]\n",
    "A, f, grad = create_ill_conditioned_quadratic(kappa=20)\n",
    "x0 = np.array([1.5, 1.0])\n",
    "\n",
    "for b1 in beta1_values:\n",
    "    adam = Adam(learning_rate=0.1, beta1=b1, beta2=0.999, max_iter=200)\n",
    "    res = adam.minimize(f, grad, x0.copy())\n",
    "    print(f\"β₁={b1:.2f}: {res.n_iterations} iterations, loss={res.f_final:.2e}\")\n",
    "\n",
    "# Typical result: β₁=0.9 is best for this problem\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Exercise 3 Solution: Rosenbrock function</b></summary>\n",
    "\n",
    "```python\n",
    "def rosenbrock(x, a=1, b=100):\n",
    "    return (a - x[0])**2 + b * (x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock_grad(x, a=1, b=100):\n",
    "    dx0 = -2*(a - x[0]) - 4*b*x[0]*(x[1] - x[0]**2)\n",
    "    dx1 = 2*b*(x[1] - x[0]**2)\n",
    "    return np.array([dx0, dx1])\n",
    "\n",
    "x0_ros = np.array([-1.0, 2.0])\n",
    "lr = 0.001\n",
    "\n",
    "gd = GradientDescent(learning_rate=lr, max_iter=5000)\n",
    "res_gd_ros = gd.minimize(rosenbrock, rosenbrock_grad, x0_ros.copy())\n",
    "print(f\"GD: {res_gd_ros.n_iterations} iter, final={res_gd_ros.x_final}\")\n",
    "\n",
    "mom = Momentum(learning_rate=lr, momentum=0.9, max_iter=5000)\n",
    "res_mom_ros = mom.minimize(rosenbrock, rosenbrock_grad, x0_ros.copy())\n",
    "print(f\"Momentum: {res_mom_ros.n_iterations} iter, final={res_mom_ros.x_final}\")\n",
    "\n",
    "adam = Adam(learning_rate=lr*10, max_iter=5000)  # Adam can use higher LR\n",
    "res_adam_ros = adam.minimize(rosenbrock, rosenbrock_grad, x0_ros.copy())\n",
    "print(f\"Adam: {res_adam_ros.n_iterations} iter, final={res_adam_ros.x_final}\")\n",
    "\n",
    "# Optimum is at [1, 1]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea3bc2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary\n",
    "report = f\"\"\"\n",
    "GRADIENT DESCENT DYNAMICS: OPTIMIZER COMPARISON\n",
    "{'='*70}\n",
    "\n",
    "QUADRATIC BOWL (κ=20):\n",
    "  GD:       {result_gd.n_iterations} iterations\n",
    "  Momentum: {result_mom.n_iterations} iterations ({100*(1 - result_mom.n_iterations/result_gd.n_iterations):.0f}% faster)\n",
    "  Adam:     {result_adam.n_iterations} iterations ({100*(1 - result_adam.n_iterations/result_gd.n_iterations):.0f}% faster)\n",
    "\n",
    "LOGISTIC REGRESSION:\n",
    "\"\"\"  \n",
    "\n",
    "for name, (res, time_elapsed, acc_train, acc_test) in results_logreg.items():\n",
    "    report += f\"\\n  {name:<10}: {res.n_iterations:3d} iter | {time_elapsed:.3f}s | Test Acc: {acc_test:.3f}\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "KEY INSIGHTS:\n",
    "  1. Momentum reduces iterations by ~{100*(1 - result_mom.n_iterations/result_gd.n_iterations):.0f}% on ill-conditioned problems\n",
    "  2. Adam adapts automatically to problem geometry\n",
    "  3. All methods achieve similar final accuracy on logistic regression\n",
    "  4. Momentum is more robust to learning rate choice than GD\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "  - Use GD for well-conditioned, simple problems\n",
    "  - Use Momentum (β=0.9) for convex optimization\n",
    "  - Use Adam as default for deep learning\n",
    "  - Always tune learning rate per optimizer!\n",
    "\n",
    "Plots saved in: {REPORTS_DIR}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "with open(REPORTS_DIR / '02_optimizer_comparison_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\n✓ Report saved: {REPORTS_DIR / '02_optimizer_comparison_report.txt'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
