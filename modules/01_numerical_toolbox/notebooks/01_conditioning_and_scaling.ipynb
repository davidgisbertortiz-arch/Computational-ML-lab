{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add module to path\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent))\n",
    "\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "# Import from repo standards\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core.seeding', 'set_seed')\n",
    "\n",
    "# Import from this module\n",
    "GradientDescent = safe_import_from('01_numerical_toolbox.src.optimizers_from_scratch', 'GradientDescent')\n",
    "condition_number = safe_import_from('01_numerical_toolbox.src.linear_algebra', 'condition_number')\n",
    "\n",
    "# Setup\n",
    "set_seed(42)\n",
    "REPORTS_DIR = Path('../reports')\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34309585",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition: What is Conditioning?\n",
    "\n",
    "**Key Ideas:**\n",
    "- The **condition number** κ(A) measures how \"stretched\" or \"squashed\" a matrix A is\n",
    "- For optimization, it determines the *shape* of the loss landscape\n",
    "- **Well-conditioned** (κ ≈ 1): Loss surface is spherical → easy optimization\n",
    "- **Ill-conditioned** (κ ≫ 1): Loss surface is elongated (like a valley) → slow optimization\n",
    "- **Definition**: κ(A) = σ_max(A) / σ_min(A) = largest eigenvalue / smallest eigenvalue\n",
    "\n",
    "**Why it matters**: Gradient descent takes *tiny* steps along narrow valleys but *overshoots* along wide directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f941a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Minimal Math: Quadratic Loss Landscape\n",
    "\n",
    "Consider a quadratic loss function (like in linear regression):\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^T A \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "The gradient is:\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}\n",
    "$$\n",
    "\n",
    "**Key insight**: The Hessian (second derivative) is $\\nabla^2 f = A$. The condition number $\\kappa(A)$ determines:\n",
    "- **Convergence rate**: $\\sim \\left(1 - \\frac{1}{\\kappa}\\right)^k$ after $k$ iterations\n",
    "- **Optimal step size**: $\\alpha_{\\text{max}} \\approx \\frac{2}{\\lambda_{\\text{max}} + \\lambda_{\\text{min}}}$\n",
    "\n",
    "**Example**: If κ = 100, gradient descent needs ~100× more iterations than if κ = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97512f20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Implementation: Quadratic Bowls with Different Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ea5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quadratic(kappa: float, dim: int = 2) -> tuple:\n",
    "    \"\"\"\n",
    "    Create a quadratic function f(x) = 0.5 * x^T A x with condition number kappa.\n",
    "    \n",
    "    Returns:\n",
    "        A: Symmetric positive definite matrix\n",
    "        f: Objective function\n",
    "        grad_f: Gradient function\n",
    "    \"\"\"\n",
    "    # Create eigenvalues with desired condition number\n",
    "    eigenvalues = np.linspace(1, kappa, dim)\n",
    "    \n",
    "    # Random rotation (orthogonal matrix)\n",
    "    Q, _ = np.linalg.qr(np.random.randn(dim, dim))\n",
    "    \n",
    "    # A = Q * diag(λ) * Q^T\n",
    "    A = Q @ np.diag(eigenvalues) @ Q.T\n",
    "    \n",
    "    # Define objective and gradient\n",
    "    def f(x):\n",
    "        return 0.5 * x @ A @ x\n",
    "    \n",
    "    def grad_f(x):\n",
    "        return A @ x\n",
    "    \n",
    "    return A, f, grad_f\n",
    "\n",
    "print(\"Example: Creating quadratic with κ = 10\")\n",
    "A, f, grad = create_quadratic(kappa=10.0)\n",
    "print(f\"Condition number: {condition_number(A):.2f}\")\n",
    "print(f\"Eigenvalues: {np.linalg.eigvalsh(A)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e099b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Experiment: Comparing Convergence Across Condition Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different condition numbers\n",
    "kappas = [1, 5, 20, 100]\n",
    "x0 = np.array([1.0, 1.0])\n",
    "learning_rate = 0.1\n",
    "\n",
    "results = {}\n",
    "\n",
    "for kappa in kappas:\n",
    "    A, f, grad_f = create_quadratic(kappa, dim=2)\n",
    "    \n",
    "    # Adjust learning rate for stability\n",
    "    max_eigenvalue = np.max(np.linalg.eigvalsh(A))\n",
    "    safe_lr = 1.9 / max_eigenvalue  # Just below 2/λ_max\n",
    "    \n",
    "    optimizer = GradientDescent(\n",
    "        learning_rate=safe_lr,\n",
    "        max_iter=500,\n",
    "        tol=1e-8\n",
    "    )\n",
    "    \n",
    "    result = optimizer.minimize(f, grad_f, x0.copy())\n",
    "    results[kappa] = result\n",
    "    \n",
    "    print(f\"κ = {kappa:3d} | Iterations: {result.n_iterations:3d} | \"\n",
    "          f\"Final loss: {result.f_final:.2e} | LR: {safe_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b1ba8",
   "metadata": {},
   "source": [
    "**Observation**: Notice how the number of iterations scales roughly with κ!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf50c7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visualization: Loss Contours and Optimization Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3266f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, kappa in enumerate(kappas):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Recreate problem\n",
    "    A, f, grad_f = create_quadratic(kappa, dim=2)\n",
    "    \n",
    "    # Create contour plot\n",
    "    x_range = np.linspace(-1.5, 1.5, 100)\n",
    "    y_range = np.linspace(-1.5, 1.5, 100)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = np.zeros_like(X)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = f(np.array([X[i, j], Y[i, j]]))\n",
    "    \n",
    "    # Plot contours\n",
    "    levels = np.logspace(-2, 1, 20)\n",
    "    contour = ax.contour(X, Y, Z, levels=levels, cmap='viridis', alpha=0.6)\n",
    "    ax.clabel(contour, inline=True, fontsize=8, fmt='%.2f')\n",
    "    \n",
    "    # Plot optimization path\n",
    "    path = np.array(results[kappa].history['x'])\n",
    "    ax.plot(path[:, 0], path[:, 1], 'r.-', linewidth=2, markersize=8, \n",
    "            label='GD path', alpha=0.8)\n",
    "    ax.plot(path[0, 0], path[0, 1], 'go', markersize=12, label='Start')\n",
    "    ax.plot(path[-1, 0], path[-1, 1], 'r*', markersize=15, label='End')\n",
    "    \n",
    "    ax.set_xlabel('x₁', fontsize=12)\n",
    "    ax.set_ylabel('x₂', fontsize=12)\n",
    "    ax.set_title(f'κ = {kappa} | {results[kappa].n_iterations} iterations', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '01_conditioning_paths.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {REPORTS_DIR / '01_conditioning_paths.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65b7d02",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "1. **κ = 1**: Circular contours → direct path to minimum\n",
    "2. **κ = 100**: Elongated ellipses → zigzag path in narrow valley\n",
    "3. Path shows \"bouncing\" behavior in ill-conditioned problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd467d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Convergence Curves: Loss and Gradient Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss vs iterations\n",
    "for kappa in kappas:\n",
    "    losses = results[kappa].history['f']\n",
    "    ax1.semilogy(losses, linewidth=2, label=f'κ = {kappa}')\n",
    "\n",
    "ax1.set_xlabel('Iteration', fontsize=12)\n",
    "ax1.set_ylabel('Loss f(x)', fontsize=12)\n",
    "ax1.set_title('Convergence Speed vs Condition Number', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Gradient norm vs iterations\n",
    "for kappa in kappas:\n",
    "    grad_norms = results[kappa].history['grad_norm']\n",
    "    ax2.semilogy(grad_norms, linewidth=2, label=f'κ = {kappa}')\n",
    "\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('||∇f(x)||', fontsize=12)\n",
    "ax2.set_title('Gradient Norm Decay', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / '01_conditioning_convergence.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {REPORTS_DIR / '01_conditioning_convergence.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87181b",
   "metadata": {},
   "source": [
    "**Analysis**: The linear decay rate in log-scale confirms the theoretical prediction:\n",
    "- Slope ∝ 1 - 1/κ\n",
    "- Higher κ → slower decay → more iterations needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4871b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Feature Scaling Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ac87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create poorly scaled data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X_unscaled = np.random.randn(n_samples, 2)\n",
    "X_unscaled[:, 0] *= 100  # First feature has huge scale\n",
    "X_unscaled[:, 1] *= 0.1   # Second feature is tiny\n",
    "\n",
    "print(\"Unscaled data:\")\n",
    "print(f\"  Feature 1: std = {X_unscaled[:, 0].std():.2f}\")\n",
    "print(f\"  Feature 2: std = {X_unscaled[:, 1].std():.2f}\")\n",
    "print(f\"  Scale ratio: {X_unscaled[:, 0].std() / X_unscaled[:, 1].std():.0f}:1\")\n",
    "\n",
    "# Compute condition numbers\n",
    "A_unscaled = X_unscaled.T @ X_unscaled / n_samples\n",
    "kappa_unscaled = condition_number(A_unscaled)\n",
    "\n",
    "# Scale features (standardization)\n",
    "X_scaled = (X_unscaled - X_unscaled.mean(axis=0)) / X_unscaled.std(axis=0)\n",
    "A_scaled = X_scaled.T @ X_scaled / n_samples\n",
    "kappa_scaled = condition_number(A_scaled)\n",
    "\n",
    "print(f\"\\nCondition numbers:\")\n",
    "print(f\"  Unscaled: κ = {kappa_unscaled:.1f}\")\n",
    "print(f\"  Scaled:   κ = {kappa_scaled:.1f}\")\n",
    "print(f\"  Improvement: {kappa_unscaled/kappa_scaled:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7bc317",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "✅ **Condition number** κ determines optimization difficulty:\n",
    "   - κ = 1: spherical loss → fast convergence\n",
    "   - κ ≫ 1: elongated loss → slow, zigzag path\n",
    "\n",
    "✅ **Feature scaling** is essential:\n",
    "   - Standardization (mean=0, std=1) often sufficient\n",
    "   - Reduces κ → speeds up gradient descent\n",
    "\n",
    "✅ **Learning rate** must be tuned to problem scale:\n",
    "   - Safe choice: α < 2/λ_max (where λ_max is largest eigenvalue)\n",
    "   - Ill-conditioned problems need smaller α\n",
    "\n",
    "✅ **Real ML impact**:\n",
    "   - Neural networks: BatchNorm reduces internal covariate shift\n",
    "   - Linear models: Always scale features before training\n",
    "   - Regularization (Ridge, L2) improves conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13220d84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Common Pitfalls\n",
    "\n",
    "❌ **Using same learning rate across problems**: α=0.01 might work for κ=10 but diverge for κ=1000\n",
    "\n",
    "❌ **Forgetting to scale features**: Mixing features with different units (e.g., meters vs millimeters) creates high κ\n",
    "\n",
    "❌ **Ignoring numerical precision**: Very high κ (>10⁸) hits floating-point limits\n",
    "\n",
    "❌ **Confusing conditioning with convexity**: Well-conditioned doesn't mean globally convex (non-convex problems can still have local ill-conditioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7617594",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Exercises\n",
    "\n",
    "Try these on your own! Solutions are at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c28901",
   "metadata": {},
   "source": [
    "**Exercise 1**: Create a 3D quadratic with κ = 50 and optimize it with GD. Plot the loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8dbf5b",
   "metadata": {},
   "source": [
    "**Exercise 2**: What happens if you use learning rate α = 2.5/λ_max (above the stability limit)? Try it with κ = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916bd4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853c631",
   "metadata": {},
   "source": [
    "**Exercise 3**: Generate random data with 5 features where one feature has 100× larger scale than others. Compute κ before and after standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d794f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b0179",
   "metadata": {},
   "source": [
    "**Exercise 4**: Theoretical - Prove that for κ = 1 (identity matrix), gradient descent converges in 1 step with α = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37feeffa",
   "metadata": {},
   "source": [
    "*Your answer here (math or code verification)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7f2ff",
   "metadata": {},
   "source": [
    "**Exercise 5**: Implement a simple line search that adapts learning rate automatically. Compare to fixed α on a κ=50 problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d4e84e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions (Expand to view)\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to show Exercise 1 solution</b></summary>\n",
    "\n",
    "```python\n",
    "A_3d, f_3d, grad_3d = create_quadratic(kappa=50, dim=3)\n",
    "x0_3d = np.array([1.0, 1.0, 1.0])\n",
    "max_eig = np.max(np.linalg.eigvalsh(A_3d))\n",
    "opt = GradientDescent(learning_rate=1.8/max_eig, max_iter=1000)\n",
    "result = opt.minimize(f_3d, grad_3d, x0_3d)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.semilogy(result.history['f'], linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('3D Quadratic with κ=50')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "print(f\"Converged in {result.n_iterations} iterations\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to show Exercise 2 solution</b></summary>\n",
    "\n",
    "```python\n",
    "A, f, grad = create_quadratic(kappa=10, dim=2)\n",
    "max_eig = np.max(np.linalg.eigvalsh(A))\n",
    "unstable_lr = 2.5 / max_eig  # Above stability limit!\n",
    "\n",
    "opt = GradientDescent(learning_rate=unstable_lr, max_iter=100)\n",
    "result = opt.minimize(f, grad, np.array([1.0, 1.0]))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(result.history['f'], linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Unstable: α too large → divergence!')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "print(\"Notice: Loss increases (diverges) instead of decreasing!\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to show Exercise 3 solution</b></summary>\n",
    "\n",
    "```python\n",
    "np.random.seed(123)\n",
    "X = np.random.randn(50, 5)\n",
    "X[:, 0] *= 100  # Scale first feature\n",
    "\n",
    "A_before = X.T @ X / X.shape[0]\n",
    "kappa_before = condition_number(A_before)\n",
    "\n",
    "X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "A_after = X_standardized.T @ X_standardized / X.shape[0]\n",
    "kappa_after = condition_number(A_after)\n",
    "\n",
    "print(f\"Before standardization: κ = {kappa_before:.1f}\")\n",
    "print(f\"After standardization:  κ = {kappa_after:.1f}\")\n",
    "print(f\"Improvement: {kappa_before/kappa_after:.0f}x\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to show Exercise 4 solution</b></summary>\n",
    "\n",
    "**Proof**: For $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T I \\mathbf{x}$ (identity matrix), we have:\n",
    "- Gradient: $\\nabla f = I \\mathbf{x} = \\mathbf{x}$\n",
    "- GD update: $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{x}_k = (1-\\alpha)\\mathbf{x}_k$\n",
    "- With $\\alpha=1$: $\\mathbf{x}_1 = (1-1)\\mathbf{x}_0 = \\mathbf{0}$ ✓\n",
    "\n",
    "Verification:\n",
    "```python\n",
    "A_identity = np.eye(2)\n",
    "f_id = lambda x: 0.5 * x @ A_identity @ x\n",
    "grad_id = lambda x: A_identity @ x\n",
    "opt = GradientDescent(learning_rate=1.0, max_iter=10)\n",
    "result = opt.minimize(f_id, grad_id, np.array([1.0, 2.0]))\n",
    "print(f\"Iterations: {result.n_iterations}\")  # Should be 1\n",
    "print(f\"Final x: {result.x_final}\")  # Should be ~[0, 0]\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to show Exercise 5 solution</b></summary>\n",
    "\n",
    "```python\n",
    "def gd_with_backtracking(f, grad, x0, max_iter=500, c=0.5, rho=0.8):\n",
    "    \"\"\"GD with backtracking line search (Armijo rule).\"\"\"\n",
    "    x = x0.copy()\n",
    "    losses = []\n",
    "    alpha = 1.0  # Initial step size\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        g = grad(x)\n",
    "        losses.append(f(x))\n",
    "        \n",
    "        if np.linalg.norm(g) < 1e-8:\n",
    "            break\n",
    "        \n",
    "        # Backtracking line search\n",
    "        alpha_k = alpha\n",
    "        while f(x - alpha_k * g) > f(x) - c * alpha_k * (g @ g):\n",
    "            alpha_k *= rho\n",
    "        \n",
    "        x = x - alpha_k * g\n",
    "    \n",
    "    return x, losses\n",
    "\n",
    "# Compare\n",
    "A, f, grad = create_quadratic(kappa=50, dim=2)\n",
    "x0 = np.array([1.0, 1.0])\n",
    "\n",
    "# Fixed LR\n",
    "opt_fixed = GradientDescent(learning_rate=0.01, max_iter=500)\n",
    "res_fixed = opt_fixed.minimize(f, grad, x0.copy())\n",
    "\n",
    "# Adaptive LR\n",
    "x_adapt, losses_adapt = gd_with_backtracking(f, grad, x0.copy())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.semilogy(res_fixed.history['f'], label='Fixed α=0.01', linewidth=2)\n",
    "plt.semilogy(losses_adapt, label='Backtracking', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Fixed vs Adaptive Learning Rate (κ=50)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "print(f\"Fixed: {len(res_fixed.history['f'])} iterations\")\n",
    "print(f\"Adaptive: {len(losses_adapt)} iterations\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17dbe6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text report\n",
    "report_text = f\"\"\"\n",
    "CONDITIONING AND SCALING ANALYSIS\n",
    "{'='*60}\n",
    "\n",
    "Condition Numbers Tested: {kappas}\n",
    "\n",
    "Convergence Summary:\n",
    "\"\"\"\n",
    "\n",
    "for kappa in kappas:\n",
    "    res = results[kappa]\n",
    "    report_text += f\"\\n  κ = {kappa:3d} → {res.n_iterations:3d} iterations, \"\n",
    "    report_text += f\"final loss = {res.f_final:.2e}\"\n",
    "\n",
    "report_text += f\"\"\"\n",
    "\n",
    "Key Finding:\n",
    "  - Iteration count scales approximately linearly with κ\n",
    "  - Well-conditioned problems (κ≈1) converge in ~10 iterations\n",
    "  - Ill-conditioned problems (κ=100) need ~{results[100].n_iterations} iterations\n",
    "\n",
    "Practical Advice:\n",
    "  1. Always standardize features before training\n",
    "  2. Monitor condition number of data covariance X^T X\n",
    "  3. Use adaptive optimizers (Adam) for problems with unknown κ\n",
    "  4. Consider regularization to improve conditioning\n",
    "\n",
    "Plots saved:\n",
    "  - {REPORTS_DIR / '01_conditioning_paths.png'}\n",
    "  - {REPORTS_DIR / '01_conditioning_convergence.png'}\n",
    "\"\"\"\n",
    "\n",
    "print(report_text)\n",
    "\n",
    "# Save report\n",
    "with open(REPORTS_DIR / '01_conditioning_report.txt', 'w') as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(f\"\\n✓ Report saved: {REPORTS_DIR / '01_conditioning_report.txt'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
