{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73980825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from modules._import_helper import safe_import_from\n",
    "\n",
    "set_seed = safe_import_from('00_repo_standards.src.mlphys_core', 'set_seed')\n",
    "ParticleFilter = safe_import_from(\n",
    "    '04_time_series_state_space.src.particle_filter',\n",
    "    'ParticleFilter'\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "plt.style.use('default')\n",
    "\n",
    "output_dir = Path('modules/04_time_series_state_space/reports/nb05_neural_decoding')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59d0b3",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Neural Data\n",
    "\n",
    "Simulate latent state + Poisson spike observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_neural_data(n_steps=200, Q=0.1, C=1.0, baseline_rate=5.0, \n",
    "                          bin_size_ms=50, seed=42):\n",
    "    \"\"\"\n",
    "    Simulate latent state + Poisson spike counts.\n",
    "    \n",
    "    Args:\n",
    "        n_steps: Number of time bins\n",
    "        Q: Process noise variance (latent state diffusion)\n",
    "        C: Encoding gain (state -> firing rate)\n",
    "        baseline_rate: Mean firing rate when x=0 (Hz)\n",
    "        bin_size_ms: Time bin size in milliseconds\n",
    "        seed: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        x_true: True latent states (n_steps,)\n",
    "        y_obs: Observed spike counts (n_steps,)\n",
    "        lambda_true: True firing rates (n_steps,)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Convert baseline rate to expected counts per bin\n",
    "    # baseline_rate is in Hz, bin_size_ms in ms\n",
    "    b = np.log(baseline_rate * bin_size_ms / 1000)  # log of expected counts at x=0\n",
    "    \n",
    "    # Generate latent state (random walk)\n",
    "    x_true = np.zeros(n_steps)\n",
    "    x_true[0] = 0.0  # Start at baseline\n",
    "    \n",
    "    for t in range(1, n_steps):\n",
    "        x_true[t] = x_true[t-1] + rng.normal(0, np.sqrt(Q))\n",
    "    \n",
    "    # Generate firing rates and spike counts\n",
    "    lambda_true = np.exp(C * x_true + b)  # Always positive!\n",
    "    y_obs = rng.poisson(lambda_true)  # Spike counts\n",
    "    \n",
    "    return x_true, y_obs, lambda_true, b\n",
    "\n",
    "# Generate data\n",
    "n_steps = 200\n",
    "Q = 0.05  # Process noise\n",
    "C = 0.8   # Encoding gain\n",
    "baseline_rate = 10.0  # 10 Hz baseline\n",
    "bin_size_ms = 50  # 50 ms bins\n",
    "\n",
    "x_true, y_obs, lambda_true, b = simulate_neural_data(\n",
    "    n_steps=n_steps, Q=Q, C=C, baseline_rate=baseline_rate, \n",
    "    bin_size_ms=bin_size_ms, seed=42\n",
    ")\n",
    "\n",
    "print(f\"Generated {n_steps} time bins ({n_steps * bin_size_ms / 1000:.1f} seconds)\")\n",
    "print(f\"Latent state range: [{x_true.min():.2f}, {x_true.max():.2f}]\")\n",
    "print(f\"Spike count range: [{y_obs.min()}, {y_obs.max()}]\")\n",
    "print(f\"Mean firing rate: {lambda_true.mean():.2f} spikes/bin ({lambda_true.mean() * 1000 / bin_size_ms:.1f} Hz)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568237d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synthetic data\n",
    "t = np.arange(n_steps) * bin_size_ms / 1000  # Time in seconds\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Latent state\n",
    "axes[0].plot(t, x_true, 'b-', linewidth=2, label='True latent state $x_t$')\n",
    "axes[0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_ylabel('Latent State', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_title('Neural State-Space Model: Latent State â†’ Firing Rate â†’ Spikes', fontsize=13)\n",
    "\n",
    "# Firing rate\n",
    "axes[1].plot(t, lambda_true, 'orange', linewidth=2, label=r'Firing rate $\\lambda_t = \\exp(Cx_t + b)$')\n",
    "axes[1].axhline(np.exp(b), color='gray', linestyle='--', alpha=0.5, label='Baseline rate')\n",
    "axes[1].set_ylabel('Firing Rate\\n(spikes/bin)', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Spike counts (as raster-like)\n",
    "axes[2].bar(t, y_obs, width=bin_size_ms/1000*0.8, color='green', alpha=0.7, label='Observed spike counts')\n",
    "axes[2].set_xlabel('Time (s)', fontsize=12)\n",
    "axes[2].set_ylabel('Spike Count', fontsize=12)\n",
    "axes[2].legend(fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'neural_data_simulation.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice: When x_t â†‘ â†’ Î»_t â†‘ â†’ more spikes observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd9534",
   "metadata": {},
   "source": [
    "## 4. Poisson Observation Likelihood\n",
    "\n",
    "For decoding, we need $P(y_t | x_t)$:\n",
    "\n",
    "$$P(y_t | x_t) = \\frac{\\lambda_t^{y_t} e^{-\\lambda_t}}{y_t!}, \\quad \\lambda_t = \\exp(C x_t + b)$$\n",
    "\n",
    "In log form (numerically stable):\n",
    "$$\\log P(y_t | x_t) = y_t \\log(\\lambda_t) - \\lambda_t - \\log(y_t!)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cbc98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gammaln  # log(n!) = gammaln(n+1)\n",
    "\n",
    "def poisson_log_likelihood(y, x, C, b):\n",
    "    \"\"\"\n",
    "    Log-likelihood of Poisson observation.\n",
    "    \n",
    "    P(y | x) = Î»^y * exp(-Î») / y!\n",
    "    log P(y | x) = y * log(Î») - Î» - log(y!)\n",
    "    \n",
    "    where Î» = exp(C*x + b)\n",
    "    \"\"\"\n",
    "    log_lambda = C * x + b\n",
    "    lambda_val = np.exp(log_lambda)\n",
    "    \n",
    "    # log P(y|x) = y * log(Î») - Î» - log(y!)\n",
    "    log_lik = y * log_lambda - lambda_val - gammaln(y + 1)\n",
    "    return log_lik\n",
    "\n",
    "# Visualize likelihood for different spike counts\n",
    "x_range = np.linspace(-2, 2, 200)\n",
    "spike_counts = [0, 1, 3, 5]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for y in spike_counts:\n",
    "    log_lik = poisson_log_likelihood(y, x_range, C, b)\n",
    "    # Convert to probability (normalized for visualization)\n",
    "    prob = np.exp(log_lik - log_lik.max())\n",
    "    ax.plot(x_range, prob, linewidth=2, label=f'y = {y} spikes')\n",
    "\n",
    "ax.set_xlabel('Latent State $x$', fontsize=12)\n",
    "ax.set_ylabel('$P(y | x)$ (normalized)', fontsize=12)\n",
    "ax.set_title('Poisson Likelihood: More Spikes â†’ Higher Estimated State', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'poisson_likelihood.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Observing more spikes shifts our belief about x_t to higher values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86627b32",
   "metadata": {},
   "source": [
    "## 5. Particle Filter for Neural Decoding\n",
    "\n",
    "Since Poisson is non-Gaussian, we use **particle filter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralParticleFilter:\n",
    "    \"\"\"\n",
    "    Particle filter for Poisson-observation state-space model.\n",
    "    \n",
    "    State model: x_t = x_{t-1} + w_t, w_t ~ N(0, Q)\n",
    "    Observation: y_t ~ Poisson(exp(C*x_t + b))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_particles, Q, C, b, seed=42):\n",
    "        self.n_particles = n_particles\n",
    "        self.Q = Q\n",
    "        self.C = C\n",
    "        self.b = b\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        \n",
    "    def run(self, observations):\n",
    "        \"\"\"\n",
    "        Run particle filter on spike count observations.\n",
    "        \n",
    "        Returns:\n",
    "            x_est: Estimated states (posterior mean)\n",
    "            x_particles_history: All particles over time\n",
    "            weights_history: Particle weights over time\n",
    "            ess_history: Effective sample size over time\n",
    "        \"\"\"\n",
    "        n_steps = len(observations)\n",
    "        \n",
    "        # Initialize particles\n",
    "        particles = self.rng.normal(0, 1, self.n_particles)\n",
    "        weights = np.ones(self.n_particles) / self.n_particles\n",
    "        \n",
    "        # Storage\n",
    "        x_est = np.zeros(n_steps)\n",
    "        x_particles_history = np.zeros((n_steps, self.n_particles))\n",
    "        weights_history = np.zeros((n_steps, self.n_particles))\n",
    "        ess_history = np.zeros(n_steps)\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            # 1. Predict: propagate particles through state model\n",
    "            particles = particles + self.rng.normal(0, np.sqrt(self.Q), self.n_particles)\n",
    "            \n",
    "            # 2. Update: weight by Poisson likelihood\n",
    "            y_t = observations[t]\n",
    "            log_weights = poisson_log_likelihood(y_t, particles, self.C, self.b)\n",
    "            \n",
    "            # Normalize weights (in log space for stability)\n",
    "            log_weights = log_weights - np.max(log_weights)  # Prevent overflow\n",
    "            weights = np.exp(log_weights)\n",
    "            weights = weights / weights.sum()\n",
    "            \n",
    "            # 3. Estimate: weighted mean\n",
    "            x_est[t] = np.sum(weights * particles)\n",
    "            \n",
    "            # Store for analysis\n",
    "            x_particles_history[t] = particles\n",
    "            weights_history[t] = weights\n",
    "            \n",
    "            # 4. Compute effective sample size\n",
    "            ess = 1.0 / np.sum(weights**2)\n",
    "            ess_history[t] = ess\n",
    "            \n",
    "            # 5. Resample if ESS too low\n",
    "            if ess < self.n_particles / 2:\n",
    "                indices = self.rng.choice(\n",
    "                    self.n_particles, size=self.n_particles, \n",
    "                    replace=True, p=weights\n",
    "                )\n",
    "                particles = particles[indices]\n",
    "                weights = np.ones(self.n_particles) / self.n_particles\n",
    "        \n",
    "        return x_est, x_particles_history, weights_history, ess_history\n",
    "\n",
    "# Run particle filter\n",
    "n_particles = 500\n",
    "pf = NeuralParticleFilter(n_particles=n_particles, Q=Q, C=C, b=b, seed=42)\n",
    "x_est, particles_hist, weights_hist, ess_hist = pf.run(y_obs)\n",
    "\n",
    "print(f\"Particle filter with {n_particles} particles\")\n",
    "print(f\"Mean ESS: {ess_hist.mean():.1f} ({ess_hist.mean()/n_particles*100:.1f}% of particles)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decoding results\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Decoded vs True\n",
    "axes[0].plot(t, x_true, 'b-', linewidth=2, label='True latent state')\n",
    "axes[0].plot(t, x_est, 'r--', linewidth=2, label='Decoded (PF estimate)')\n",
    "axes[0].fill_between(t, x_est - 0.5, x_est + 0.5, color='red', alpha=0.2)\n",
    "axes[0].set_ylabel('Latent State', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_title('Neural Decoding: Particle Filter on Poisson Observations', fontsize=13)\n",
    "\n",
    "# Spike observations\n",
    "axes[1].bar(t, y_obs, width=bin_size_ms/1000*0.8, color='green', alpha=0.7)\n",
    "axes[1].set_ylabel('Spike Count', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# ESS\n",
    "axes[2].plot(t, ess_hist, 'purple', linewidth=2)\n",
    "axes[2].axhline(n_particles/2, color='red', linestyle='--', label='Resample threshold')\n",
    "axes[2].set_xlabel('Time (s)', fontsize=12)\n",
    "axes[2].set_ylabel('Effective Sample Size', fontsize=12)\n",
    "axes[2].legend(fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'decoding_results.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compute error\n",
    "rmse = np.sqrt(np.mean((x_true - x_est)**2))\n",
    "corr = np.corrcoef(x_true, x_est)[0, 1]\n",
    "print(f\"\\nðŸ“Š Decoding Performance:\")\n",
    "print(f\"   RMSE: {rmse:.4f}\")\n",
    "print(f\"   Correlation: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd70494",
   "metadata": {},
   "source": [
    "## 6. Binning Trade-offs\n",
    "\n",
    "**Key question:** How does bin size affect decoding?\n",
    "\n",
    "- **Small bins** (e.g., 10 ms): High temporal resolution, but mostly 0-1 spikes (low SNR)\n",
    "- **Large bins** (e.g., 200 ms): More spikes per bin (better SNR), but lose temporal detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a23721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decoding_experiment(bin_size_ms, baseline_rate=10.0, n_seconds=10, seed=42):\n",
    "    \"\"\"Run complete simulation + decoding for given bin size.\"\"\"\n",
    "    n_steps = int(n_seconds * 1000 / bin_size_ms)\n",
    "    \n",
    "    # Simulate\n",
    "    x_true, y_obs, lambda_true, b = simulate_neural_data(\n",
    "        n_steps=n_steps, Q=0.05, C=0.8, \n",
    "        baseline_rate=baseline_rate, bin_size_ms=bin_size_ms, seed=seed\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    pf = NeuralParticleFilter(n_particles=300, Q=0.05, C=0.8, b=b, seed=seed+1)\n",
    "    x_est, _, _, _ = pf.run(y_obs)\n",
    "    \n",
    "    # Metrics\n",
    "    rmse = np.sqrt(np.mean((x_true - x_est)**2))\n",
    "    corr = np.corrcoef(x_true, x_est)[0, 1]\n",
    "    mean_spikes = y_obs.mean()\n",
    "    \n",
    "    return {\n",
    "        'bin_size_ms': bin_size_ms,\n",
    "        'rmse': rmse,\n",
    "        'correlation': corr,\n",
    "        'mean_spikes_per_bin': mean_spikes,\n",
    "        'n_bins': n_steps,\n",
    "    }\n",
    "\n",
    "# Test different bin sizes\n",
    "bin_sizes = [10, 20, 50, 100, 200]\n",
    "results_binning = []\n",
    "\n",
    "print(\"Running binning experiment...\")\n",
    "for bs in bin_sizes:\n",
    "    result = run_decoding_experiment(bs, seed=42)\n",
    "    results_binning.append(result)\n",
    "    print(f\"  Bin {bs:3d} ms: RMSE={result['rmse']:.4f}, r={result['correlation']:.3f}, \"\n",
    "          f\"mean spikes={result['mean_spikes_per_bin']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize binning trade-off\n",
    "import pandas as pd\n",
    "\n",
    "df_bin = pd.DataFrame(results_binning)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RMSE vs bin size\n",
    "axes[0].plot(df_bin['bin_size_ms'], df_bin['rmse'], 'bo-', linewidth=2, markersize=10)\n",
    "axes[0].set_xlabel('Bin Size (ms)', fontsize=12)\n",
    "axes[0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0].set_title('Decoding Error vs Bin Size', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation vs bin size\n",
    "axes[1].plot(df_bin['bin_size_ms'], df_bin['correlation'], 'go-', linewidth=2, markersize=10)\n",
    "axes[1].set_xlabel('Bin Size (ms)', fontsize=12)\n",
    "axes[1].set_ylabel('Correlation', fontsize=12)\n",
    "axes[1].set_title('Decoding Correlation vs Bin Size', fontsize=13)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean spikes per bin\n",
    "axes[2].bar(df_bin['bin_size_ms'], df_bin['mean_spikes_per_bin'], color='orange', alpha=0.7)\n",
    "axes[2].set_xlabel('Bin Size (ms)', fontsize=12)\n",
    "axes[2].set_ylabel('Mean Spikes/Bin', fontsize=12)\n",
    "axes[2].set_title('Signal Strength vs Bin Size', fontsize=13)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'binning_tradeoff.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Trade-off:\")\n",
    "print(\"   Small bins â†’ High temporal resolution but noisy (few spikes)\")\n",
    "print(\"   Large bins â†’ More spikes but lose temporal detail\")\n",
    "print(f\"   Optimal bin size depends on firing rate and state dynamics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980ce6f",
   "metadata": {},
   "source": [
    "## 7. Failure Modes\n",
    "\n",
    "### 7.1 Low Firing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a43786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low firing rate failure\n",
    "firing_rates = [1, 5, 10, 20, 50]\n",
    "results_rate = []\n",
    "\n",
    "print(\"Testing different firing rates...\")\n",
    "for rate in firing_rates:\n",
    "    result = run_decoding_experiment(bin_size_ms=50, baseline_rate=rate, seed=42)\n",
    "    result['baseline_rate'] = rate\n",
    "    results_rate.append(result)\n",
    "    print(f\"  Rate {rate:2d} Hz: RMSE={result['rmse']:.4f}, r={result['correlation']:.3f}\")\n",
    "\n",
    "df_rate = pd.DataFrame(results_rate)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(df_rate['baseline_rate'], df_rate['correlation'], 'ro-', linewidth=2, markersize=10)\n",
    "ax.set_xlabel('Baseline Firing Rate (Hz)', fontsize=12)\n",
    "ax.set_ylabel('Decoding Correlation', fontsize=12)\n",
    "ax.set_title('Failure Mode: Low Firing Rate â†’ Poor Decoding', fontsize=13)\n",
    "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5, label='Poor decoding threshold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'failure_low_rate.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸ Low firing rate â†’ sparse observations â†’ poor state estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ac189",
   "metadata": {},
   "source": [
    "### 7.2 Model Mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ae6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model mismatch: wrong C parameter\n",
    "x_true, y_obs, lambda_true, b = simulate_neural_data(\n",
    "    n_steps=200, Q=0.05, C=0.8, baseline_rate=10.0, bin_size_ms=50, seed=42\n",
    ")\n",
    "\n",
    "C_values = [0.2, 0.5, 0.8, 1.2, 2.0]  # True C = 0.8\n",
    "results_mismatch = []\n",
    "\n",
    "print(\"Testing model mismatch (true C = 0.8)...\")\n",
    "for C_assumed in C_values:\n",
    "    pf = NeuralParticleFilter(n_particles=300, Q=0.05, C=C_assumed, b=b, seed=42)\n",
    "    x_est, _, _, _ = pf.run(y_obs)\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((x_true - x_est)**2))\n",
    "    corr = np.corrcoef(x_true, x_est)[0, 1]\n",
    "    \n",
    "    results_mismatch.append({'C_assumed': C_assumed, 'rmse': rmse, 'correlation': corr})\n",
    "    print(f\"  C = {C_assumed:.1f}: RMSE = {rmse:.4f}, r = {corr:.3f}\")\n",
    "\n",
    "df_mismatch = pd.DataFrame(results_mismatch)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(df_mismatch['C_assumed'], df_mismatch['rmse'], 'bs-', linewidth=2, markersize=10)\n",
    "ax.axvline(0.8, color='red', linestyle='--', linewidth=2, label='True C = 0.8')\n",
    "ax.set_xlabel('Assumed Encoding Gain C', fontsize=12)\n",
    "ax.set_ylabel('RMSE', fontsize=12)\n",
    "ax.set_title('Failure Mode: Wrong Model Parameters', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'failure_model_mismatch.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸ Model mismatch: incorrect parameters degrade decoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15467cbe",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "âœ… **State-space models** naturally describe neural encoding/decoding  \n",
    "âœ… **Poisson observations** are appropriate for spike counts (discrete, non-negative)  \n",
    "âœ… **Particle filter** handles non-Gaussian likelihoods  \n",
    "âœ… **Binning trade-off**: temporal resolution vs. signal strength  \n",
    "âœ… **Failure modes**: low firing rate, model mismatch, too few particles  \n",
    "\n",
    "**Real-world applications:**\n",
    "- Brain-machine interfaces (decode intended movement)\n",
    "- Sensory neuroscience (decode stimulus from neural response)\n",
    "- Clinical: epilepsy prediction, Parkinson's tremor detection\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a5b25",
   "metadata": {},
   "source": [
    "### Exercise 1: Vary Particle Count\n",
    "\n",
    "**Task:** Run decoding with N = 50, 100, 200, 500, 1000 particles.  \n",
    "Plot RMSE and computation time vs N. What's the practical minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b5ec9",
   "metadata": {},
   "source": [
    "### Exercise 2: Multi-Neuron Decoding\n",
    "\n",
    "**Task:** Simulate 3 neurons with different encoding gains C = [0.5, 0.8, 1.2].  \n",
    "Combine their spike counts for decoding. Does more neurons help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f10db",
   "metadata": {},
   "source": [
    "### Exercise 3: Oscillatory Latent State\n",
    "\n",
    "**Task:** Instead of random walk, use oscillatory latent state:  \n",
    "$x_t = A \\sin(2\\pi f t / T) + \\text{noise}$  \n",
    "Does the particle filter track oscillations? How does frequency affect decoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bee31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596afd75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f7ca9",
   "metadata": {},
   "source": [
    "### Solution 1: Particle Count Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb028a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x_true, y_obs, _, b = simulate_neural_data(n_steps=200, seed=42)\n",
    "\n",
    "n_particles_list = [50, 100, 200, 500, 1000]\n",
    "results_particles = []\n",
    "\n",
    "for N in n_particles_list:\n",
    "    start = time.time()\n",
    "    pf = NeuralParticleFilter(n_particles=N, Q=0.05, C=0.8, b=b, seed=42)\n",
    "    x_est, _, _, _ = pf.run(y_obs)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((x_true - x_est)**2))\n",
    "    results_particles.append({'N': N, 'rmse': rmse, 'time_ms': elapsed*1000})\n",
    "\n",
    "df_particles = pd.DataFrame(results_particles)\n",
    "print(df_particles.to_string(index=False))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(df_particles['N'], df_particles['rmse'], 'bo-', markersize=10)\n",
    "axes[0].set_xlabel('Number of Particles', fontsize=12)\n",
    "axes[0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0].set_title('Accuracy vs Particle Count', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(df_particles['N'], df_particles['time_ms'], 'ro-', markersize=10)\n",
    "axes[1].set_xlabel('Number of Particles', fontsize=12)\n",
    "axes[1].set_ylabel('Time (ms)', fontsize=12)\n",
    "axes[1].set_title('Computation Time vs Particle Count', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'ex1_particle_count.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ ~200-500 particles often sufficient for this problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e844d9",
   "metadata": {},
   "source": [
    "### Solution 2: Multi-Neuron Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e706c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_multi_neuron(n_steps, n_neurons, Q, C_list, baseline_rate, bin_size_ms, seed=42):\n",
    "    \"\"\"Simulate multiple neurons sharing same latent state.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Shared latent state\n",
    "    x_true = np.zeros(n_steps)\n",
    "    for t in range(1, n_steps):\n",
    "        x_true[t] = x_true[t-1] + rng.normal(0, np.sqrt(Q))\n",
    "    \n",
    "    b = np.log(baseline_rate * bin_size_ms / 1000)\n",
    "    \n",
    "    # Each neuron has different encoding gain\n",
    "    y_all = np.zeros((n_neurons, n_steps))\n",
    "    for i, C in enumerate(C_list):\n",
    "        lambda_i = np.exp(C * x_true + b)\n",
    "        y_all[i] = rng.poisson(lambda_i)\n",
    "    \n",
    "    return x_true, y_all, b\n",
    "\n",
    "class MultiNeuronPF:\n",
    "    \"\"\"Particle filter for multiple Poisson neurons.\"\"\"\n",
    "    def __init__(self, n_particles, Q, C_list, b, seed=42):\n",
    "        self.n_particles = n_particles\n",
    "        self.Q = Q\n",
    "        self.C_list = C_list\n",
    "        self.b = b\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def run(self, y_all):\n",
    "        n_neurons, n_steps = y_all.shape\n",
    "        particles = self.rng.normal(0, 1, self.n_particles)\n",
    "        x_est = np.zeros(n_steps)\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            particles = particles + self.rng.normal(0, np.sqrt(self.Q), self.n_particles)\n",
    "            \n",
    "            # Combine likelihoods from all neurons\n",
    "            log_weights = np.zeros(self.n_particles)\n",
    "            for i, C in enumerate(self.C_list):\n",
    "                log_weights += poisson_log_likelihood(y_all[i, t], particles, C, self.b)\n",
    "            \n",
    "            log_weights -= np.max(log_weights)\n",
    "            weights = np.exp(log_weights)\n",
    "            weights /= weights.sum()\n",
    "            \n",
    "            x_est[t] = np.sum(weights * particles)\n",
    "            \n",
    "            if 1.0 / np.sum(weights**2) < self.n_particles / 2:\n",
    "                indices = self.rng.choice(self.n_particles, self.n_particles, replace=True, p=weights)\n",
    "                particles = particles[indices]\n",
    "        \n",
    "        return x_est\n",
    "\n",
    "# Compare 1 vs 3 neurons\n",
    "C_list = [0.5, 0.8, 1.2]\n",
    "x_true, y_all, b = simulate_multi_neuron(200, 3, Q=0.05, C_list=C_list, \n",
    "                                          baseline_rate=10, bin_size_ms=50, seed=42)\n",
    "\n",
    "# Single neuron\n",
    "pf1 = NeuralParticleFilter(300, 0.05, C_list[1], b, seed=42)\n",
    "x_est_1 = pf1.run(y_all[1])[0]\n",
    "rmse_1 = np.sqrt(np.mean((x_true - x_est_1)**2))\n",
    "\n",
    "# Three neurons\n",
    "pf3 = MultiNeuronPF(300, 0.05, C_list, b, seed=42)\n",
    "x_est_3 = pf3.run(y_all)\n",
    "rmse_3 = np.sqrt(np.mean((x_true - x_est_3)**2))\n",
    "\n",
    "print(f\"Single neuron RMSE: {rmse_1:.4f}\")\n",
    "print(f\"Three neurons RMSE: {rmse_3:.4f}\")\n",
    "print(f\"\\nâœ“ More neurons improve decoding by {(rmse_1-rmse_3)/rmse_1*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b765829",
   "metadata": {},
   "source": [
    "### Solution 3: Oscillatory State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8dea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_oscillatory(n_steps, freq_hz, amplitude, Q_noise, C, baseline_rate, bin_size_ms, seed=42):\n",
    "    \"\"\"Oscillatory latent state + Poisson observations.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    dt = bin_size_ms / 1000  # seconds per bin\n",
    "    t = np.arange(n_steps) * dt\n",
    "    \n",
    "    # Oscillatory state with noise\n",
    "    x_true = amplitude * np.sin(2 * np.pi * freq_hz * t) + rng.normal(0, np.sqrt(Q_noise), n_steps)\n",
    "    \n",
    "    b = np.log(baseline_rate * bin_size_ms / 1000)\n",
    "    lambda_t = np.exp(C * x_true + b)\n",
    "    y_obs = rng.poisson(lambda_t)\n",
    "    \n",
    "    return t, x_true, y_obs, b\n",
    "\n",
    "# Test different frequencies\n",
    "frequencies = [0.5, 1, 2, 5]  # Hz\n",
    "fig, axes = plt.subplots(len(frequencies), 1, figsize=(14, 3*len(frequencies)), sharex=True)\n",
    "\n",
    "for i, freq in enumerate(frequencies):\n",
    "    t_osc, x_true_osc, y_obs_osc, b_osc = simulate_oscillatory(\n",
    "        n_steps=200, freq_hz=freq, amplitude=1.0, Q_noise=0.01, \n",
    "        C=0.8, baseline_rate=10, bin_size_ms=50, seed=42\n",
    "    )\n",
    "    \n",
    "    pf = NeuralParticleFilter(300, Q=0.1, C=0.8, b=b_osc, seed=42)\n",
    "    x_est_osc, _, _, _ = pf.run(y_obs_osc)\n",
    "    \n",
    "    corr = np.corrcoef(x_true_osc, x_est_osc)[0, 1]\n",
    "    \n",
    "    axes[i].plot(t_osc, x_true_osc, 'b-', linewidth=2, label='True')\n",
    "    axes[i].plot(t_osc, x_est_osc, 'r--', linewidth=2, label='Decoded')\n",
    "    axes[i].set_ylabel(f'{freq} Hz\\nr={corr:.2f}', fontsize=11)\n",
    "    axes[i].legend(loc='upper right', fontsize=9)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_title('Oscillatory Latent State Decoding', fontsize=13)\n",
    "axes[-1].set_xlabel('Time (s)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'ex3_oscillatory.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ PF tracks oscillations, but high frequencies require smaller bins!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada3cd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = f\"\"\"\n",
    "# Notebook 05: Neural Time Series Decoding - Summary\n",
    "\n",
    "**Date:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Key Results\n",
    "\n",
    "1. **State-Space Neural Model:**\n",
    "   - Latent: random walk (x_t = x_{{t-1}} + noise)\n",
    "   - Observation: Poisson(exp(CÂ·x + b))\n",
    "\n",
    "2. **Decoding Performance:**\n",
    "   - RMSE: {rmse:.4f}\n",
    "   - Correlation: {corr:.4f}\n",
    "\n",
    "3. **Binning Trade-off:**\n",
    "   - Small bins: high resolution, low SNR\n",
    "   - Large bins: high SNR, low resolution\n",
    "\n",
    "4. **Failure Modes:**\n",
    "   - Low firing rate â†’ sparse observations\n",
    "   - Model mismatch â†’ biased estimates\n",
    "\n",
    "## Outputs\n",
    "- neural_data_simulation.png\n",
    "- poisson_likelihood.png\n",
    "- decoding_results.png\n",
    "- binning_tradeoff.png\n",
    "- failure_*.png\n",
    "\n",
    "## Applications\n",
    "- Brain-machine interfaces\n",
    "- Sensory neuroscience\n",
    "- Clinical neural monitoring\n",
    "\"\"\"\n",
    "\n",
    "with open(output_dir / 'summary.md', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ Notebook 05 Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Outputs saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
